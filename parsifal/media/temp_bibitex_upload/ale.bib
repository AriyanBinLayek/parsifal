
@ARTICLE{Chen2011344,
author={Chen, L. and Ali Babar, M.},
title={A systematic review of evaluation of variability management approaches in software product lines},
journal={Information and Software Technology},
year={2011},
volume={53},
number={4},
pages={344-362},
note={cited By (since 1996)21},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79951813796&partnerID=40&md5=13b0fa3f17d9fedcfdd7cbc5c5f174ce},
abstract={Context: Variability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. Objective: The objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. Method: We carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. Results: We selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. Conclusion: The findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial. © 2010 Elsevier B.V. All rights reserved.},
author_keywords={Empirical studies;  Software product line;  Systematic literature reviews;  Variability management},
document_type={Article},
source={Scopus},
}

@ARTICLE{DaMotaSilveiraNeto2011407,
author={Da Mota Silveira Neto, P.A. and Carmo MacHado, I.D. and McGregor, J.D. and De Almeida, E.S. and De Lemos Meira, S.R.},
title={A systematic mapping study of software product lines testing},
journal={Information and Software Technology},
year={2011},
volume={53},
number={5},
pages={407-423},
note={cited By (since 1996)14},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79952451558&partnerID=40&md5=a966054f3f281e0f2e4234c8bea7f399},
abstract={Context: In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development. Objective: This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature. Method: A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated. Results: Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed. Conclusion: The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet. © 2010 Elsevier B.V. All rights reserved.},
author_keywords={Mapping study;  Software product lines;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ali2009275,
author={Ali, M.S. and Babar, M.A. and Schmid, K.},
title={A comparative survey of economic models for software product lines},
journal={Conference Proceedings of the EUROMICRO},
year={2009},
pages={275-278},
art_number={5349963},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-74549139714&partnerID=40&md5=9733199dc7a39736187b495497440432},
abstract={Software product line engineering aims at achieving systematic reuse by exploiting commonalities among related products in order to reduce cost and time-tomarket. Before adopting this approach, organizations are likely to estimate the benefits they can expect to achieve and the level of investment required to transition to product line engineering. Several economic models and analysis approaches have been developed in order to help make a sound business case. There is a need to review the existing approaches in order to better understand the overall landscape of economic models. To this objective, this paper provides an overview of some existing economic models and discusses important issues and directions in product line economic modeling. © 2009 IEEE.},
author_keywords={Economic models;  Software product lines},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Alves2010806,
author={Alves, V. and Niu, N. and Alves, C. and Valença, G.},
title={Requirements engineering for software product lines: A systematic literature review},
journal={Information and Software Technology},
year={2010},
volume={52},
number={8},
pages={806-820},
note={cited By (since 1996)11},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650863537&partnerID=40&md5=eccf5f8da7b8ddd42753a67ce4c36ec0},
abstract={Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies. © 2010 Elsevier B.V. All rights reserved.},
author_keywords={Requirements engineering;  Software product lines;  Systematic literature review},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Chen2004103,
author={Chen, Y. and Gannod, G.C. and Collofello, J.S. and Sarjoughian, H.S.},
title={Using simulation to facilitate the study of software product line evolution},
journal={International Workshop on Principles of Software Evolution (IWPSE)},
year={2004},
pages={103-112},
note={cited By (since 1996)2},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-11244351551&partnerID=40&md5=20ca90a51c7f29e9871dd5c0af63608a},
abstract={A product line approach is a disciplined methodology for strategic reuse of source code, requirement specifications, software architectures, design models, components, test cases, and the processes for using the aforementioned artifacts. Software process simulation modeling is a valuable tool for enabling decision making for a wide variety of purposes, ranging from adoption and strategic management to process improvement and planning. In this paper, discrete event simulation is used to provide a framework for the simulation of software product line engineering. We have created an environment that facilitates strategic management and long-term forecasting with respect to software product line development and evolution.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dowie2005,
author={Dowie, U. and Gellner, N. and Hanssen, S. and Helferich, A. and Herzwurm, G. and Schockert, S.},
title={Quality assurance of integrated business software: An approach to testing software product lines},
journal={Proceedings of the 13th European Conference on Information Systems, Information Systems in a Rapidly Changing Economy, ECIS 2005},
year={2005},
page_count={12},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84871009082&partnerID=40&md5=863c94cb2b690bb5a8c57dbfdf1c3cd5},
abstract={The use of integrated business software can be instrumental in reducing the difficulties occurring when various information systems have to be integrated. As a downside of this and due to the fact that these systems are designed to be used in all sorts of enterprises, the internal complexity of these systems increases exponentially. Software product lines on the one hand promise remedy by the conscious use of variability, on the other hand create new demands on quality assurance. The article on hand provides a theoretical framework for evaluating approaches to software testing, regarding their use in the development of software product lines. It turns out that only a practice-oriented approach emphasizing the buyer's view will be successful in the end.},
author_keywords={Application engineering;  Business software;  Domain engineering;  Quality assurance;  Software product line;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2006385,
author={Chen, Y. and Gannod, G.C. and Collofello, J.S.},
title={A software product line process simulator},
journal={Software Process Improvement and Practice},
year={2006},
volume={11},
number={4},
pages={385-409},
note={cited By (since 1996)6},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33747097965&partnerID=40&md5=edf7fc97c538adb41403d8cebdb262cd},
abstract={Organizations are moving towards the use of software product line approaches to build product families. Cases have shown that software product line approaches can reduce time-to-market (TTM), costs, and resource usage. However, those benefits are not guaranteed in all situations, as they are affected by many factors including the number of available resources, market demands, reuse rates, and product line adoption and evolution strategies. Before initiating a software product line, an organization needs to evaluate available process options in order to see which ones best fit its goals. The aim of this research is to help this decision-making process by providing practical approaches and tools. In this article, a process evaluation approach is proposed, a process meta-model is introduced, and a product line process simulator is presented. The approach contains three steps: process definition, simulation, and evaluation. The process meta-model is used for defining software product line processes. The simulator can predict the development costs, schedule, and resource usage rates for a selected software product line process at a high level. The simulator uses DEVSJAVA as the modeling and simulation formalism and COPLOMO as the cost model. An example is also given and some simulation results are discussed. Copyright © 2006 John Wiley & Sons, Ltd.},
author_keywords={Process simulation;  Product line economics;  Software product lines},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Altintas20121376,
author={Altintas, N.I. and Cetin, S. and Dogru, A.H. and Oguztuzun, H.},
title={Modeling product line software assets using domain-specific kits},
journal={IEEE Transactions on Software Engineering},
year={2012},
volume={38},
number={6},
pages={1376-1402},
art_number={6065739},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84870562503&partnerID=40&md5=cd1a7761c863b07e50804a3c652b0387},
abstract={Software Product Line Engineering (SPLE) is a prominent paradigm for the assembly of a family of products using product line core assets. The modeling of software assets that together form the actual products is critical for achieving the strategic benefits of Software Product Lines (SPLs). We propose a feature-based approach to software asset modeling based on abstractions provided by Domain-Specific Kits (DSKs). This approach involves a software Asset Metamodel (AMM) used to derive Asset Modeling Languages (AMLs) that define reusable software assets in domain-specific terms. The approach also prescribes a roadmap for modeling these software assets in conjunction with the product line reference architecture. Asset capabilities can be modeled using feature diagrams as the external views of the software assets. Internal views can be expressed in terms of Domain-Specific Artifacts (DSAs) with Variability Points (VPs), where the domain-specific artifacts are created using Domain-Specific Kits. This approach produces loosely coupled and highly cohesive software assets that are reusable for multiple product lines. The approach is validated by assessing software asset reuse in two different product lines in the finance domain. We also evaluated the productivity gains in large-scale complex projects, and found that the approach yielded a significant reduction in the total project effort. © 2012 IEEE.},
author_keywords={Asset modeling;  domain-specific kits;  feature models;  reuse;  software asset;  software product lines},
document_type={Article},
source={Scopus},
}

@CONFERENCE{FerreiraBastos201111,
author={Ferreira Bastos, J. and Anselmo Da Mota Silveira Neto, P. and Santana De Almeida, E. and Romero De Lemos Meira, S.},
title={Adopting software product lines: A systematic mapping study},
journal={IET Seminar Digest},
year={2011},
volume={2011},
number={1},
pages={11-20},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-82955171618&partnerID=40&md5=1206b796e305c685a8db0a9585a6b919},
abstract={Context: The benefits of taking a product line approach in order to achieve significant reductions in cost and time to market and, at the same time, increasing the quality has encouraged product line adoption. Objective: In this context, this study focuses on some SPL adoption aspects and has the following goals: investigate state-of-the-art SPL adoption, synthesize available evidence, and identify gaps between required strategies, organizational structures, maturity level and existing adoption barriers, available in the literature. Method: A systematic mapping study was undertaken to analyze the important aspects that should be considered when adopting SPL approaches. A set of four questions were defined in which 34 primary studies were evaluated. Results: A total of 34 primary studies were considered. They reported four different strategies (Incremental, Big Bang, Tactical and Pilot project), however there is insufficient information about how such strategies link to factors as organizational structure and process maturity. By investigating all primary studies we found 23 barriers to adoption. Conclusions: Researchers need to consider the relationships between SPL adoption and factors such as company maturity and organization structure in more detail. There is also a need for patterns to assist in SPL adoption and overcoming SPL adoption barriers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Myllärniemi201241,
author={Myllärniemi, V. and Raatikainen, M. and Männistö, T.},
title={A systematically conducted literature review: Quality attribute variability in software product lines},
journal={ACM International Conference Proceeding Series},
year={2012},
volume={1},
pages={41-45},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84867465595&partnerID=40&md5=cf4bb84df53eebc33092b7fda13d4745},
abstract={Typically, products in a software product line differ by their functionality, and quality attributes are not intentionally varied. Why, how, and which quality attributes to vary has remained an open issue. A systematically conducted literature review on quality attribute variability is presented, where primary studies are selected by reading all content of full studies in Software Product Line Conference. The results indicate that the success of feature modeling influences the proposed approaches, different approaches suit specific quality attributes differently, and empirical evidence on industrial quality variability is lacking. Copyright © 2012 ACM.},
author_keywords={Quality attribute;  Systematic literature review;  Variability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nonaka200763,
author={Nonaka, M. and Babar, M.A. and Zhu, L. and Staples, M.},
title={Impacts of architecture and quality investment in software product line development},
journal={Proceedings - 11th International Software Product Line Conference, SPLC 2007},
year={2007},
pages={63-73},
art_number={4339256},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-47949121286&partnerID=40&md5=c559c7f4c976384c5a8faaff296451d1},
abstract={Investment in architecture and quality improvement for a software product line can increase reuse, and consequently reduce effort, enhance product reliability, and shorten time-to-market. Such investments should be carefully chosen to be effective, to avoid over-investment, and to return benefits within the desired time. In this paper, we show how a stochastic simulation model can be used to explore the impacts of such investments. The model is validated by comparison to COPLIMO, a COCOMO II based effort estimation model for product line development, and by inspecting effort distributions of the generated unplanned work. For the illustrative model and scenarios in this paper, we show that the degree of architecture reuse has the largest impact. Preventing degraded architectural dependencies itself does not have a meaningful impact, but if such degradation is also associated with adverse effects on defect injection and detection, it can be significant. Process improvement has a meaningful impact, but over-investment is possible. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tang2010,
author={Tang, A. and Couwenberg, W. and Scheppink, E. and De Burgh, N.A. and Deelstra, S. and Van Vliet, H.},
title={SPL migration tensions: An industry experience},
journal={Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering, KOPLE'10},
year={2010},
art_number={1964141},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79955085454&partnerID=40&md5=09442762a005007be1cac5e4a8a75f07},
abstract={In a software development environment where legacy software systems have been successfully deployed, there are tensions that deter the organization from moving towards software product line engineering (SPLE). An example is the effort required to develop a product line architecture versus time-to-market pressure or the lack of evidence to justify the benefits of SPLE. In this report we discuss the tensions that exist in Océ Technologies. A reactive software reuse approach has not yielded the desired long-term benefits of reusability. A proactive approach requires knowledge exchange and coordination between software management and technical staff. We describe how such knowledge sharing can ease the tensions and facilitate a SPLE migration process. © 2010 ACM.},
author_keywords={agile development process;  architecture management;  industry case study;  software product line engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{DaSilva2011899,
author={Da Silva, I.F. and Da Mota Silveira Neto, P.A. and O'Leary, P. and De Almeida, E.S. and De Lemos Meira, S.R.},
title={Agile software product lines: A systematic mapping study},
journal={Software - Practice and Experience},
year={2011},
volume={41},
number={8},
pages={899-920},
note={cited By (since 1996)2},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79958287763&partnerID=40&md5=8687fba0bb786cdbe7ecc53cdfb10d35},
abstract={Background: Software product lines and Agile methods have been an effective solution for dealing with the growing complexity of software and handling competitive needs of software organizations. They also share common goals, such as improving productivity, reducing time-to-market, decreasing development costs and increasing customer satisfaction. There has been growing interest in whether the integration of Agile and SPL could provide further benefits and solve many of the outstanding issues surrounding software development. Objective: This study investigates the state-of-the-art in Agile SPL approaches, while identifying gaps in current research and synthesizing available evidence. It also provides a basis for a deeper understanding of the issues involved in the integration of Agile and SPL. Method: A mapping study was undertaken to analyze the relation between Agile and SPL methods. A set of four research questions were defined in which the 32 primary studies were evaluated. Results: This study provides insights into the integration of Agile and SPL approaches, it identifies the current gaps in the research, synthesize the available evidence and propose specific Agile methods and practices for integration in SPL. Conclusions: In general, few studies describe the underlying Agile principles being adopted by proposed Agile SPL solutions. The most common Agile practices proposed by the studies came from the XP and Scrum methods, particularly in the pro-active SPL strategy. We identify certain Agile methods that are being overlooked by the Agile SPL community, and propose specific SPL practices areas suitable for adoption of Agile practices. Copyright © 2011 John Wiley & Sons, Ltd.},
author_keywords={Agile methods;  Agile principles;  software product lines;  systematic mapping study},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nolan2010121,
author={Nolan, A.J. and Abrahão, S.},
title={Dealing with cost estimation in software product lines: Experiences and future directions},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6287 LNCS},
pages={121-135},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78049373099&partnerID=40&md5=2733004bbe7f9e36f5ede375336a08ab},
abstract={After 5 years invested in developing accurate cost estimation tools, Rolls-Royce has learnt about the larger potential of the tools to shape many aspects of the business. A good estimation tool is a "model" of a project and is usually used to estimate cost and schedule, but it can also estimate and validate risks and opportunities. Estimation tools have unified engineering, project and business needs. The presence of good estimation tools has driven higher performance and stability in the business. It was evident we needed this capability to underpin decisions in our new Software Product Line strategy. The objective of this paper is twofold. First, we report the experiences gained in the past on the use of estimation tools. Second, we describe the current efforts and future directions on the development of an estimation tool for Software Product Lines. At the heart of the Product Line estimation tool is a simple representation of the product - represented as the number of Lines Of Code (LOC). The next generation of tool, will need to consider wider aspects of product quality in order to create more accurate estimates and support better decisions about our products. © 2010 Springer-Verlag Berlin Heidelberg.},
author_keywords={Cost Estimation;  Industrial Experiences;  Software Product Lines},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tanhaei201050,
author={Tanhaei, M. and Moaven, S. and Habibi, J. and Ahmadi, H.},
title={Toward a business model for software product line architecture},
journal={8th ACIS International Conference on Software Engineering Research, Management and Applications, SERA 2010},
year={2010},
pages={50-56},
art_number={5489097},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77955318693&partnerID=40&md5=d32d1a460b092e04ca9985a24cdb6002},
abstract={Nowadays, software product line is an approach to reduce costs of software development, decrease time to market, and increase capabilities of reuse in designing and exploiting software development processes. Moreover, other quality attributes of the project domain should be considered to enhance quality of the product. Meanwhile, taking advantage of software product line makes developers capable of estimating development costs and time to market in a more realistic way. However, old approaches to estimate cost of development and foresee time to market are not suitable enough for software product line. In this paper, some important business parameters and a way to calculate cost and time to market in a product line are presented. Changing components among time, portion of the change in a specific product and organization issues are observed in the estimation function. © 2010 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wu2011811,
author={Wu, Z. and Tang, J. and Kwong, C.K. and Chan, C.Y.},
title={An optimization model for reuse scenario selection considering reliability and cost in software product line development},
journal={International Journal of Information Technology and Decision Making},
year={2011},
volume={10},
number={5},
pages={811-841},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80052562807&partnerID=40&md5=22c5978d62be8937fa6cc4479bbc1828},
abstract={In this paper, a model that assists developers to evaluate and compare alternative reuse scenarios in software product line (SPL) development systematically in proposed. The model can identify basic activities (abstracted as operations) and precisely relate cost and reliability with each basic operation. A typical reuse mode is described from the perspectives of application and domain engineering. According to this scheme, six reuse modes are identified, and alternative industry reuse scenarios can be derived from these modes. A bi-objective 0-1 integer programming model is developed to help decision makers select reuse scenarios when they develop a SPL to minimize cost and maximize reliability while satisfying system requirements to a certain degree. This model is called the cost and reliability optimization under constraint satisfaction (CROS). To design the model efficiently, a three-phase algorithm for finding all efficient solutions is developed, where the first two phases can obtain an efficient solution, and the last phase can generate a nonsupported efficient solution. Two practical methods are presented to facilitate decision making on selecting from the entire range of efficient solutions in light of the decision-maker's preference for mancomputer interaction. An application of the CROS model in a mail server system development is presented as a case study. © 2011 World Scientific Publishing Company.},
author_keywords={efficient solution;  optimization models;  reuse models;  Software product line},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li201213,
author={Li, D. and Yang, Y.},
title={Enhance value by building trustworthy software-reliant system of systems from software product lines},
journal={2012 3rd International Workshop on Product LinE Approaches in Software Engineering, PLEASE 2012 - Proceedings},
year={2012},
pages={13-16},
art_number={6229761},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84864513631&partnerID=40&md5=e654e60bbf98aed050eef5d5a35e1f53},
abstract={Ever growing and expanding mission-critical domains generate ever emerging and more serious challenges. Complexity which may greatly reduce the trustworthiness of a system is the key reason. Based on the practice of FISCAN in the security inspection domain, we believe a software-reliant system of systems (srSoS) built from software product lines (SPL) is a viable solution to address these issues. In this paper, we present a framework of SPL-to-srSoS to extend SPL practices to srSoS and argue its value-enhancement effects by rapidly meeting increased complexity and emergent behaviors of System of Systems (SoS). The framework employs a basic principle of system hiding, and consists of a SPL-to-srSoS process model and an initially conceived value model. A successfully deployed FISCAN srSoS at Airports provides demonstration for the discussion throughout the paper. © 2012 IEEE.},
author_keywords={software product lines;  software-reliant;  SPL-to-srSOS;  system hiding;  system of systems;  trustworthy;  value model},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Heider2010758,
author={Heider, W. and Froschauer, R. and Grünbacher, P. and Rabiser, R. and Dhungana, D.},
title={Simulating evolution in model-based product line engineering},
journal={Information and Software Technology},
year={2010},
volume={52},
number={7},
pages={758-769},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84863632025&partnerID=40&md5=70c5e2a1ad230a63fb36fb4e8ee15e3f},
abstract={Context: Numerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments. Objective: In this paper, we thus aim to explore the benefit of simulation to investigate the evolution of model-based product lines. Method: We present a simulation approach for exploring the effects of product line evolution on model complexity and maintenance effort. Our simulation considers characteristics of product lines (e.g., size, dependencies in models) and we experiment with different evolution profiles (e.g., technical refactoring vs. placement of new products). Results: We apply the approach in a simulation experiment that uses data from real-world product lines from the domain of industrial automation systems to demonstrate its feasibility. Conclusion: Our results demonstrate that simulation contributes to understanding the effects of maintenance and evolution in model-based product lines. © 2010 Elsevier B.V. All rights reserved.},
author_keywords={Industrial automation systems;  Maintenance and evolution;  Model-based development;  Product line engineering;  Simulation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lee2010637,
author={Lee, J. and Kang, S. and Lee, D.},
title={A comparison of software product line scoping approaches},
journal={International Journal of Software Engineering and Knowledge Engineering},
year={2010},
volume={20},
number={5},
pages={637-663},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650587332&partnerID=40&md5=7f2ae7377ce2dbf5bd9fb548ed918f0c},
abstract={During the past decade a number of methods and techniques for software product line scoping have been developed. Although their basic goal is the same, when it comes to details it is often hard to see what they have in common, where they differ and what their strengths and weaknesses are. This makes it difficult for the user to decide when and how to use them because these methods and techniques sometimes describe the same concepts and activities with different terminologies and, more often than not, by that the activities and tasks defined in them do not exactly match with each other and their inputs/outcomes are not clearly defined. In this paper, we compare and analyze the mainstream approaches to software product line scoping, deduce their essential components and develop them into a unified approach that can be easily referred to and utilized by the user companies planning to launch product lines. © 2010 World Scientific Publishing Company.},
author_keywords={product line scoping;  Software product line;  software reuse},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ganesan200613,
author={Ganesan, D. and Muthig, D. and Yoshimura, K.},
title={Predicting return-on-investment for product line generations},
journal={Proceedings - 10th International Software Product Line Conference, SPLC 2006},
year={2006},
pages={13-22},
art_number={1691573},
note={cited By (since 1996)7},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-34547330411&partnerID=40&md5=fccc703f996bf004e202c03996317b33},
abstract={The decision of an organization to introduce product line engineering depends on a sound and careful analysis of risks and return on investment. The latter is computed by an economic model, which relies on high quality input and must reflect the envisioned migration strategy sufficiently. To facilitate risk analysis, this paper applies Monte-Carlo simulation to an existing product line economic model. Additionally, the model is extended by the support of product line generations that is, considering the degeneration of product line infrastructures and taking reinvestment into an existing product line into account. The practical application of the model is demonstrated by an industrial case study. © 2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Boehm2004156,
author={Boehm, B. and Brown, A.W. and Madachy, R. and Yang, Y.},
title={A software product line life cycle cost estimation model},
journal={Proceedings - 2004 International Symposium on Empirical Software Engineering, ISESE 2004},
year={2004},
pages={156-164},
note={cited By (since 1996)26},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-11244335623&partnerID=40&md5=9f918078822076ce873410bab1a007ca},
abstract={Most software product line cost estimation models are calibrated only to local product line data rather than to a broad range of product lines. They also underestimate the return on investment for product lines by focusing only on development vs. life-cycle savings, and by applying writing-for-reuse surcharges to the entire product rather that to the portions of the product being reused. This paper offers some insights based on the exploratory development and collaborative refinement of a software product line life cycle economics model, the Constructive Product Line Investment Model (COPLIMO) that addresses these shortfalls. COPLIMO consists of two components: a product line development cost model and an annualized post-development life cycle extension. It focuses on modeling the portions of the software that involve product-specific newly-built software, fully reused black-box product line components, and product line components that are reused with adaptation. This model is an extension built upon USC-CSE's well-calibrated, multi-parameter Constructive Cost Model (COCOMO) II, tailored down to cover the essentials of strategic software product line decision issues and available supporting data from industries.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nolan2011203,
author={Nolan, A.J. and Abrahão, S. and Clements, P. and McGregor, J.D. and Cohen, S.},
title={Towards the integration of quality attributes into a software product line cost model},
journal={Proceedings - 15th International Software Product Line Conference, SPLC 2011},
year={2011},
pages={203-212},
art_number={6030062},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80054064419&partnerID=40&md5=20e6f4d0003bc112680f98263dc45c6d},
abstract={A good estimation tool offers a "model" of a project and is usually used to estimate cost and schedule, but it can also be used to help make trade decisions that affect cost and schedule as well as to estimate risks and opportunities. It was evident that Rolls-Royce needed a cost model to underpin decisions when they launched a Software Product Line initiative. The first generation cost model was based on COCOMO II, which represents the software product as a single size measure (Source Lines of Code) but makes limited use of the architecture or any characteristics of the product being developed. The next generation of the cost model, currently under development, is intended to account for the quality attributes of the core assets and the resulting products in order to estimate their impact on cost and net-benefit to the business. The objective of this paper is to describe our current efforts to integrate key quality attributes into the SPL cost model. We describe the quality attributes selected, the reason for their selection and the benefits we expect to obtain after integrating them into the model. © 2011 IEEE.},
author_keywords={Cost Estimation;  Industrial Experiences;  Quality Attributes;  Safety-Critical Software;  Software Product Lines},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nonaka2007330,
author={Nonaka, M. and Zhu, L. and Babar, M.A. and Staples, M.},
title={Project cost overrun simulation in software product line development},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2007},
volume={4589 LNCS},
pages={330-344},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-35148830771&partnerID=40&md5=08db6696e98155692f34de3c6d64eae6},
abstract={The cost of a Software Product Line (SPL) development project sometimes exceeds the initially planned cost, because of requirements volatility and poor quality. In this paper, we propose a cost overrun simulation model for time-boxed SPL development. The model is an enhancement of a previous model, specifically now including: consideration of requirements volatility, consideration of unplanned work for defect correction during product projects, and nominal project cost overrun estimation. The model has been validated through stochastic simulations with fictional SPL project data, by comparing generated un-planned work effort to actual change effort, and by sensitivity analysis. The result shows that the proposed model has reasonable validity to estimate nominal project cost overruns and its variability. Analysis indicates that poor management of requirements and quality will almost double estimation error, for the studied simulation settings. © Springer-Verlag Berlin Heidelberg 2007.},
author_keywords={Cost overrun estimation;  Process simulation;  Software product line development},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Heradio-Gil2011496,
author={Heradio-Gil, R. and Fernandez-Amoros, D. and Cerrada, J.A. and Cerrada, C.},
title={Supporting commonality-based analysis of software product lines},
journal={IET Software},
year={2011},
volume={5},
number={6},
pages={496-509},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84864265673&partnerID=40&md5=d35bed2fc7540a45deb655c838b0c7d6},
abstract={Software product line (SPL) engineering is a cost-effective approach to developing families of similar products. Key to the success of this approach is to correctly scope the domain of the SPL, identifying the common and variable features of the products and the interdependencies between features. In this study, the authors show how the commonality of a feature (i.e. the reuse ratio of the feature among the products) can be used to detect scope flaws in the early stages of development. SPL domains are usually modelled by means of feature diagrams following the feature-oriented domain analysis (FODA) notation. The authors extend classical FODA trees with unrestricted cardinalities, and present an algorithm to compute the number of products modelled by a feature diagram and the commonality of the features. Finally, the authors compare the performance of their algorithm with two other approaches built on top of boolean logic satisfiability (SAT)-solver technology such as cachet and relsat. © 2011 The Institution of Engineering and Technology.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Schmid200561,
author={Schmid, K. and Biffl, S.},
title={Systematic management of software product lines},
journal={Software Process Improvement and Practice},
year={2005},
volume={10},
number={1},
pages={61-76},
note={cited By (since 1996)5},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-15244354431&partnerID=40&md5=1446ae6f8778c10a0087b8316613d05c},
abstract={Software product lines can effectively facilitate large-scale reuse and can thus bring about order of magnitude improvements in terms of time to market (TTM), costs, and quality. This comes at the price of a more complex development environment in which many interdependencies are created through shared generic assets. Owing to this complexity, the specific strategy chosen for product line development can be expected to have a strong impact on the benefits that can be gained from product line development. This is systematically studied in this work, as we vary different strategies and apply them to various forms of products lines. On the basis of the analysis of the performed simulations, we were able to determine optimal, heuristic strategies to the integrated management of the product line. As a result of the analysis, we identify strategies and guidelines that can be employed by practitioners in order to improve the success of their management of a software product line. Copyright © 2005 John Wiley & Sons, Ltd.},
author_keywords={Heuristic management;  Multiproject management;  Product line;  Project management;  Simulation;  Software product lines},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rabiser2007141,
author={Rabiser, R. and Grünbacher, P. and Dhungana, D.},
title={Supporting product derivation by adapting and augmenting variability models},
journal={Proceedings - 11th International Software Product Line Conference, SPLC 2007},
year={2007},
pages={141-150},
art_number={4339263},
note={cited By (since 1996)27},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-47949122381&partnerID=40&md5=15e57e82a319cf1285478f8745e475ac},
abstract={Product derivation is the process of constructing products from the core assets in a product line. Guidance and support are needed to increase efficiency and to deal with the complexity of product derivation. Research has, however, devoted comparatively little attention to this process. In this paper we describe an approach for supporting product derivation. We show that variability models need to be prepared for concrete projects before they can be effectively utilized in the derivation process. Project-specific information and sales knowledge should be added and irrelevant variability should be pruned. We also present tool support and illustrate the approach using examples from ongoing research collaboration. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Müller201115,
author={Müller, J.},
title={Value-based portfolio optimization for software product lines},
journal={Proceedings - 15th International Software Product Line Conference, SPLC 2011},
year={2011},
pages={15-24},
art_number={6030042},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80054059529&partnerID=40&md5=b4ae53297393d47d630876a746c6ded6},
abstract={Software Product Lines are a mean to improve the economic performance of firms that offer several products to a market by systematically reusing software artifacts. In most cases the definitive company goal is profit maximization. That can be reached by increasing revenue or by reducing cost. Revenue is increased by offering products with a wide variety of features to an audience willing to pay. However, the fewer features are realized the fewer cost incur. Hence, a firm may ask what features are most important to realize. We approach this question by introducing Value-Based Portfolio Optimization as an addition to common Product Portfolio Scoping approaches that helps deciding on this question. © 2011 IEEE.},
author_keywords={Economic Model;  Optimization;  Portfolio;  Scoping;  Software Product Line},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2011213,
author={Li, D. and Weiss, D.M.},
title={Adding value through software product line engineering: The evolution of the FISCAN software product lines},
journal={Proceedings - 15th International Software Product Line Conference, SPLC 2011},
year={2011},
pages={213-222},
art_number={6030063},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80054053549&partnerID=40&md5=4664c15de0ca02c63de0b61af8aad649},
abstract={The concept of value can be formalized as the ratio of benefits to costs. The value is realized if total benefits outweigh total costs, while value-adding consists of delivering higher benefit at lower cost. Software Product Line Engineering (SPLE) has shown its power to bring benefits and reduce costs for practitioners in various domains, however, systematic value-adding analysis is rare and adding value through SPLE is a dynamic process. FISCAN, a leading manufacturer of security inspection systems in China, continues to explore the value-adding process through SPLE during the evolution of its software product lines in the security inspection domain. This paper discusses the value-adding effect of the "invisible hand" of the market. We identify economic, technical and organizational factors for value-adding through SPLE based on the SPLE practices at FISCAN. We describe the influence of the FISCAN product lines on FISCAN itself, the market and other stakeholders, and suggest what the future may bring in value-adding through SPLE. © 2011 IEEE.},
author_keywords={3-Stage Market Model;  common technical chain;  market chain;  Max-Com & Min-Var;  MM-CAM;  SPL;  SPLE;  system of systems;  value;  value chain;  value-adding},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Heradio20127919,
author={Heradio, R. and Fernandez-Amoros, D. and Torre-Cubillo, L. and Perez Garcia-Plaza, A.},
title={Improving the accuracy of COPLIMO to estimate the payoff of a software product line},
journal={Expert Systems with Applications},
year={2012},
volume={39},
number={9},
pages={7919-7928},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84858342500&partnerID=40&md5=ece2ba6729a480b6c327c8a3231f3416},
abstract={Software product line engineering pursues the efficient development of families of similar products. COPLIMO is an economic model that relies on COCOMO II to estimate the benefits of adopting a product line approach compared to developing the products one by one. Although COPLIMO is an ideal economic model to support decision making on the incremental development of a product line, it makes some simplifying assumptions that may produce high distortions in the estimates (e.g.; COPLIMO takes for granted that all the products have the same size). This paper proposes a COPLIMO reformulation that avoids such assumptions and, consequently, improves the accuracy of the estimates. To support our proposal, we present an algorithm that infers the additional information that our COPLIMO reformulation requires from feature diagrams, which is a widespread notation to model the domain of a product line. © 2012 Elsevier Ltd. All rights reserved.},
author_keywords={Decision support;  Economic model;  Feature diagram;  Product counting;  Software product line},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wu201069,
author={Wu, Z.-Q. and Tang, J.-F. and Zhou, J.-G.},
title={Selecting optimal reuse scenarios based on the two-stage procedure model in a software product line},
journal={Kongzhi Lilun Yu Yingyong/Control Theory and Applications},
year={2010},
volume={27},
number={1},
pages={69-76},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77950899089&partnerID=40&md5=9b9cf3176b3e57395005fbb547d1a863},
abstract={We investigate the selection of the reuse scenarios on a two-stage software product line for satisfying the given requirements within a budgetary constraint while maximizing the product quality. Based on the two-stage process model, we conclude 6 typical reuse scenarios and their realizations, and propose a method for selecting the best one from them by considering the budget limitations, the development cost and time, the failure rate and reliability to optimize the production quality. Based on the Bayes theory, we developed a simplified testing function for estimating the intensity of domain testing in the developing phase of the product. A mailbox service system is used as a real example to demonstrate the effectiveness of the model.},
author_keywords={Optimization models;  Software product line;  Software quality;  Software reliability;  Software reuse},
document_type={Article},
source={Scopus},
}

@ARTICLE{McGregor2010502,
author={McGregor, J.D.},
title={The many paths to quality core assets},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6287 LNCS},
pages={502},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78049366803&partnerID=40&md5=e8a85b36b53ee8971a19604ab10a2552},
abstract={Can all of the different approaches to software product line operation lead to quality core assets? Some organizations use a recognized method such as the Software Engineering Institute's Framework for Product Line Practice [4] while others use a homegrown variant of their non-product line process. Some organizations iteratively mature their core assets while others build proactively. © 2010 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Falessi201318,
author={Falessi, D. and Cantone, G. and Canfora, G.},
title={Empirical principles and an industrial case study in retrieving equivalent requirements via natural language processing techniques},
journal={IEEE Transactions on Software Engineering},
year={2013},
volume={39},
number={1},
pages={18-44},
art_number={6112783},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84872040610&partnerID=40&md5=1026a9b2a682485c55884a288edf6f44},
abstract={Though very important in software engineering, linking artifacts of the same type (clone detection) or different types (traceability recovery) is extremely tedious, error-prone, and effort-intensive. Past research focused on supporting analysts with techniques based on Natural Language Processing (NLP) to identify candidate links. Because many NLP techniques exist and their performance varies according to context, it is crucial to define and use reliable evaluation procedures. The aim of this paper is to propose a set of seven principles for evaluating the performance of NLP techniques in identifying equivalent requirements. In this paper, we conjecture, and verify, that NLP techniques perform on a given dataset according to both ability and the odds of identifying equivalent requirements correctly. For instance, when the odds of identifying equivalent requirements are very high, then it is reasonable to expect that NLP techniques will result in good performance. Our key idea is to measure this random factor of the specific dataset(s) in use and then adjust the observed performance accordingly. To support the application of the principles we report their practical application to a case study that evaluates the performance of a large number of NLP techniques for identifying equivalent requirements in the context of an Italian company in the defense and aerospace domain. The current application context is the evaluation of NLP techniques to identify equivalent requirements. However, most of the proposed principles seem applicable to evaluating any estimation technique aimed at supporting a binary decision (e.g., equivalent/nonequivalent), with the estimate in the range [0,1] (e.g., the similarity provided by the NLP), when the dataset(s) is used as a benchmark (i.e., testbed), independently of the type of estimator (i.e., requirements text) and of the estimation method (e.g., NLP). © 1976-2012 IEEE.},
author_keywords={Empirical software engineering;  equivalent requirements;  metrics and measurement;  natural language processing;  traceability recovery},
document_type={Article},
source={Scopus},
}

@ARTICLE{Krishnan20131479,
author={Krishnan, S. and Strasburg, C. and Lutz, R.R. and Goseva-Popstojanova, K. and Dorman, K.S.},
title={Predicting failure-proneness in an evolving software product line},
journal={Information and Software Technology},
year={2013},
volume={55},
number={8},
pages={1479-1495},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84878337129&partnerID=40&md5=fe988560e9b43c911f1a212671c320d1},
abstract={Previous work by researchers on 3 years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. Objective The work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. Method This investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. Results Our experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. Conclusion As the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist. © 2012 Elsevier B.V. All rights reserved.},
author_keywords={Change metrics;  Failure-prone files;  Post-release defects;  Prediction;  Reuse;  Software product lines},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nonaka2007283,
author={Nonaka, M. and Liming, Z. and Babar, M.A. and Staples, M.},
title={Project delay variability simulation in software product line development},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2007},
volume={4470 LNCS},
pages={283-294},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-37149008719&partnerID=40&md5=b4e7eb528a0a345611c8bbfb5412c11d},
abstract={The possible variability of project delay is useful information to understand and mitigate the project delay risk. However, it is not sufficiently considered in the literature concerning effort estimation and simulation in software product line development. In this paper, we propose a project delay simulation model by introducing a random variable to represent the variability of adaptive rework. The model has been validated through stochastic simulations by comparing generated adaptive rework to an actual change effort distribution, and by sensitivity analysis. The result shows that the proposed model is capable of producing reasonable variability of adaptive rework, and consequently, variability of project delay. Analysis of our model indicates that the strength of dependency has a larger impact than the number of residual defects, for the studied simulation settings. However, high levels of adaptive rework variability did not have great impact on overall project delay. © Springer-Verlag Berlin Heidelberg 2007.},
author_keywords={Process simulation;  Product quality;  Project planning;  Software product line development},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yoshida2006,
author={Yoshida, M. and Iwane, N.},
title={An approach to the software product line system for web applications},
journal={2006 International Conference on Computing and Informatics, ICOCI '06},
year={2006},
art_number={5276435},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-71249159292&partnerID=40&md5=319fbc7014f61fa7079e7412055f04c9},
abstract={Quality, cost and delivery are the most crucial factors to be managed in developing software systems in the industry. The management of these software systems includes not only resolving the necessarily technical problems but also the organizational problems and the business problems. Companies depending upon their environments decide their own strategies to solve these problems. Software product line system is one of the solutions to these problems. This paper describes the way we experimented in these several years to develop the software systems at the company to meet some software development solutions. The efforts of web-based application developments by the source code generator we developed are evaluated based on COCOMO model. Furthermore, the software development model for software product line systems using the toolkit is examined. This paper describes an approach to the software product line systems for web application systems. ©2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bin20051820,
author={Bin, X.U.},
title={Managing customer satisfaction in maintenance of software product family via id3},
journal={2005 International Conference on Machine Learning and Cybernetics, ICMLC 2005},
year={2005},
pages={1820-1824},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-28444487142&partnerID=40&md5=c91c30a29629695f2638a22bb1440e7c},
abstract={Customer satisfaction becomes more and more important in currently competitive marketplace. Managing the customer satisfaction is important for the software vendors especially those who provide product family. This paper suggests using ID3 to manage customer satisfaction in maintenance of software product family. Factors analysis, attributes choosing, and effort estimation with LD3 are also stated in this paper. The result from the case study showed the great benefit of this method. © 2005 IEEE.},
author_keywords={AHP;  C4.5;  Cost estimation;  Customer satisfaction;  Decision tree;  ID3;  Machine learning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yoshida2007910,
author={Yoshida, M. and Iwane, N.},
title={Towards the software life cycle cost for integrated software product line systems},
journal={2006 IEEE International Conference on Industrial Informatics, INDIN'06},
year={2007},
pages={910-916},
art_number={4053510},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-38949114890&partnerID=40&md5=b5a42e2a1776975445983ce1cf5044d1},
abstract={Quality, cost and delivery are the most crucial factors to be managed in developing software systems in the industry. The management of these software systems includes not only resolving the necessarily technical problems but also the organizational problems and the business problems. Companies depending upon their environments decide their own strategies to solve these problems. This paper describes the way we experimented in these several years to reduce the cost foisoftware systems at the company to meet some software development solutions. The efforts of web-based application developments by the toolkit, the source code generator we developed, are evaluated. The toolkit extended to the software product line systems is described. And, the cost for the software life cycle is estimated. © 2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yoshimura200663,
author={Yoshimura, K. and Ganesan, D. and Muthig, D.},
title={Defining a strategy to introduce a software product line using existing embedded systems},
journal={IEEE International Conference on Embedded Software, EMSOFT 2006},
year={2006},
pages={63-72},
note={cited By (since 1996)8},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-34547520700&partnerID=40&md5=ee5999e8e0bddc51fca543edc5b39221},
abstract={Engine Control Systems (ECS) for automobiles have numerous variants for many manufactures and different markets. To improve development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets the business background of ECS. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing this strategy into existing products.This paper explains an approach for assessing the potential of merging existing embedded software into a product line approach. The definition of an economically useful product line approach requires two things: analyzing return on investment (ROI) expectations of a product line and understanding the effort required for building reusable assets. We did a clone analysis to provide the basis for effort estimation for merge potential assessment of existing variants. We also report on a case study with ECS. We package the lessons learned and open issues that arose during the case study. Copyright 2006 ACM.},
author_keywords={Clone detection and classification;  Economics;  Engine control systems;  Reverse rngineering;  Software;  Software product line},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor2010,
title={Software Product Lines: Going Beyond - 14th International Conference, SPLC 2010, Proceedings},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6287 LNCS},
page_count={544},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78049389332&partnerID=40&md5=058b7296efe50821678d8112952676ee},
abstract={The proceedings contain 60 papers. The topics discussed include: context-dependent product line practice for constructing reliable embedded systems; configuring software product line feature models based on stakeholders' soft and hard requirements; a flexible approach for generating product-specific documents in product lines; formal definition of syntax and semantics for documenting variability in activity diagrams; delta-oriented programming of software product lines; architecting automotive product lines: industrial practice; developing a software product line for train control: a case study of CVL; dealing with cost estimation in software product lines: experiences and future directions; variability modeling for distributed development - a comparison with established practice; consistent product line configuration across file type and product line boundaries; and improving the testing and testability of software product lines.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Wu20091880,
author={Wu, Z.-Q. and Tang, J.-F. and Wang, L.-Y.},
title={An optimization framework for reuse component selection in software product line},
journal={2009 Chinese Control and Decision Conference, CCDC 2009},
year={2009},
pages={1880-1884},
art_number={5192848},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-70449377969&partnerID=40&md5=c6a1f1a7a6865c608621152f0086429d},
abstract={We tackle the problem that selecting a series of components (simple reuse, build in-house or COTS) to satisfy a set of requirements within a budgetary constraint while maximize quality is becoming more difficult in the requirements phase. We study two main issues, analysis of optimal framework component selection process and survey on the activities of the entire software development in product line. Our search results introduce the framework based on decision variables indicating the set of architectural components to reuse scenario in order to pursue the highest quality while assuring a certain degree of satisfaction of the system requirements with a given cost budget. It also extends the quality properties to the non-functional requirement, such as reliability and delivery time. © 2009 IEEE.},
author_keywords={Optimization models;  Software product line;  Software quality;  Software reliability;  Software reuse},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lamine2008524,
author={Lamine, S.B.A.B. and Jilani, L.L. and Ghezala, H.H.B.},
title={A software cost estimation meta-model for systematic reuse approaches},
journal={Proceedings of the 2008 International Conference on Software Engineering Research and Practice, SERP 2008},
year={2008},
pages={524-530},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-62749182323&partnerID=40&md5=9fe194bef49a198051616b17a7827bb6},
abstract={PLE, CBSE and COTS-BD are systematic reuse approaches, where reuse is planned at the beginning of a project and is a large scale activity. Therefore, they need important initial investments. Cost estimation environments (models associated with supporting tools) can help the stakeholders to discover the strength of each large scale reuse approach, to better understanding them, to appreciate their promised benefits and choose between them for specific needs and contexts. In this paper, a software cost estimation meta-model is proposed for such approaches, as a result of an investigation showing the similarities between them. The meta-model adopts a four-cycle reuse investment. Variabilities between the approaches necessitate an instantiation of the meta-model for each of them. To quantify estimations, many techniques can be used, depending on the organization's needs, habits, available data, etc.},
author_keywords={CBSE;  Cost estimation model;  COTS;  PLE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nóbrega200827,
author={Nóbrega, J.P. and De Almeida, E.S. and Meira, S.R.L.},
title={InCoME: Integrated cost model for product line engineering},
journal={EUROMICRO 2008 - Proceedings of the 34th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2008},
year={2008},
pages={27-34},
art_number={4725702},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-60349123147&partnerID=40&md5=cef2789ad06c587af1d5bcf56e0e5076},
abstract={A large number of software organizations are adopting the software product line approach in their reuse program. One fundamental factor to evaluate cost-benefit of this approach is the practical use of cost models to estimate if an investment is worthwhile for a family of products. This paper analyzes the most significant cost models for product line engineering and it highlights the set of features that makes an effective model. This work also presents an integrated cost model for product line engineering with its foundations and elements. At the end, is presented a discussion over the results of a case study where the model was applied. © 2008 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{In2006,
author={In, H.P. and Baik, J. and Kim, S. and Yang, Y. and Boehm, B.},
title={A quality-based cost estimation model for the product line life cycle},
journal={Communications of the ACM},
year={2006},
volume={49},
number={12},
art_number={1183273},
note={cited By (since 1996)10},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33751574275&partnerID=40&md5=61b95cd95af73fd11a469da2c7225b37},
abstract={Different aspect of quality-based SPL cost estimation (qCOPLIMO), a quality-based product line life cycle cost estimation model are discussed. The proposed model include the software quality cost in the SPL business case analysis and it consists of the two cost models, Relative Cost of Writing for Reuse (RCWR) for initial product line development and Realtive Cost for Reuse (RCR) model. RCWR is the added cost of writing software to be most cost-effectively reused across a product line family of applications, relative to the cost of writing a standalone application. RCR is the cost of reusing the software in a new application with the same product line family, relative to developing newly built software for the application. After the initial product is developed using product line engineering practice, the portion or the whole can be used for other products in the same product line. The proposed qCOPLIMO model provides a framework to estimate the effects of software quality cost.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li200855,
author={Li, J. and Ruhe, G.},
title={Multi-criteria decision analysis for customization of Estimation by Analogy method AQUA+},
journal={Proceedings - International Conference on Software Engineering},
year={2008},
pages={55-62},
art_number={1370803},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-57049116724&partnerID=40&md5=0e94db61c1eb56c7b48d74356d6fe364},
abstract={The quality of results from a predictor model depends on the proper customization of the parameters of the model. For Estimation by Analogy (EBA), the impact of the parameter "Attribute weighting technique" has been shown by several authors. The decision problem "Which attribute weighting technique is preferable for EBA in which situation?" is considered in this paper from the perspective of multi-criteria decision analysis (MCDA). The empirical results are given for the EBA method AQUA+. More specifically, two MCDA techniques, ELECTRE and Paretooptimality are applied. Three evaluation criteria MMRE (Mean Magnitude of Relative Error), Pred (Prediction at certain accuracy level), and Strength are considered. We discuss the insights gained from this more in-depth decision analysis for the stated decision problem. Copyright 2008 ACM.},
author_keywords={Estimation by analogy;  Multicriteria decision analysis;  Software effort estimation;  Software engineering decision support;  Technology customization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Daneva2010329,
author={Daneva, M.},
title={Balancing uncertainty of context in ERP project estimation: An approach and a case study},
journal={Journal of Software Maintenance and Evolution},
year={2010},
volume={22},
number={5},
pages={329-357},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77955438327&partnerID=40&md5=ece4ab28feeafcf1748d19cdd1e60970},
abstract={The increasing demand for Enterprise Resource Planning (ERP) solutions as well as the high rates of troubled ERP implementations and outright cancellations calls for developing effort estimation practices to systematically deal with uncertainties in ERP projects. This paper describes an approach-and a case study-to balancing uncertainties of context in the very early project stages, when an ERP adopter initiates a request-for-proposal process and when alternative bids are to be compared for the purpose of choosing an implementation partner. The proposed empirical approach leverages the complementary application of three techniques, an algorithmic estimation model, Monte Carlo simulation, and portfolio management. Our case study findings show how the ability of our approach to model uncertainty allows practitioners to address the challenging question of how to adjust project context factors so that chances of project success are increased. We also include a discussion on the implications of our approach for practice as well as on the possible validity threats and what the practitioner could do to counterpart them. Copyright © 2010 John Wiley & Sons, Ltd.},
author_keywords={COCOMO;  Enterprise resource planning implementation;  Monte Carlo simulation;  Portfolio management;  Project effort estimation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Postmus2008753,
author={Postmus, D. and Meijler, T.D.},
title={Aligning the economic modeling of software reuse with reuse practices},
journal={Information and Software Technology},
year={2008},
volume={50},
number={7-8},
pages={753-762},
note={cited By (since 1996)6},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-42649127085&partnerID=40&md5=afeb75fcd1e54e812c2a37bdafbab762},
abstract={In contrast to current practices where software reuse is applied recursively and reusable assets are tailored trough parameterization or specialization, existing reuse economic models assume that (i) the cost of reusing a software asset depends on its size and (ii) reusable assets are developed from scratch. The contribution of this paper is that it provides modeling elements and an economic model that is better aligned with current practices. The functioning of the model is illustrated in an example. The example also shows how the model can support practitioners in deciding whether it is economically feasible to apply software reuse recursively. © 2007 Elsevier B.V. All rights reserved.},
author_keywords={Composition;  Reuse economic model;  Software reuse;  Variation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kulkarni2013220,
author={Kulkarni, V.},
title={Model driven software development: A practitioner takes stock and looks into future},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={7949 LNCS},
pages={220-235},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84879857481&partnerID=40&md5=a4e2294b4c3795d23c835556e46437c8},
abstract={We discuss our experience in use of models and model-driven techniques for developing large business applications. Benefits accrued and limitations observed are highlighted. We describe possible means of overcoming some of the limitations and experience thereof. A case for shift in focus of model driven engineering (MDE) community in the context of large enterprises is argued. Though emerging from a specific context, we think, the takeaways from this experience may have a more general appeal for MDE practitioners, tool vendors and researchers. © 2013 Springer-Verlag.},
author_keywords={meta modeling;  model driven engineering workbench;  model driven enterprise;  model transformation;  Modeling;  separation of concerns;  software product lines},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Capilla201081,
author={Capilla, R. and Dueñas, J.C. and Nava, F.},
title={Viability for codifying and documenting architectural design decisions with tool support},
journal={Journal of Software Maintenance and Evolution},
year={2010},
volume={22},
number={2},
pages={81-119},
note={cited By (since 1996)4},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77749315848&partnerID=40&md5=58809175d7c0aae0f82279564838a4dd},
abstract={Current software architecture practices have been focused on modeling and documenting the architecture of a software system by means of several architectural views. In practice, the standard architecture documentation lacks explicit description of the decisions made and their underlying rationale, which often leads to knowledge loss. This fact strongly affects the maintenance activities as we need to spend additional effort to replay the decisions made as well as to understand the changes performed in the design. Hence, codifying this architectural knowledge is a challenging task that requires adequate tool support. In this research, we test the capabilities of Architecture Design Decision Support System (ADDSS), a web-based tool for supporting the creation, maintenance, use, and documentation of architectural design decisions (ADD) with their architectures. We used ADDSS to codify architectural knowledge and to maintain those trace links between the design decisions and other software artefacts that would help in the maintenance operations. We illustrate the usage of the tool through four different experiences and discuss the potential benefits of using this architectural knowledge and its impact on the maintenance and evolution activities. Copyright © 2009 John Wiley & Sons, Ltd.},
author_keywords={Architectural knowledge;  Design decisions;  Design rationale;  Software architecture;  Software maintenance;  Traceability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lamine2005649,
author={Lamine, S.B.A.B. and Jilani, L.L. and Ghezala, H.H.B.},
title={A Software Cost Estimation Model for Product Line Engineering: SoCoEMo-PLE},
journal={Proceedings of the 2005 International Conference on Software Engineering Research and Practice, SERP'05},
year={2005},
volume={2},
pages={649-655},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-60749108922&partnerID=40&md5=046c51348ecc94312e813acc07a903d7},
abstract={Economic models for reuse can help managers to forecast the benefits of a reuse investment before committing any resources. In this paper, we propose a Software Cost Estimation Model for Product Line Engineering (SoCoEMo-PLE). In fact, PLE approaches to software development seem to be very attractive in matter of product quality and time-tomarket. Thus, we need an economic model to quantify the predicted benefits of using such approach. In this paper, we present an introduction in section one. In section two, we present the economic models that were the basis of the development of SoCoEMo-PLE. Then, in section three, we detail the model SoCoEMo-PLE and recapitulate its contribution in a synthesis. We end with a conclusion in section four.},
author_keywords={Cost Estimation Model;  PLE;  Software reuse},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mohagheghi2005303,
author={Mohagheghi, P. and Anda, B. and Conradi, R.},
title={Effort estimation of use cases for incremental large-scale software development},
journal={Proceedings - 27th International Conference on Software Engineering, ICSE05},
year={2005},
pages={303-311},
note={cited By (since 1996)28},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33244495310&partnerID=40&md5=548bf7ce123b7c7d5942769f169e1eaf},
abstract={This paper describes an industrial study of an effort estimation method based on use cases, the Use Case Points method. The original method was adapted to incremental development and evaluated on a large industrial system with modification of software from the previous release. We modified the following elements of the original method: a) complexity assessment of actors and use cases, and b) the handling of non-functional requirements and team factors that may affect effort. For incremental development, we added two elements to the method: c) counting both all and the modified actors and transactions of use cases, and d) effort estimation for secondary changes of software not reflected in use cases. We finally extended the method to: e) cover all development effort in a very large project. The method was calibrated using data from one release and it produced an estimate for the successive release that was only 17% lower than the actual effort. The study identified factors affecting effort on large projects with incremental development. It also showed how these factors can be calibrated for a specific context and produce relatively accurate estimates. Copyright 2005 ACM.},
author_keywords={Estimation;  Incremental development;  Use cases},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{BenAbdallahBenLamine2005383,
author={Ben Abdallah Ben Lamine, S. and Labed Jilani, L. and Hajjami Ben Ghezala, H.},
title={A software cost estimation model for a product line engineering approach: Supporting tool and UML modeling},
journal={Proceedings - Third ACIS International Conference on Software Engineering Research, Management and Applications, SERA 2005},
year={2005},
volume={2005},
pages={383-390},
art_number={1563187},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33845973779&partnerID=40&md5=2ae61a9f0484ead438370cc75182088e},
abstract={The Product Line Engineering approaches (PLE) to software development are promising in matter of quality, productivity and time-to-market. Some results achieved in industry can prove that. But managers need quantitative models reassuring them concerning the important initial investment they are going to commit. This paper reports on the need for such economic models for reuse as well as the underlying supported tools. Thus, we introduce a new Software Cost Estimation Model for Product Line Engineering that we denote as SoCoEMo-PLE. This model is based on two previous models: the integrated cost estimation model for reuse in general and Paulin's model for PLE. In fact, we present a new model which takes into account PLE software development cycle approach and takes some features of the two previous cost estimation models. The tool supporting SoCoEMo-PLE is described and an UML modeling is presented. Results of a preliminary experiment in the use of the model and the tool are reported. © 2005 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Charles2011377,
author={Charles, O. and Schalk, M. and Thiel, S.},
title={Cost models for software product lines: Introduction and overview [Kostenmodelle für softwareproduktlinien]},
journal={Informatik-Spektrum},
year={2011},
volume={34},
number={4},
pages={377-390},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80052736709&partnerID=40&md5=5aa917ac9d3dbb1df0d0bbc745358391},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yoshida2006,
author={Yoshida, M. and Iwane, N.},
title={An approach to the cost reduction for software systems},
journal={2006 IEEE Conference on Cybernetics and Intelligent Systems},
year={2006},
art_number={4017806},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-42749107026&partnerID=40&md5=b3b616344fc7f0c3a6bccbb69e0d4fc0},
abstract={Quality, cost and delivery are the most crucial factors to be managed in developing software systems in the industry. The management of these software systems includes not only resolving the necessarily technical problems but also the organizational problems and the business problems. Companies depending upon their environments decide their own strategies to solve these problems. This paper describes the way we experimented in these several years to reduce the cost for software systems at the company to meet some software development solutions. The efforts of web-based application developments by the toolkit, the source code generator we developed, are evaluated. The toolkit extended to the software product line systems is described. And, the cost for the software life cycle is estimated. © 2006 IEEE.},
author_keywords={Program generator;  Software cost;  Software product line;  Web application},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Leach20081907,
author={Leach, R.J.},
title={Missed opportunities in software cost reduction},
journal={Annual Forum Proceedings - AHS International},
year={2008},
volume={3},
pages={1907-1919},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-50049097112&partnerID=40&md5=82f6c01e854b7e038c2b897d90066b18},
abstract={The author conducted a survey of the publicly available literature in English on helicopters and relevant software cost estimation as part of an Army-funded project. The purpose of the survey was to ascertain the state-of-the-art and the state-of-the-practice in these areas. We found some excellent work, especially in the area of open software architectures, but also missed opportunities in the areas of reuse, certification, and product line architectures. In this paper, we indicate some opportunities for software cost reduction that can be employed with little initial effort or project reporting overhead. The paper also suggests some ways in which these reductions can be incorporated into cost estimation models. Copyright © 2008 by the American Helicopter Society International, Inc. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Borjesson2011276,
author={Borjesson, E. and Feldt, R.},
title={Structuring software engineering case studies to cover multiple perspectives},
journal={SEKE 2011 - Proceedings of the 23rd International Conference on Software Engineering and Knowledge Engineering},
year={2011},
pages={276-281},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84855545397&partnerID=40&md5=5854879fb9ec1626eded5e40693b33d3},
abstract={Case studies are used in software engineering (SE) research for detailed study of phenomena in their real-world context. There are guidelines listing important factors to consider when designing case studies, but there is a lack of advice on how to structure the collected information and ensure its breadth. Without considering multiple perspectives, such as business and organization, there is a risk that too few perspectives are covered. The objective of this paper is to develop a framework to give structure and ensure breadth of a SE case study. For an analysis of the verification and validation practices of a Swedish software company we developed an analytical framework based on two dimensions. The matrix spanned by the dimensions (perspective and lime) helped structure data collection and connect different findings. A six-step process was defined to adapt and execute the framework at the company and we exemplify its use and describe its perceived advantages and disadvantages. The framework simplified the analysis and gave a broader understanding of the studied practices but there is a tradeoff with the depth of the results, making the framework more suitable for explorative, open-ended studies.},
author_keywords={Case study;  Empirical;  Framework;  Multi-perspective},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{America2012478,
author={America, P. and Van De Laar, P. and Muller, G.},
title={Experiences in evolvability research},
journal={Advanced Engineering Informatics},
year={2012},
volume={26},
number={3},
pages={478-486},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84863778767&partnerID=40&md5=b5a0dbe3cb98bb9ba42197b51fc831bb},
abstract={Many technical products and systems nowadays have functionality that is largely determined by software, so called software-intensive systems. The requirements for software-intensive systems change over time, causing the system to evolve. We define evolvability as the ability of the system to respond to such changes. Improving evolvability of software-intensive systems was the goal of the Darwin project. The vision of this project consisted of four cornerstones. In this paper we share the obtained experiences, insights, and results. We have collected some evidence that three of the vision's cornerstones, which are about knowledge, i.e., extracting knowledge, representing knowledge, and economic decision making, improve evolvability. The representation of knowledge in A3 architecture overviews is the result with the most evidence that it is useful in practice. © 2012 Elsevier Ltd. All rights reserved.},
author_keywords={Evolvability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2004,
title={Proceedings - 2004 International Symposium on Empirical Software Engineering, ISESE 2004},
journal={Proceedings - 2004 International Symposium on Empirical Software Engineering, ISESE 2004},
year={2004},
page_count={290},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-11244304384&partnerID=40&md5=b1ec2d1794465b1a0a51f571843724d5},
abstract={The proceedings contain 26 papers from 2004 International Symposium on Empirical Software Engineering, ISESE 2004. The topics discussed include: the influence of the level of abstraction on the evolvability of conceptual models of information systems; the architectural change process; tool-supported unobtrusive evaluation of software engineering process conformance; analyzing systems failures through the use of case histories; investigation of the active guidance factor in reading techniques for defect detection; a software product line life cycle cost estimation model; and towards evidence in software engineering.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{BenAbdallahBenLamine2005113,
author={Ben Abdallah Ben Lamine, S. and Labed Jilani, L. and Hajjami Ben Ghezala, H.},
title={Cost estimation for product line engineering using COTS components},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2005},
volume={3714 LNCS},
pages={113-123},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33646198972&partnerID=40&md5=764ac2d5abc5210034a3befffe3c8d50},
abstract={Economic models for reuse are very important to organizations aiming to develop software with large scale reuse approaches. In fact, the initial investment is so important that it can discourage managers to commit to those approaches. Thus, economic models can help them to assess the worthiness of such an investment. Product Line Engineering (PLE) seems to be an attractive reuse approach in matter of product quality and time-to-market. Using Commercial Off The Shelf (COTS) in a PLE approach may have a positive impact. This paper reports on the need for an economic model to quantify the predicted benefits of the PLE software development with the use of COTS components. We introduce a Model for Software Cost Estimation in a Product Line Engineering approach that we denote SoCoEMo-PLE 2. This latter includes the usage of COTS components. The potential benefits of the model are described. © Springer-Verlag Berlin Heidelberg 2005.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bin20051320,
author={Bin, X. and Mingkui, Y. and Hongbing, L. and Haibin, Z.},
title={Maximizing customer satisfaction in maintenance of software product family},
journal={Canadian Conference on Electrical and Computer Engineering},
year={2005},
volume={2005},
pages={1320-1323},
art_number={1557220},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33751311592&partnerID=40&md5=7b10e4dc2e8e144c22ce43463f89d303},
abstract={Customer satisfaction and cost are very important factors in software maintenance. However, the tradeoff between them is formidable in maintenance of software_product family. This paper suggests a decision tree method to improve multicustomer satisfaction adaptively with the available human resource. An experiment was conducted in the maintenance department of a Chinese software vendor who has provided more than 1000 hotels with its proprietary hotel information management system. The machine learning approach improved customer satisfaction with the same human resource cost. © 2005 IEEE.},
author_keywords={C4.5;  Cost estimation;  Customer satisfaction;  Decision tree;  ID3;  Machine learning},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kiebusch2006435,
author={Kiebusch, S. and Franczyk, B. and Speck, A.},
title={An unadjusted size measurement of embedded software system families and its validation},
journal={Software Process Improvement and Practice},
year={2006},
volume={11},
number={4},
pages={435-446},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33747117093&partnerID=40&md5=ef44fabd54f473dace3c81be432cd205},
abstract={Embedded software systems have become the driving force in many areas of technology, like the automotive industry. Functions for the control of cars, driver assistance as well as systems for information and entertainment are accomplished by software driven control units. Owing to the high complexity and development effort of embedded systems, these resources have to be reused. Software system families (SSF) are a promising solution to achieve cost reduction by reusing common software assets in different variants of an automobile. To support the economic management of this developmental approach, we need software metrics to estimate the effort of building embedded software system families. Techniques of size measurement and cost estimation for software system families are highly insufficient, in general, and do not exist for the automotive domain. Therefore, this article describes a conglomeration of innovative metrics to measure the size of a system family oriented software development. These size metrics analyze a real-time and a process focused perspective of embedded software system families in the automotive domain. A combination of both viewpoints describes the unadjusted size of software driven control units to indicate and estimate their development costs. Copyright © 2006 John Wiley & Sons, Ltd.},
author_keywords={Embedded systems;  Measurement;  Metrics;  Process;  Software system families},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhiqiao20123376,
author={Zhiqiao, W. and Kwong, C.K. and Tang, J. and Chan, J.W.K.},
title={Integrated model for software component selection with simultaneous consideration of implementation and verification},
journal={Computers and Operations Research},
year={2012},
volume={39},
number={12},
pages={3376-3393},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84863002969&partnerID=40&md5=172fb3e5a728e105429209d89a16625b},
abstract={One important objective of component-based software engineering is the minimization of the development cost of software products. Thus, the costs of software component implementation and verification, which may involve substantial expenses while under development, should be reduced. In addition, the costs for these processes should not be considered individually, but in an integrated manner, to further reduce development cost. In the current paper, an integrated decision model is proposed to assist decision-makers in selecting reuse scenarios for components used for implementation and in simultaneously determining the optimal number of test cases for verification. An objective of the model is the minimization of development cost, while satisfying the required system and reliability requirements. The Lagrange relaxation decomposition (LRD) method with heuristics was developed to solve integrated decision problems. Based on LRD, the nonlinear model is condensed into a 0-1 knapsack problem for the subproblem on reuse scenario selection and an integer knapsack problem for the subproblem on the determination of the optimal number of tests. Combined with the Lagrange multiplier-determined heuristic, the proposed algorithm can determine the global optimum solution. Simulations of varying sizes for problems and sensitivity analyses were conducted, and the results indicate that LRD is more effective than previous methods in determining global optimal solutions for the integrated decision problem. © 2012 Elsevier Ltd. All rights reserved.},
author_keywords={Component selection;  Component-based software engineering;  Lagrange relaxation;  Optimization model;  Reliability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Francalanci2008,
author={Francalanci, C. and Merlo, F.},
title={The impact of complexity on software design quality and costs: An exploratory empirical analysis of open source applications},
journal={16th European Conference on Information Systems, ECIS 2008},
year={2008},
page_count={12},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84870648497&partnerID=40&md5=5b278a02bb82b94941aaee0747aac771},
abstract={It is well known that complexity affects software development and maintenance costs. In the Open Source context, the sharing of development and maintenance effort among developers is a fundamental tenet, which can be thought as a driver to reduce the impact of complexity on maintenance costs. However, complexity is a structural property of code, which is not quantitatively accounted for in traditional cost models. This paper introduces the concept of functional complexity, which weights the well-established McCabe's cyclomatic complexity metric to the number of interactive functional elements that an application provides to users. Such metric is used to analyze how Open Source development costs are affected by complexity. Traditional cost models, like CoCoMo, do not take into account the impact of complexity in estimating costs by means of accurate indicators. In contrast, results show how a higher complexity is associated with a lower design quality of code, and, hence, higher maintenance costs. Consequently, results suggest that a reliable effort estimation should be based on a precise evaluation of software complexity. Analyses are based on quality, complexity, and maintenance effort data collected for 59 Open Source applications (corresponding to 906 versions) selected from the SourceForge.net repository.},
author_keywords={Costs and quality;  Open source software complexity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sarang2010569,
author={Sarang, N. and Sanglikar, M.A.},
title={Using decision structures for policy analysis in software product-line evolution - A case study},
journal={Advanced Techniques in Computing Sciences and Software Engineering},
year={2010},
pages={569-574},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84878592840&partnerID=40&md5=ea54ba3d655df1c42a1926384208cf54},
abstract={Project management decisions are the primary basis for project success (or failure). Mostly, such decisions are based on an intuitive understanding of the underlying software engineering and management process and have a likelihood of being misjudged. Our problem domain is product-line evolution. We model the dynamics of the process by incorporating feedback loops appropriate to two decision structures: staffing policy, and the forces of growth associated with long-term software evolution. The model is executable and supports project managers to assess the long-term effects of possible actions. Our work also corroborates results from earlier studies of E-type systems, in particular the FEAST project and the rules for software evolution, planning and management. © Springer Science+Business Media B.V. 2010.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mazhelis2013690,
author={Mazhelis, O. and Tyrväinen, P. and Frank, L.},
title={Vertical software industry evolution: The impact of software costs and limited customer base},
journal={Information and Software Technology},
year={2013},
volume={55},
number={4},
pages={690-698},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84875222060&partnerID=40&md5=29f53265c900351fcf7033a018e47269},
abstract={Context: Software systems are commonly used in a variety of industries as a means of automating organizational business processes. Initially, such software is often developed in-house by the vertical organizations possibly with the support of professional IT service providers; however, in many cases, internally developed software is eventually replaced with the software products provided by independent software vendors. These vendors often use license fees to recover their software development investments, as well as to gain some margin. However, if the vendor's customer base for a specific type of software is limited, then either the license fees are too high and hence the customers may prefer to develop the software internally, or the margin has to be decreased. As a result, the market for software products of that type may not materialize. Objective: The paper introduces an analytical model that defines the minimum number of customers that the software vendor should have for its software to be less expensive as compared to the in-house software. Method: Following a conceptual-analytical approach, a model is constructed wherein the minimum number of a vendor's customers is represented as a function of other factors affecting software development costs. This model is verified by applying it to estimate the minimum customer base in the segment of telecommunications billing mediation software. Results: Using the proposed analytical model, the minimum number of customers and the maximum number of software vendors in this segment are evaluated. The obtained results are found to be in line with the information available from a telecommunications software market database. Conclusions: Based on the model, a preliminary conclusion is made that in industries with high software development costs, heterogeneous legacy systems to integrate with, and a limited pool of potential customers, the number of software vendors is unlikely to be significant, and hence the in-house or custom-made software is unlikely to be superseded by the software products. © 2012 Elsevier B.V. All rights reserved.},
author_keywords={Customer base;  Software development costs;  Software industry;  Telecommunications OSS/BSS software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lettner2012171,
author={Lettner, D. and Vierhauser, M. and Rabiser, R. and Grünbacher, P.},
title={Supporting end users with business calculations in product configuration},
journal={ACM International Conference Proceeding Series},
year={2012},
volume={1},
pages={171-180},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84867478373&partnerID=40&md5=e36930fc1f34774113c482f848a63220},
abstract={Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation. Copyright © 2012 ACM.},
author_keywords={Business calculations;  Product configuration;  Variability models},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sun200715,
author={Sun, H. and Hauptman, M. and Lutz, R.},
title={Integrating product-line fault tree analysis into AADL models},
journal={Proceedings of IEEE International Symposium on High Assurance Systems Engineering},
year={2007},
pages={15-22},
art_number={4404723},
note={cited By (since 1996)9},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-48349130748&partnerID=40&md5=c75e3e80af736dc6daf58183cc3c3139},
abstract={Fault Tree Analysis (FTA) is a safety-analysis technique that has been extended recently to accommodate product-line engineering. This paper describes a tool-supported approach for integrating product-line FTA with the AADL (Architecture Analysis and Design Language) models and associated AADL Error Models for a product line. The AADL plug-in we have developed provides some automatic pruning and adaptation of the fault tree for a specific product from the product-line FTA. This work supports consistent reuse of the FTA across the systems in the product line and reduces the effort of maintaining traceability between the safety analysis and the architectural models. Incorporating the product-line FTA into the AADL models also allows derivation of basic quantitative and cut set analyses for each product-line member to help identify and eliminate design weaknesses. The tool-supported capabilities enable comparisons among candidate new members to assist in design decisions regarding redundancy, safety features, and the evaluation of alternative designs. Results from a small case study illustrate the approach. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Brosch20103,
author={Brosch, F. and Gitzel, R. and Koziolek, H. and Krug, S.},
title={Combining architecture-based software reliability predictions with financial impact calculations},
journal={Electronic Notes in Theoretical Computer Science},
year={2010},
volume={264},
number={1},
pages={3-17},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77955682717&partnerID=40&md5=2849b58cf2f6a34c007606c2af1dfc55},
abstract={Software failures can lead to substantial costs for the user. Existing models for software reliability prediction do not provide much insight into this financial impact. Our approach presents a first step towards the integration of reliability prediction from the IT perspective and the business perspective. We show that failure impact should be taken into account not only at their date of occurrence but already in the design stage of the development. First we model cost relevant business processes as well as the associated IT layer and then connect them to failure probabilities. Based on this we conduct a reliability and cost estimation. The method is illustrated by a case study. © 2010 Elsevier B.V.},
author_keywords={Cost;  Reliability;  Software failure},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sharma20131,
author={Sharma, S. and Wong, J. and Tim, U.S. and Gadia, S.},
title={Bidirectional migration between variability and commonality in product line engineering of smart homes},
journal={International Journal of Systems Assurance Engineering and Management},
year={2013},
volume={4},
number={1},
pages={1-12},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84874629337&partnerID=40&md5=e7e1d187e20f6ff923e4a033e61e5160},
abstract={The product line engineering (PLE) has proven to be the paradigm for developing diversified products and systems in shorter time, at lower cost, and higher quality. Ultimately the goal of PLE is to evolve a set of products that have both commonality and variations built into them, which allows a high degree of variability between the different products. The current trend in smart home deployment - based on commissions - guarantees a complete satisfaction for each inhabitant, as designed per resident basis. To reduce high deployment cost and to resolve the non-reusability of the resources, the product line engineering methodology can be employed in selecting or configuring the appropriate smart home technology. Within the smart homes and ambient assisted living, not only do the reusable features reduce cost, they also do not compromise with the guarantee of complete satisfaction of each resident as few senior inhabitants may impose severe constraints on the binding of few common features. In our previous work (Sharma et al., 9th International Conference on Smart Homes and Health Telematics (ICOST), 2011), we propose an approach called, wrenching that claims the best resident satisfaction at the reduced deployment cost. Today seniors are fast growing population globally (http://www.aoa.gov/ agingstatsdotnet/Main-Site/Data/2008) and the increased demand for smart homes in the near future is undeniable. Thus to provide assistance and independence to each senior resident of a smart home, reduction in time to market (Wohlin and Ahlgren, Softw Qual J 4:189-205, 1995) of appropriate assistive smart home technology becomes an essential consideration. Long term analysis of variability binding can help to analyze that few variants are bound more often over others. Respecting the satisfaction guarantee, the highly demanded variants can be permanently migrated into commonality, advocating the reusability. The improved reusability of features (e.g., sensors) not only enhances economy of scale but also time to market. The migration of a feature from variability to commonality is known as realizing. © 2012 The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.},
author_keywords={Commonality;  Customization;  Deployment;  EOC;  Product line engineering;  Realizing;  Smart home;  Smart variability model;  TTM;  Variability;  Wrenching},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2009,
title={International Conference on Software Engineering Theory and Practice 2009, SETP 2009},
journal={International Conference on Software Engineering Theory and Practice 2009, SETP 2009},
year={2009},
page_count={235},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84878227295&partnerID=40&md5=e8a90a79c0c9e9bfd99f053aea6237fe},
abstract={The proceedings contain 31 papers. The topics discussed include: a study to observe relations between software engineers' responses to incomplete requirements and requirements volatility; software product line architecture for distributed real-time systems; Hume cost analyses for imperative programs; knowledge-based method for conversion and reuse of identical and different types of diagrams; designing change request forms for better effort estimates on requirements changes; analysis and measurement of software components' non-functional specifications; recovering design artifacts from legacy systems using automata theory and formal specifications; critical success factors in software process improvement: case of Finnish software company; software cost estimation ? an experimental approach; an effort based model of software usability; and a short comparison of tasks and achievements of different groups of students with the common software engineering course.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{NoAuthor2011,
title={Software Engineering Techniques - Third IFIP TC 2 Central and East European Conference, CEE-SET 2008, Revised Selected Papers},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={4980 LNCS},
page_count={290},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80053134182&partnerID=40&md5=5214039ca67d121da320439c427f907b},
abstract={The proceedings contain 21 papers. The topics discussed include: automated generation of implementation from textual system requirements; mining design patterns from existing projects using static and run-time analysis; transformational design of business processes for SOA; SMA-the Smyle modeling approach; open work of two-hemisphere model transformation definition into UML Class diagram in the context of MDA; HTCPNs-based tool for web-server clusters development; refactoring the documentation of software product lines; advanced data organization for java-powered mobile devices; developing applications with aspect-oriented change realization; exploratory comparison of expert and novice pair programmers; state of the practice in software effort estimation: a survey and literature review; a framework for defect prediction in specific software project contexts; and meeting organisational needs and quality assurance through balancing agile and formal usability testing results.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Leitner2010,
author={Leitner, A. and Kreiner, C.},
title={Managing ERP configuration variants: An experience report},
journal={Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering, KOPLE'10},
year={2010},
art_number={1964140},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79955125676&partnerID=40&md5=bf24f46c47b6393c85a5103c833d5356},
abstract={The concepts of Software Product Line Engineering (SPLE) have been adapted and applied to enterprise IT systems, in particular the ERP systems of a production company. Based on a 2-layer feature model for the domain of the company's business processes, individual, albeit similar division's ERP system configurations can be derived by feature selection forming a variant description model. It is indicated that regular release upgrades can also benefit from the SPLE approach. The customization capabilities of the ERP platform are captured in another model; building up this model is automated according to information extracted online. As well, customizing an ERP system - based on the models mentioned - is performed online with the help of a connector developed in this project. Quantitative analysis and lessons learned during the project conclude this experience report. © 2010 ACM.},
author_keywords={enterprise resource planning;  experience report;  IT management;  software product line engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2011,
title={PROMISE 2011 - 7th International Conference on Predictive Models in Software Engineering, Co-located with ESEM 2011},
journal={ACM International Conference Proceeding Series},
year={2011},
page_count={144},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80054060912&partnerID=40&md5=291ee0355bc02070a1dab8871f2416ae},
abstract={The proceedings contain 16 papers. The topics discussed include: does measuring code change improve fault prediction?; an analysis of trends in productivity and cost drivers over years; handling missing data in software effort prediction with naive bayes and em algorithm; failure is a four-letter word: a parody in empirical research; software effort estimation based on optimized model tree; are change metrics good predictors for an evolving software product line?; detecting bug duplicate reports through local references; a principled evaluation of ensembles of learning machines for software effort estimation; nothing else matters: what predictive model should i use?; studying the fix-time for bugs in large open source projects; selecting discriminating terms for bug assignment: a formal analysis; and empirical validation of human factors in predicting issue lead time in open source projects.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Genero201146,
author={Genero, M. and Fernández-Saez, A.M. and James Nelson, H. and Poels, G. and Piattini, M.},
title={A systematic literature review on the quality of UML models},
journal={Journal of Database Management},
year={2011},
volume={22},
number={3},
pages={46-66},
note={cited By (since 1996)4},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80052903126&partnerID=40&md5=0110875f6b322d0280483fd36d02275e},
abstract={The quality of conceptual models directly affects the quality of the understanding of the application domain and the quality of the final software products that are ultimately based on them. This paper describes a systematic literature review (SLR) of peer-reviewed conference and journal articles published from 1997 through 2009 on the quality of conceptual models written in UML, undertaken to understand the state-of-the-art, and then identify any gaps in current research. Six digital libraries were searched, and 266 papers dealing specifically with the quality of UML models were identified and classified into five dimensions: type of model quality, type of evidence, type of research result, type of diagram, and research goal. The results indicate that most research focuses on semantic quality, with relatively little on semantic completeness; as such, this research examines new modeling methods vs. quality frameworks and metrics, as well as quality assurance vs. understanding quality issues. The results also indicate that more empirical research is needed to develop a theoretical understanding of conceptual model quality. The classification scheme developed in this paper can serve as a guide for both researchers and practitioners. Copyright © 2011, IGI Global.},
author_keywords={Conceptual model quality;  Conceptual models;  Software;  Systematic literature review;  Unified modeling language (UML)},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Gerlec2010,
author={Gerlec, C. and Krajnc, A. and Goljat, U.},
title={A method for evaluating the actual functional size in model driven software development},
journal={ICSTE 2010 - 2010 2nd International Conference on Software Technology and Engineering, Proceedings},
year={2010},
volume={1},
pages={V1185-V1189},
art_number={5608886},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78649993675&partnerID=40&md5=03c174b5a48186384560743bc8c3b959},
abstract={Accurate functional size measurement is essential for a software project's success. A modern software development approach uses many additional tools that increase developers' productivity. The model driven software development approach (MDSD) uses engines for the generation of some parts of a system and its functionalities. Furthermore, there is a gap between the origin functional size and actual functional size that has to be implemented by developers. In this paper, a method for evaluating the actual development functional size in the MDSD approach has been proposed and tested on industry projects. © 2010 IEEE.},
author_keywords={Function points analysis;  Functional size measurement;  Model driven development;  Project size estimation;  Source code generation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Egyed2011188,
author={Egyed, A.},
title={Automatically detecting and tracking inconsistencies in software design models},
journal={IEEE Transactions on Software Engineering},
year={2011},
volume={37},
number={2},
pages={188-203},
art_number={5432227},
note={cited By (since 1996)9},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79953196410&partnerID=40&md5=58440a8fcc5c610d14c30354117a807f},
abstract={Software models typically contain many inconsistencies and consistency checkers help engineers find them. Even if engineers are willing to tolerate inconsistencies, they are better off knowing about their existence to avoid follow-on errors and unnecessary rework. However, current approaches do not detect or track inconsistencies fast enough. This paper presents an automated approach for detecting and tracking inconsistencies in real time (while the model changes). Engineers only need to define consistency rules - in any language - and our approach automatically identifies how model changes affect these consistency rules. It does this by observing the behavior of consistency rules to understand how they affect the model. The approach is quick, correct, scalable, fully automated, and easy to use as it does not require any special skills from the engineers using it. We evaluated the approach on 34 models with model sizes of up to 162,237 model elements and 24 types of consistency rules. Our empirical evaluation shows that our approach requires only 1.4 ms to reevaluate the consistency of the model after a change (on average); its performance is not noticeably affected by the model size and common consistency rules but only by the number of consistency rules, at the expense of a quite acceptable, linearly increasing memory consumption. © 2006 IEEE.},
author_keywords={design;  Design tools and techniques},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mao2006136,
author={Mao, C. and Lu, Y. and Wang, X.},
title={A study on the distribution and cost prediction of requirements changes in the software life-cycle},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={3840 LNCS},
pages={136-150},
note={cited By (since 1996)2},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33745166925&partnerID=40&md5=130dc8da2f76b638468b56c3a0234ce3},
abstract={Software development is a dynamic process. Requirements change (RC) is inevitable and brings great challenges to the software development. How to precisely predict requirements change is especially important in the field of requirements engineering. In this paper, an assessment framework for the factors of RCs' distribution is constructed firstly. Apart from the rough prediction method based on the statistic process control of RCs, an artificial neural network method for predicting RCs' distribution is presented. In this case, the weight of each factor is calculated by a fuzzy logic method, called experts ranking. Furthermore, we propose a model to pre-evaluate the cost caused by RCs. With some practical projects data, a validation experiment has been drawn, whose result shows that our method and model are practical and efficient to predict the distribution and cost of RCs. © Springer-Verlag Berlin Heidelberg 2005.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Korkmaz2007264,
author={Korkmaz, M. and Mili, A.},
title={A product line of software reuse cost models},
journal={ICSOFT 2007 - 2nd International Conference on Software and Data Technologies, Proceedings},
year={2007},
volume={SE},
pages={264-269},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-67650258049&partnerID=40&md5=af198d6be482fd62ba3cfa3f4d952944},
abstract={In past work, we had proposed a software reuse cost model that combines relevant stakes and stakeholders in an integrated ROI-based model. In this paper we extend our earlier work in two directions: conceptually, by capturing aspects of the model that were heretofore unaccounted for; practically, by proposing a product line that supports a wide range of cost modeling applications.},
author_keywords={A software reuse cost model;  Non-linear optimization problem;  Product line tool;  Quantify time-to-market gains;  ROI functions},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2010,
title={Proceedings of the IASTED International Conference on Software Engineering, SE 2010},
journal={Proceedings of the IASTED International Conference on Software Engineering, SE 2010},
year={2010},
page_count={263},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77954649828&partnerID=40&md5=a480287d5f005c371f3db6790d3d1c43},
abstract={The proceedings contain 34 papers. The topics discussed include: improving software development processes in small companies: a case study; analyzing impact of change in UML sequence diagrams on state machine based regression testing; updating imperatively defined model transformations by merging paths; performance comparison for effort estimation models with log-normal and gamma distributions; testing embedded systems software using open source virtual platforms; transformation to formalized business process model; experiences initiating software product line engineering in small teams with pulse; a new constructive model for reengineering software systems; expressive power of a new iconic visual query language for mobile GIS; query formulation of a visual query language for mobile GIS; a validation of refactorings impact on maintainability: a case study; and incorporating usability in the software development process.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Choi2012406,
author={Choi, S. and Park, S. and Sugumaran, V.},
title={A rule-based approach for estimating software development cost using function point and goal and scenario based requirements},
journal={Expert Systems with Applications},
year={2012},
volume={39},
number={1},
pages={406-418},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-81855175933&partnerID=40&md5=bcbdbd5eef08d0cfaf802c786db0ea33},
abstract={Function point is a method used to measure software size and estimate the development cost. However, for large complex systems, cost estimation is difficult because of the large number of requirements expressed in natural language. In this paper we propose a rule-based approach for estimating software development cost in the requirements analysis phase. It combines goal and scenario based requirements analysis with function point based cost estimation. In our proposed approach, Context Analysis Guiding rules, Data Function Extraction Guiding rules, and Transaction Function Extraction Guiding rules have been developed to identify function points from text based goal and scenario descriptions. These rules are established based on a linguistic approach. The contribution of the proposed approach is to help project managers decide which requirements should be realized. © 2011 Elsevier Ltd. All rights reserved.},
author_keywords={Cost estimation;  Function point;  Goal;  Project management;  Requirements triage;  Scenario},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mahmood2005693,
author={Mahmood, S. and Lai, R. and Kim, Y.S. and Kim, J.H. and Park, S.C. and Oh, H.S.},
title={A survey of component based system quality assurance and assessment},
journal={Information and Software Technology},
year={2005},
volume={47},
number={10},
pages={693-707},
note={cited By (since 1996)19},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-21244433139&partnerID=40&md5=c66b066111952d7772927356a159684e},
abstract={Component Based Software Development (CBSD) is focused on assembling existing components to build a software system, with a potential benefit of delivering quality systems by using quality components. It departs from the conventional software development process in that it is integration centric as opposed to development centric. The quality of a component based system using high quality components does not therefore necessarily guarantee a system of high quality, but depends on the quality of its components, and a framework and integration process used. Hence, techniques and methods for quality assurance and assessment of a component based system would be different from those of the traditional software engineering methodology. It is essential to quantify factors that contribute to the overall quality, for instances, the trade off between cost and quality of a component, analytical techniques and formal methods, and quality attribute definitions and measurements. This paper presents a literature survey of component based system quality assurance and assessment; the areas surveyed include formalism, cost estimation, and assessment and measurement techniques for the following quality attributes: performance, reliability, maintainability and testability. The aim of this survey is to help provide a better understanding of CBSD in these aspects in order to facilitate the realisation of its potential benefits of delivering quality systems. © 2005 Elsevier B.V. All rights reserved.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Martens201084,
author={Martens, A. and Ardagna, D. and Koziolek, H. and Mirandola, R. and Reussner, R.},
title={A hybrid approach for multi-attribute QoS optimisation in component based software systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6093 LNCS},
pages={84-101},
note={cited By (since 1996)4},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77954611347&partnerID=40&md5=93bdb3f4975fdfe6b9c4d4914e5f3dae},
abstract={Design decisions for complex, component-based systems impact multiple quality of service (QoS) properties. Often, means to improve one quality property deteriorate another one. In this scenario, selecting a good solution with respect to a single quality attribute can lead to unacceptable results with respect to the other quality attributes. A promising way to deal with this problem is to exploit multi-objective optimization where the objectives represent different quality attributes. The aim of these techniques is to devise a set of solutions, each of which assures a trade-off between the conflicting qualities. To automate this task, this paper proposes a combined use of analytical optimization techniques and evolutionary algorithms to efficiently identify a significant set of design alternatives, from which an architecture that best fits the different quality objectives can be selected. The proposed approach can lead both to a reduction of development costs and to an improvement of the quality of the final system. We demonstrate the use of this approach on a simple case study. © 2010 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Unterkalmsteiner2012398,
author={Unterkalmsteiner, M. and Gorschek, T. and Islam, A.K.M.M. and Cheng, C.K. and Permadi, R.B. and Feldt, R.},
title={Evaluation and measurement of software process improvement-A systematic literature review},
journal={IEEE Transactions on Software Engineering},
year={2012},
volume={38},
number={2},
pages={398-424},
art_number={5728832},
note={cited By (since 1996)5},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84859747286&partnerID=40&md5=a322ad836a1731f0abd9a1d0c991c660},
abstract={BACKGROUND-Software Process Improvement (SPI) is a systematic approach to increase the efficiency and effectiveness of a software development organization and to enhance software products. OBJECTIVE-This paper aims to identify and characterize evaluation strategies and measurements used to assess the impact of different SPI initiatives. METHOD-The systematic literature review includes 148 papers published between 1991 and 2008. The selected papers were classified according to SPI initiative, applied evaluation strategies, and measurement perspectives. Potential confounding factors interfering with the evaluation of the improvement effort were assessed. RESULTSSeven distinct evaluation strategies were identified, wherein the most common one, Pre-Post Comparison, was applied in 49 percent of the inspected papers. Quality was the most measured attribute (62 percent), followed by Cost (41 percent), and Schedule (18 percent). Looking at measurement perspectives, Project represents the majority with 66 percent. CONCLUSION-The evaluation validity of SPI initiatives is challenged by the scarce consideration of potential confounding factors, particularly given that Pre-Post Comparison was identified as the most common evaluation strategy, and the inaccurate descriptions of the evaluation context. Measurements to assess the short and mid-term impact of SPI initiatives prevail, whereas long-term measurements in terms of customer satisfaction and return on investment tend to be less used. © 1976-2012 IEEE.},
author_keywords={metrics/measurement;  Process implementation and change;  process measurement;  systematic literature review},
document_type={Review},
source={Scopus},
}

@CONFERENCE{NoAuthor2007,
title={QUATIC 2007 - 6th International Conference on the Quality of Information and Communications Technology},
journal={QUATIC 2007 - 6th International Conference on the Quality of Information and Communications Technology},
year={2007},
page_count={240},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-48249110344&partnerID=40&md5=61bde93bc3a0ce84f21dec151d4b719a},
abstract={The proceedings contain 24 papers. The topics discussed include: a vision for international standardization in software and systems engineering; an SEI process improvement path to software quality; a practical model for measuring maintainability; towards paradigm-independent software assessment; modeling the experimental software engineering process; MPS model-based software acquisition process improvement in Brazil; a scalable quality of service middleware system with passive monitoring agents over wireless video transmission; a probabilistic approach to web portal's data quality evaluation; towards the support of contextual information to a measurement and evaluation framework; a nationwide program for software process improvement in Brazil; lessons learned and results from applying date-driven cost estimation to industrial data sets; and model driven development of software product lines.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Capra2007395,
author={Capra, E. and Francalanci, C. and Merlo, F.},
title={The economics of open source software: An empirical analysis of maintenance costs},
journal={IEEE International Conference on Software Maintenance, ICSM},
year={2007},
pages={395-404},
art_number={4362652},
note={cited By (since 1996)9},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-47349083517&partnerID=40&md5=6234c3b956817f10b8f26da0c553776b},
abstract={A quality degradation effect ofpmprietary code has been observed as a consequence of maintenance. This quality degradation effect, called entropy, is a cause for higher maintenance costs. In the Open Source context, the quality of code is a fundamental tenet of open software developers. As a consequence, the quality degradation principle measured by entmpy cannot be assumed to be valid. The goal of the paper is to analyze the entropy of Open Source applications by measuring the evolution of maintenance costs over time. Analyses are based on cost data collected from a sample of 1251 Open Source application versions, compared with the costs estimated with a traditional model for proprietary software. Findings indicate that Open Source applications are less subject to entropy, have lower maintenance costs and also a lower need for maintenance interventions aimed at restoring quality. Finally, results show that a lower entropy is favored by greater functional simplicity. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schmid200432,
author={Schmid, K.},
title={A quantitative model of the value of architecture in product line adoption},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3014},
pages={32-43},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-35048850424&partnerID=40&md5=ec906910a7db935610f35876a2daf324},
abstract={Product line adoption is a key issue in product line development, as the right adoption approach is central to the overall success of product line de velopment. Thus, this is a strongly discussed area of product line engineering. While so far, guidelines and experiences on the best approach to product line adoption have been presented, no detailed quantitative model was provided. In this paper we present a quantitative model of the product line adoption problem. From this model we deduce general guidelines for product line adoption, particularly highlighting the role of the architecture in the cost-effective adoption of a product line. © Springer-Verlag Berlin Heidelberg 2004.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Alves2010710,
author={Alves, N. and Carvalho, W. and Lamounier, E.},
title={Scrum and plan-driven process integration and its impact on effort estimation},
journal={SEKE 2010 - Proceedings of the 22nd International Conference on Software Engineering and Knowledge Engineering},
year={2010},
pages={710-715},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79952413669&partnerID=40&md5=5c80b238a8579877880d878b5d9d53ad},
abstract={This paper analyses how the effort estimation of software development projects is impacted by integrating Scrum practices in traditional plan-driven process. A case study was carried out in a company in which several projects were analyzed according to a set of metrics. We made a comparison between a set of projects executed by a plan-driven process and another set of projects executed by the same plan-driven process streamlined using Scrum practices. We discuss the case study findings on effort estimation, which was characterized by recurring project overestimation on those projects in which Scrum practices were introduced.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mili2001175,
author={Mili, A. and Chmiel, S.F. and Gottumukkala, R. and Zhang, L.},
title={Managing Software Reuse Economics: An Integrated ROI-based Model},
journal={Annals of Software Engineering},
year={2001},
volume={11},
number={1},
pages={175-218},
note={cited By (since 1996)6},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0035565426&partnerID=40&md5=93d9ebddc934c1860c6a1f85eb9a245a},
abstract={Several cost models have been proposed in the past for estimating, predicting, and analyzing the costs of software reuse. In this paper we propose an integrated ROI-based cost model which attempts to encompass existing models. Our approach is to analyze existing models, identify their dimensions of variance, classify the models along these dimensions, then provide an integrated cost model that makes explicit provisions for these dimensions of variance. In this paper, we also discuss in what sense our model encompasses existing models, present a prototype that supports the cost model, and illustrate the model with a sample example.},
author_keywords={Application engineering;  COCOMO;  Component engineering;  Domain engineering;  Return on investment;  Software cost estimation;  Software reuse},
document_type={Article},
source={Scopus},
}

@ARTICLE{Daramola2010521,
author={Daramola, O.J.},
title={A process framework for semantics-aware tourism information systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6385 LNCS},
pages={521-532},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78649805138&partnerID=40&md5=07bd354a9b898e216f71b7903c06c6cb},
abstract={The growing sophistication of user requirements in tourism due to the advent of new technologies such as the Semantic Web and mobile computing has imposed new possibilities for improved intelligence in Tourism Information Systems (TIS). Traditional software engineering and web engineering approaches cannot suffice, hence the need to find new product development approaches that would sufficiently enable the next generation of TIS. The next generation of TIS are expected among other things to: enable semantics-based information processing, exhibit natural language capabilities, facilitate inter-organization exchange of information in a seamless way, and evolve proactively in tandem with dynamic user requirements. In this paper, a product development approach called Product Line for Ontology-based Semantics-Aware Tourism Information Systems (PLOSATIS) which is a novel hybridization of software product line engineering, and Semantic Web engineering concepts is proposed. PLOSATIS is presented as potentially effective, predictable and amenable to software process improvement initiatives. © 2010 Springer-Verlag.},
author_keywords={Product line;  Semantic Web;  Software Process Improvement;  Software product development;  Tourism Information System;  Web engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schmid2002593,
author={Schmid, K.},
title={A comprehensive product line scoping approach and its validation},
journal={Proceedings - International Conference on Software Engineering},
year={2002},
pages={593-603},
note={cited By (since 1996)48},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0036036581&partnerID=40&md5=17dcd87bb247802df9a93d4adc64a17d},
abstract={Product Line Engineering is a recent approach to software development that specifically aims at exploiting commonalities and systematic variabilities among functionally overlapping systems in terms of large scale reuse. Taking full advantage of this potential requires adequate planning and management of the reuse approach as otherwise huge economic benefits will be missed due to an inappropriate alignment of the reuse infrastructure. Key in product line planning is the scoping activity, which aims at focussing the reuse investment where it pays. Scoping actually happens on several levels in the process: during the domain analysis step (analysis of product line requirements) a focusing needs to happen just like during the decision of what to implement for reuse. The latter decision has also important ramifications for the development of an appropriate reference architecture as it provides the reusability requirements for this step. In this paper, we describe an integrated approach that has been developed, improved, and validated over the last few years. The approach fully covers the scoping activities of domain scoping and reuse infrastructure scoping and was validated in several industrial case studies.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tomer2004601,
author={Tomer, A. and Goldin, L. and Kuflik, T. and Kimchi, E. and Schach, S.R.},
title={Evaluating software reuse alternatives: A model and its application to an industrial case study},
journal={IEEE Transactions on Software Engineering},
year={2004},
volume={30},
number={9},
pages={601-612},
note={cited By (since 1996)28},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-4644266052&partnerID=40&md5=c661403d7e0a01b926a5e88f5623b705},
abstract={We propose a model that enables software developers to systematically evaluate and compare all possible alternative reuse scenarios. The model supports the clear identification of the basic operations involved and associates a cost component with each basic operation in a focused and precise way. The model is a practical tool that assists developers to weigh and evaluate different reuse scenarios, based on accumulated organizational data, and then to decide which option to select in a given situation. The model is currently being used at six different companies for cost-benefit analysis of alternative reuse scenarios; we give a case study that illustrates how it has been used in practice. © 2004 IEEE.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bunse2007121,
author={Bunse, C. and Gross, H.-G. and Peper, C.},
title={Applying a model-based approach for embedded system development},
journal={EUROMICRO 2007 - Proceedings of the 33rd EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2007},
year={2007},
pages={121-128},
art_number={4301072},
note={cited By (since 1996)10},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-48049121916&partnerID=40&md5=17fea8ee82a6d51775f74e60edf2b031},
abstract={Model-based and component-oriented software development approaches are slowly superseding traditional ways of developing embedded systems. For investigating to which extent model-based development is feasible for embedded system development, we conducted a case study in which a small embedded system is developed using the MARMOT approach. In order to evaluate the degree of reuse that might be achieved, the components of the case study are used in the context of different small projects. Several aspects of reuse, application size, ease of adaptation, and development effort are quantified. This analysis reveals that model-based and component-oriented development performs well for small embedded systems, and it leads to the conclusion that model-driven/ cornponent-based development of embedded systems enables adaptable applications with higher-than-normal reuse rate. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li2013777,
author={Li, Z. and Liang, P. and Avgeriou, P.},
title={Application of knowledge-based approaches in software architecture: A systematic mapping study},
journal={Information and Software Technology},
year={2013},
volume={55},
number={5},
pages={777-794},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84875214360&partnerID=40&md5=6b620704b8852ac57288f15d5e35c03c},
abstract={Context: Knowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture. Objective: This work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classification and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions. Method: A systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011. Results: Fifty-five studies were selected and classified according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation. Conclusions: The study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which "Embedded software" has received the most attention. © 2012 Elsevier B.V. All rights reserved.},
author_keywords={Architecting activity;  Knowledge-based approach;  Software architecture;  Systematic mapping study},
document_type={Review},
source={Scopus},
}

@ARTICLE{Verhoef2002,
author={Verhoef, C.},
title={Quantitative IT portfolio management},
journal={Science of Computer Programming},
year={2002},
volume={45},
number={1},
page_count={96},
note={cited By (since 1996)49},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0036776630&partnerID=40&md5=2bcf61c73a4ee7e9c36959df241a1ab6},
abstract={We present a quantitative approach for IT portfolio management. This is an approach that CMM level 1 organizations can use to obtain a corporate wide impression of the state of their total IT portfolio, how IT costs spent today project into the budgets of tomorrow, how to assess important risks residing in an IT portfolio, and to explore what-if scenarios for future IT investments. Our quantitative approach enables assessments of proposals from business units, risk calculations, cost comparisons, estimations of TCO of entire IT portfolios, and more. Our approach has been applied to several organizations with annual multibillion dollar IT budgets each, and has been instrumental for executives in coming to grips with the largest production factor in their organizations: information technology. © 2002 Elsevier Science B.V. All rights reserved.},
author_keywords={Cost allocation distribution;  Cost allocation formulas;  IT portfolio;  IT portfolio database;  IT portfolio exposure;  IT portfolio management;  IT portfolio risk;  Operational cost tsunami;  Rayleigh distribution;  ROI threshold quavering;  Seismic IT impulse},
document_type={Review},
source={Scopus},
}

@ARTICLE{Engel2003135,
author={Engel, A. and Barad, M.},
title={A methodology for modeling VVT risks and costs},
journal={Systems Engineering},
year={2003},
volume={6},
number={3},
pages={135-151},
note={cited By (since 1996)13},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-1842754758&partnerID=40&md5=fdb0ed3a2600adadb7933f67661a1951},
abstract={The cost of large systems' Verification, Validation, and Testing (VVT) is in the neighborhood of 40% of the total life cycle cost. The cost associated with systems' failures is even more dramatic, often exceeding 10% of industrial organizations turnover. There is a great potential benefit in streamlining and optimizing the VVT process. The first step in accomplishing this aim is to define a VVT strategy and then to quantify the cost and risk associated with carrying it out. This paper provides an overview of the methodologies for risk and cost monitoring for VVT and proposes a novel approach for modeling VVT strategies as decision problems. A quantitative VVT process and risk model is proposed. Due to the nondeterministic nature of risk, simulation is used to generate distributions of possible costs, schedules, and risk outcomes. These distributions represent a probabilistic approach and are analyzed in relation to impact events. The model provides means to explore different VVT strategies for optimizing relevant decision parameters. To demonstrate the proposed procedure the paper describes a case study depicting a planned avionics suite upgrade program for a fighter aircraft. Some simplified partial quantitative results are also presented. © 2003 Wiley Periodicals, Inc.},
author_keywords={Cost;  CVM;  Life cycle;  Risk;  Systems;  Testing;  Validation;  Verification;  VVT},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lagerström2013285,
author={Lagerström, R. and Sporrong, U. and Wall, A.},
title={Increasing software development efficiency and maintainability for complex industrial systems - A case study},
journal={Journal of software: Evolution and Process},
year={2013},
volume={25},
number={3},
pages={285-301},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84879695237&partnerID=40&md5=d79f61df8cce71a28a6f218b63687021},
abstract={It is difficult to manage complex software systems. Thus, many research initiatives focus on how to improve software development efficiency and maintainability. However, the trend in the industry is still alarming, software development projects fail, and maintenance is becoming more and more expensive. One problem could be that research has been focusing on the wrong things. Most research publications address either process improvements or architectural improvements. There are few known approaches that consider how architectural changes affect processes and vice versa. One method proposed, called the Business-Architecture-Process method, takes these aspects into consideration. In 2007 the method was tested in one case study. Findings in the 2007 case study show that the method is useful, but in need of improvements and further validation. The present paper employs the method in a second case study. The contribution in this paper is thus a second test and validation of the proposed method, and useful method improvements for future use of the method. Copyright © 2011 John Wiley &Sons, Ltd.},
author_keywords={Architectural change;  Method support;  Process change;  Software development efficiency;  Software maintainability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Spoelstra2011315,
author={Spoelstra, W. and Iacob, M. and Van Sinderen, M.},
title={Software reuse in agile development organizations: A conceptual management tool},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2011},
pages={315-322},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79959325951&partnerID=40&md5=4e6187407eb6c664d60b07ed7d2617b3},
abstract={The reuse of knowledge is considered a major factor for increasing productivity and quality. In the software industry knowledge is embodied in software assets such as code components, functional designs and test cases. This kind of knowledge reuse is also referred to as software reuse. Although the benefits can be substantial, software reuse has never reached its full potential. Organizations are not aware of the different levels of reuse or do not know how to address reuse issues. This paper proposes a conceptual management tool for supporting software reuse. Furthermore the paper presents the findings of the application of the management tool in an agile development organization. © 2011 ACM.},
author_keywords={agile development;  assessment method;  knowledge management;  maturity levels;  reuse factors;  reuse maturity model;  software reuse},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Barbagallo2008,
author={Barbagallo, D. and Francalanci, C. and Merlo, F.},
title={The impact of social networking on software design quality and development effort in open source projects [L'impact des réseaux sociaux sur la qualité de la conception logicielle et l'effort de développement dans les projets de logiciels libres]},
journal={ICIS 2008 Proceedings - Twenty Ninth International Conference on Information Systems},
year={2008},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84870970367&partnerID=40&md5=fdb5d6f3f43f4e6c3b123a545e84ba12},
abstract={This paper focuses on Open Source (OS) social networks. The literature indicates that OS networks have a few nodes with a number of relationships significantly higher than the network's average, called hubs. It also provides numerous metrics that help verify whether a node is a hub, called centrality metrics. This paper posits that higher values of centrality metrics are positively correlated with project success. Second, it posits that higher values of centrality metrics are positively correlated with the ability of a project to attract new contributions. Third, it posits that projects with greater success have a lower software design quality. Hypotheses are tested on a sample of 56 applications written in Java from the SourceForge.net online OS repository. The corresponding social network is built by considering all the contributors, both developers and administrators, of our application sample and all contributors directly or indirectly connected with them within SourceForge.net, with a total of 57,142 nodes. Empirical results support our hypotheses, indicating that centrality metrics are significant drivers of project success that should be monitored from the perspective of a project administrator or team manager. However, they also prove that successful projects tend to have a significantly lower design quality of software. This has a number of consequences that could be visible to users and cause negative feedback effects over time.},
author_keywords={Network centrality metrics;  Open source;  Social networks;  Software design quality;  Software development effort},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ray2011277,
author={Ray, A. and Ackermann, C. and Cleaveland, R. and Shelton, C. and Martin, C.},
title={Functional and Nonfunctional Design Verification for Embedded Software Systems},
journal={Advances in Computers},
year={2011},
volume={83},
pages={277-321},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79955613204&partnerID=40&md5=d9c2e56874dcf4f56c19fb47a0bdfa60},
abstract={In model-based design verification, software models are checked against functional and nonfunctional requirements. Many of the mathematically well-founded theories developed for functional verification suffer from limitations (poor integration with industrial modeling tools and inability to scale to larger, more complicated systems) that restrict their practical adoption. Nonfunctional verification approaches, because of their inherent subjectivity ("what does it mean for a system to be modifiable?"), remain largely ad hoc and manual. There is also another overarching problem in that functional and nonfunctional verification take place on widely different representations of the system making it difficult to ensure that these representations are consistent with each other. This chapter highlights the latest research done into (1) a practical, formal, coverage-based functional verification approach called instrumentation-based verification, (2) quality attribute reasoning, a semiautomated, nonfunctional verification theory, and (3) an integrated functional and nonfunctional verification approach for model-based development. © 2011 Elsevier Inc.},
author_keywords={Embedded software;  Functional and non-functional verification of software;  Model-based engineering;  Model-based testing;  Software engineering},
document_type={Book},
source={Scopus},
}

@ARTICLE{Daneva20131333,
author={Daneva, M. and Van Der Veen, E. and Amrit, C. and Ghaisas, S. and Sikkel, K. and Kumar, R. and Ajmeri, N. and Ramteerthkar, U. and Wieringa, R.},
title={Agile requirements prioritization in large-scale outsourced system projects: An empirical study},
journal={Journal of Systems and Software},
year={2013},
volume={86},
number={5},
pages={1333-1353},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84875238853&partnerID=40&md5=d53f2fc8948594c1b33032a4c07c1b1c},
abstract={The application of agile practices for requirements prioritization in distributed and outsourced projects is a relatively recent trend. Hence, not all of its facets are well-understood. This exploratory study sets out to uncover the concepts that practitioners in a large software organization use in the prioritization process and the practices that they deem good. We seek to provide a rich analysis and a deep understanding of three cases in an exploratory study that was carried out in a large and mature company, widely recognized for its excellence and its engagement in outsourced software development. We used in-depth interviews for data collection and grounded theory techniques for data analysis. Our exploration efforts yielded the following findings: (i) understanding requirements dependencies is of paramount importance for the successful deployment of agile approaches in large outsourced projects. (ii) Next to business value, the most important prioritization criterion in the setting of outsourced large agile projects is risk. (iii) The software organization has developed a new artefact that seems to be a worthwhile contribution to agile software development in the large: 'delivery stories', which complement user stories with technical implications, effort estimation and associated risk. The delivery stories play a pivotal role in requirements prioritization. (iv) The vendor's domain knowledge is a key asset for setting up successful client-developer collaboration. (v) The use of agile prioritization practices depends on the type of project outsourcing arrangement. Our findings contribute to the empirical software engineering literature by bringing a rich analysis of cases in agile and distributed contexts, from a vendor's perspective. We also discuss the possible implications of the results for research and in practice. © 2013 Elsevier Inc.},
author_keywords={Agile requirements engineering;  Case study;  Distributed project management Qualitative research;  Large projects;  Outsourced software development;  Requirements dependencies;  Requirements prioritization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Boehmm20044,
author={Boehmm, B. and Brown, A.W. and Basili, V. and Turner, R.},
title={Spiral acquisition of software-intensive systems of systems},
journal={CrossTalk},
year={2004},
number={5},
pages={4-9},
note={cited By (since 1996)12},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-3042568232&partnerID=40&md5=8167337c6271e55fc888dcd6d09b37ff},
abstract={The old and new sources of risks encountered in acquiring and developing complex software-intensive systems of systems (SISOS) are discussed. A top-10 list of risks and challenges that needs to be resolved in developing and evolving software-intensive systems is also provided. These risks can be addressed through risk analysis, risk management planning and control, and application of the risk-driven Win-Win Spiral Model. The Win-Win Spiral Development and evolutionary acquisition process is helpful for identifying and coping with SISOS risks.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ramasubbu2010107,
author={Ramasubbu, N. and Balan, R.K.},
title={Evolution of a bluetooth test application product line: A case study},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2010},
pages={107-116},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78751505424&partnerID=40&md5=33b758708793d10f438e575f0c137af4},
abstract={In this paper, we study the decision making process involved in the five year lifecycle of a Bluetooth software product produced by a large, multi-national test and measurement firm. In this environment, customer change requests either have to be added as a standard feature in the product, or developed as a special customized version of the product. We first discuss the influential factors, such as evolving standards, market share, installed-base, and complexity, which collectively determined how the firm responded to product change requests. We then develop a predictive decision model to test the collective impact of these factors on determining whether to standardize or customize a customer's change request. Finally, we develop and test a customization cost estimation model, for use by software product teams, which specifically accounts for factors unique to the customization stage of a product lifecycle. © 2010 ACM.},
author_keywords={complexity;  product development;  product life cycle;  software engineering economics;  software evolution;  software process},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sneed2011192,
author={Sneed, H.M. and Jungmayr, S.},
title={More test efficiency through value driven testing [Mehr Testwirtschaftlichkeit durch Value-Driven-Testing]},
journal={Informatik-Spektrum},
year={2011},
volume={34},
number={2},
pages={192-209},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79955985792&partnerID=40&md5=b625e10b5e9e0b12d06fcc498e2b9ad8},
abstract={The following contribution is an extension of the latest research on value-driven software engineering to the field of software testing. The goals andmethods of value-driven software engineering are reviewed and analyzed in regard to their application to testing. The particular difficulties of cost justifying testing are pointed out. The point is made that despite these difficulties it is possible to quantify the benefits of testing in terms of the costs of errors occurring in production and the costs of correcting those errors. The paperproposesusing themethodsof riskanalysis to predict those costs.On the other hand, a cost estimation method for calculating the costs of testing based on a modified COCOMO-II equation is proposed. In the end the costs and benefits are compared to compute a return of investment (ROI)ontesting.This is illustratedinconstructed examples.The goal of thework is to put testmanagers ina positiontojustify the increasingly high costs of systemtesting. © Springer-Verlag 2011.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bjarnason20121107,
author={Bjarnason, E. and Wnuk, K. and Regnell, B.},
title={Are you biting off more than you can chew? A case study on causes and effects of overscoping in large-scale software engineering},
journal={Information and Software Technology},
year={2012},
volume={54},
number={10},
pages={1107-1124},
note={cited By (since 1996)2},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84863435148&partnerID=40&md5=0bbc42d90f2ab1c295376069de734a2b},
abstract={Context: Scope management is a core part of software release management and often a key factor in releasing successful software products to the market. In a market-driven case, when only a few requirements are known a priori, the risk of overscoping may increase. Objective: This paper reports on findings from a case study aimed at understanding overscoping in large-scale, market-driven software development projects, and how agile requirements engineering practices may affect this situation. Method: Based on a hypothesis of which factors that may be involved in an overscoping situation, semi-structured interviews were performed with nine practitioners at a large, market-driven software company. The results from the interviews were validated by six (other) practitioners at the case company via a questionnaire. Results: The results provide a detailed picture of overscoping as a phenomenon including a number of causes, root causes and effects, and indicate that overscoping is mainly caused by operating in a fast-moving market-driven domain and how this ever-changing inflow of requirements is managed. Weak awareness of overall goals, in combination with low development involvement in early phases, may contribute to 'biting off' more than a project can 'chew'. Furthermore, overscoping may lead to a number of potentially serious and expensive consequences, including quality issues, delays and failure to meet customer expectations. Finally, the study indicates that overscoping occurs also when applying agile requirements engineering practices, though the overload is more manageable and perceived to result in less wasted effort when applying a continuous scope prioritization, in combination with gradual requirements detailing and a close cooperation within cross-functional teams. Conclusion: The results provide an increased understanding of scoping as a complex and continuous activity, including an analysis of the causes, effects, and a discussion on possible impact of agile requirements engineering practices to the issue of overscoping. The results presented in this paper can be used to identify potential factors to address in order to achieve a more realistic project scope. © 2012 Elsevier B.V. All rights reserved.},
author_keywords={Agile requirements engineering;  Case study;  Empirical study;  Requirements scoping;  Software release planning},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2012280,
author={Wang, T. and Guo, S. and Sarker, B.R. and Li, Y.},
title={Process planning for collaborative product development with CD-DSM in optoelectronic enterprises},
journal={Advanced Engineering Informatics},
year={2012},
volume={26},
number={2},
pages={280-291},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84859597708&partnerID=40&md5=9ddc40cf520a63f5cb9862755a181d81},
abstract={Coupled activities are the main reasons to cause collaboration in product development (PD) process. Previous modeling approaches such as Petri net, IDEF and DSM fail to represent the collaboration characteristic of PD process well. Considering the characteristics of optoelectronic PD process especially for demand of collaborative development, this paper proposes the process planning framework, establishes the three-dimensional collaboration model, and analyzes nine collaboration types among activities. The Process Collaboration Degree (PCD) and Activity Collaboration Degree (ACD) considering information delivery times and probability are defined to strengthen the modeling ability of DSM, and then the Collaboration Degree Design Structure Matrix (CD-DSM) is constructed to model the collaborative development process of optoelectronic products. In order to decrease the coupled complexity, PCD is applied to decompose the nested activities into atomic activities based on the information input/output points and ACD is used to express the value of the elements in the CD-DSM. Furthermore, the upstream and downstream relationship of atomic activities is optimized based on the CD-DSM to plan the collaborative PD process. Finally, the proposed framework is realized in a prototype system, and an example of LED display module development process is carried out in an optoelectronic company to illustrate the application. And the results show that the proposed method improves the process planning of collaborative PD effectively. © 2012 Elsevier Ltd. All rights reserved.},
author_keywords={CD-DSM;  Collaborative product development;  Process modeling;  Process planning},
document_type={Article},
source={Scopus},
}

@ARTICLE{Alyahya2012,
author={Alyahya, M. and Ahmad, R. and Lee, S.},
title={Impact of CMMI-based process maturity levels on effort, productivity and diseconomy of scale},
journal={International Arab Journal of Information Technology},
year={2012},
volume={9},
number={4},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-83255164856&partnerID=40&md5=15c2ecf742726954aac7daea2235b224},
abstract={The Software Capability Maturity Model Integration (CMMI) has become a popular Software Process Improvement (SPI) model for enhancing software development processes with the goal of developing high-quality software within budget and schedule. Since Software development effort can be greatly affected by the organizational process maturity level, this study examines the impact of different CMMI-based process maturity levels on effort, productivity development team and diseconomy of scale for a standard project sizes. The COnstructive COst MOdel (COCOMO) is employed to compute the software development effort. The percentage of change (increase or decrease) in software development effort, productivity and diseconomy of scale is employed as a measure of effectiveness for this study. The results of this work demonstrate that each higher CMMI maturity level has a considerable impact in decreasing the development effort, increasing the productivity rate and reducing the diseconomy of scale. The results also indicate that the impact of CMMI-based maturity levels significantly increases with project sizes.},
author_keywords={CMMI;  COCOMO ii;  Diseconomy of scale;  Effort multipliers;  Process maturity;  Productivity rate;  Scale factors},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lee20111655,
author={Lee, M. and Rothenberger, M. and Peffers, K.},
title={Identifying effort estimation factors for corrective maintenance in object-oriented systems},
journal={17th Americas Conference on Information Systems 2011, AMCIS 2011},
year={2011},
volume={2},
pages={1655-1661},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84870181927&partnerID=40&md5=885bbf8cd5b468a6d705bea7018d3c07},
abstract={This research explores the decision-making process of expert estimators of corrective maintenance projects by using qualitative methods to identify the factors that they use in deriving estimates. We implement a technique called causal mapping, which allows us to identify the cognitive links between the information that estimators use, and the estimates that they produce based on that information. Results suggest that a total of 17 factors may be relevant for corrective maintenance effort estimation, covering constructs related to developers, code, defects, and environment. This line of research aims at addressing the limitations of existing maintenance estimation models that do not incorporate a number of soft factors, thus, achieving less accurate estimates than human experts.},
author_keywords={Causal mapping;  Effort;  Estimation;  Software maintenance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Svensson201020,
author={Svensson, R.B. and Regnell, B. and Aurum, A.},
title={Towards modeling guidelines for capturing the cost of improving software product quality in release planning},
journal={ACM International Conference Proceeding Series},
year={2010},
pages={20-23},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80053193222&partnerID=40&md5=d1f14793907391726c3d420dc7561bf1},
abstract={Quality requirements complement functional requirements with information on the target quality levels of software functions or emergent system quality aspects. In release planning the allocation of development effort in further investments into product enhancements, new or enhanced functions are competing with quality improvements for limited resources. When setting balanced quality targets for development of coming releases, the estimated costs of improved software quality is needed as a basis for decisions. In this paper, we present initial results from an industrial evaluation of modeling guidelines for the Cost View in the QUality PERformance (QUPER) model. The initial results indicate the feasibility of the proposed modeling approach to cost estimation of quality requirements in QUPER, but further validation is needed. © 2010 ACM.},
author_keywords={empirical;  quality requirements;  QUPER;  release planning},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Céret2013795,
author={Céret, E. and Dupuy-Chessa, S. and Calvary, G. and Front, A. and Rieu, D.},
title={A taxonomy of design methods process models},
journal={Information and Software Technology},
year={2013},
volume={55},
number={5},
pages={795-821},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84875231909&partnerID=40&md5=ec29791104be6b5c5f36f3bc38da4689},
abstract={Context: Designers and developers are increasingly expected to deliver high quality systems, i.e. systems that are usable, robust, consistent as well as evolutionary, and that fulfill users' needs. To produce such systems, Design Methods suggest many approaches. However, the important number of existing approaches makes the choice of a method among the others particularly difficult. In addition to this, and because of the time required for understanding (and then operationalizing) new methods, designers tend to use already known methods, even though those which sometimes may not really be adapted to their needs. Objective: This paper proposes a classification of characteristics of design methods process models. In other terms, it proposes a taxonomy that aims to facilitate the discovery and the choice of methods for designers and developers. Method: From a study of process models of several design methods, we identify six main axes, namely Cycle, Collaboration, Artifacts, Recommended Use, Maturity and Flexibility, which are in turn divided into 34 characteristics. Results: This paper provides a deep theorical insight. For each characteristic identified from relevant literature, a definition and a gradation, illustrated using examples, are given. Moreover, it presents a web site that offers various tools for exploring the axes of our taxonomy. This web site provides an overview of process models as well as means for comparing them, textually or graphically. Finally, the paper relates the first evaluation conducted in order to estimate designers' adhesion to the taxonomy in terms of easiness of learning, completeness and intention to use. Conclusion: We show, based on evaluation results, that our taxonomy of process models facilitates the discovery of new methods and helps designers in choosing suitable methods, really adapted to their needs. Therefore, it enhances chances to conduct high quality projects. © 2012 Elsevier B.V. All rights reserved.},
author_keywords={Characterization;  Design methods;  Process models;  Taxonomy},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ngo-The200895,
author={Ngo-The, A. and Ruhe, G.},
title={A systematic approach for solving the wicked problem of software release planning},
journal={Soft Computing},
year={2008},
volume={12},
number={1},
pages={95-108},
note={cited By (since 1996)23},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-34548040975&partnerID=40&md5=2cb40dfbfc5063672251c76c3977940c},
abstract={Release planning is known to be a cognitively and computationally difficult problem. Different kinds of uncertainties make it hard to formulate and solve the problem. Our solution approach called EVOLVE+ mitigates these difficulties by (i) an evolutionary problem solving method combining rigorous solution methods to solve the actual formalization of the problem combined with the interactive involvement of the human experts in this process, (ii) provision of a portfolio of diversified and qualified solutions at each iteration of the solution process, and (iii) the application of a multi-criteria decision aid method (ELECTRE IS) to assist the selection of the final solution from a set of qualified solutions. At the final stage of the process, an outranking relation is established among the qualified candidate solutions to address existing soft constraints or objectives. A case study is provided to illustrate and initially evaluate the given approach. The proposed method and results are not limited to software release planning, but can be adapted to a wider class of wicked planning problems. © Springer-Verlag 2007.},
author_keywords={Decision support;  Diversification;  Evolutionary problem solving;  Fuzzy outranking relation;  Release planning;  Soft criteria;  Uncertainty;  Wicked planning problems},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Harmon2009,
author={Harmon, R. and Demirkan, H. and Hefley, B. and Auseklis, N.},
title={Pricing strategies for information technology services: A value-based approach},
journal={Proceedings of the 42nd Annual Hawaii International Conference on System Sciences, HICSS},
year={2009},
art_number={4755465},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650760278&partnerID=40&md5=95c02419a4428f246245cebda518eaa8},
abstract={While commoditization is creating opportunities for customers of information technology services, it is creating new challenges for the service providers. Pricing strategies are one of the most important challenges and decisions for today's IT service providers. Pricing strategies for IT services have traditionally focused on covering costs, achieving desired margins and meeting the competition. These pricing schemes range from simple approaches, easily copied by competitors, to complex models with high management costs. In order to be successful in today's competitive business world, the service providers need to define their pricing strategies by considering the customer's perceived value from the service they receive rather than using traditional cost-based pricing strategies. This paper surveys literature on IT services pricing and presents a value-based approach to effectively price IT services. © 2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kruchten200936,
author={Kruchten, P. and Capilla, R. and Dueñas, J.C.},
title={The decision view's role in software architecture practice},
journal={IEEE Software},
year={2009},
volume={26},
number={2},
pages={36-42},
note={cited By (since 1996)35},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-63049086371&partnerID=40&md5=fd42f7cc597efee9c9c9ad5a856befe2},
abstract={A decision view provides a useful complement to the traditional sets of architectural views and viewpoints. It gives an explanatory perspective that illuminates the reasoning process itself and not solely its results. The decision view documents aspects of the architecture that are hard to reverse-engineer from the software itself and that are often left tacit. The decision view and the decisions that it captures embody high-level architectural knowledge that can be transferred to other practitioners and merged when systems are merged, and they offer useful support for maintaining large, long-lived software-intensive systems. This article leads readers through a succession of epiphanies: from design to architecture, then architecture representation to architecture design methods, and finally to architectural design decisions. © 2009 IEEE.},
author_keywords={Architectural design decision;  Architectural knowledge;  Architecture views;  Decision view;  Software architecture},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ivanović2013369,
author={Ivanović, A. and America, P. and Snijders, C.},
title={Modeling customer-centric value of system architecture investments},
journal={Software and Systems Modeling},
year={2013},
volume={12},
number={2},
pages={369-385},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84878166756&partnerID=40&md5=fde33982f91feac5b855ed8bc6efb2a9},
abstract={System architecture investments aim at improving the quality of the system in alignment with (current and future) business goals. While the costs of architecture changes are routinely calculated, identifying benefits of architecture changes and translating them to a monetary value has been a challenge in practice. Currently, architecture value estimation is largely based on cost-savings or on risk mitigation, without much reliance on potential customer benefits. This article reports on our experience in modeling the customer value and evaluating its potential use in choosing between different system architectures in two case studies conducted in an organization developing healthcare systems. To model the customer value, we exploit best practices in management and marketing. Management tools, in particular strategy maps and balanced scorecards, are used to identify customer-centric benefits caused by architecture design decisions. Furthermore, two marketing concepts, customer value-in-use and customer segments, are adopted to quantify the value of architecture changes for a single customer and multiple customers, respectively. The paper shows that using the customer value in addition to the existing value indicators in the organization has several advantages but also calls for future improvements to be adopted in practice. © 2012 Springer-Verlag.},
author_keywords={Architecture investments;  Balanced scorecards;  Case study;  Customer segments;  Customer value;  Customer value-in-use;  Decision making;  Software architecture;  Strategy maps;  Value-based software engineering},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Eklin2009135,
author={Eklin, M. and Shtub, A. and Arzi, Y.},
title={A rough-cut cost estimation in a finite-capacity stochastic environment based on forced idle time prediction},
journal={2008 Proceedings of the 9th Biennial Conference on Engineering Systems Design and Analysis},
year={2009},
volume={1},
pages={135-143},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-70349112746&partnerID=40&md5=21ec965fd0232505bb9394f2c68b0be9},
abstract={In recent years several researchers suggested cost estimation models that consider the limited capacity of the shop. In these studies, the stochastic nature of the shop floor is modeled by a time-consuming simulation. This paper proposes five alternative rough-cut cost estimation methods that can replace the simulation. Three of five methods based on forced idle time prediction. The study compares the cost estimations derived from these methods. A cost estimation method, based on the forced idle time of the bottleneck workstation, was found to be outperform the others. As the best method, the bottleneck-based method was compared to the actual order's cost and was found as a replacement to simulation.Copyright © 2008 by ASME.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mohebzada2012200,
author={Mohebzada, J.G. and Ruhe, G. and Eberlein, A.},
title={Systematic mapping of recommendation systems for requirements engineering},
journal={2012 International Conference on Software and System Process, ICSSP 2012 - Proceedings},
year={2012},
pages={200-209},
art_number={6225965},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84864338213&partnerID=40&md5=4dee3803b43ca5561d9e77d9ed0dd106},
abstract={Recommendation systems provide users with up-to-date guidance on processes, artifacts or other project-relevant information. Recommendation systems for requirements engineering can be used to provide the right information, at the right time, to requirements engineers. In this paper, we use systematic mapping to provide an overview of recommendation systems for the requirements engineering process, their characteristics, and state of validation. The resulting maps are analyzed to provide conclusions and to identify the limitations of current studies, and future research areas. The results of the mapping are used to outline the motivation for our future work on a recommendation system that helps product managers decide on the assignment of requirements to subsequent releases while considering constraints such as time, effort, quality, and resources. © 2012 IEEE.},
author_keywords={recommendation systems;  requirements engineering;  systematic mapping study},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mahmood2011583,
author={Mahmood, S. and Khan, A.},
title={An industrial study on the importance of software component documentation: A system integrator-s perspective},
journal={Information Processing Letters},
year={2011},
volume={111},
number={12},
pages={583-590},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79953168746&partnerID=40&md5=1af7a244f8ab0d9febc7a9a09a5e7316},
abstract={Component integration is widely recognized as a process which plays a central role in overall Component Based System (CBS) development. A system integrator focuses on assembling existing components, developed by different parties, to build a software system. The integration process usually involves adapting existing component interfaces and writing new functions to handle the mismatches between stakeholder needs and available component features. The lack of detailed component documentation has been a key area of concern in CBS development due to its profound impact on the integration phase of a CBS development life cycle. In this paper, we report results of an industrial survey conducted among system integrators to understand role of component documentation in the CBS integration phase. The survey investigates whether the presence of component documentation helps a system integrator and its correlations with typical CBS integration success factors. The result reinforces current perceptions of the significance of component documentation in CBS integration. However, the lack of comprehensive component documentation presents a potential risk for a system integrator during integration effort estimation and testing processes. © 2011 Elsevier B.V.},
author_keywords={Component based systems;  Component documentation;  Integration;  Software components;  Software engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Leach200419,
author={Leach, R.J.},
title={Separate money tubs hurt software productivity},
journal={CrossTalk},
year={2004},
number={12},
pages={19-22},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-18444398984&partnerID=40&md5=8f26ee8cf5e12da4975f13f90431215c},
abstract={The simple change in management practice, and project accounting can encourage software development that meets the four goals such as developing more software, better, cheaper, and faster. The organizations cost estimation takes into account the amount of reuse, and whether the requirements, design, or source code from project. With perfect software reuse, and extremely accurate cost estimation, project has been created by a software development process that is both efficient, and giving more efficient, giving more software per month. Many organizations use some of the every-tub-on-its-bottom approach to funding software projects in which a manager is given the budget for the completion of project.},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Boehm2005706,
author={Boehm, B. and Brown, W. and Turner, R.},
title={Spiral development of software-intensive systems of systems},
journal={Proceedings - 27th International Conference on Software Engineering, ICSE05},
year={2005},
pages={706-707},
note={cited By (since 1996)4},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33244476713&partnerID=40&md5=29c82b8fd2900583e0d23833650aa847},
author_keywords={Feasibility Rationale;  Risk Management;  Software Processes;  Software-Intensive Systems of Systems;  Spiral Development;  Win-Win Spiral},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeMello2013216,
author={De Mello, R.M. and Travassos, G.H.},
title={An ecological perspective towards the evolution of quantitative studies in software engineering},
journal={ACM International Conference Proceeding Series},
year={2013},
pages={216-219},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84877262797&partnerID=40&md5=963108463a42331f4f6124af212693a7},
abstract={Context: Two of the most common external threats to validity in quantitative studies in software engineering (SE) are concerned with defining the population by convenience and nonrandom sampling assignment. Although these limitations can be reduced by increasing the number of replications and aggregating their results, the acquired evidence rarely can be generalized to the field. Objective: To investigate the state of practice of meta-analysis in SE and its limitations, intending to propose an alternative perspective to understand the relationships among experimentation, production, threats to validity and evidence. To propose and evaluate means to strengthen quantitative studies in software engineering and making them less risky due to population and sampling issues. Method: To use the underlying idea from the Theory of Food Chains to alternatively understand the impact of external threats to validity in the SE experimental cycle (experimental chains). Next, to accomplish an initial technical literature survey to observe basic features of secondary studies aggregating primary studies results. Third, to organize a set of experimental chain's concepts and make initial discussions regarding the observed secondary studies concerned with this metaphor. Results: By applying the experimental chains concepts it was initially observed that, although important and necessary, most of the current effort in the conduction of quantitative studies in SE does not produce (mainly due to population/sampling constraints) results strong enough to positively impact the engineering of software. It promotes an imbalance between research and practice. However, more investigation is necessary to support this claim. Conclusion: We argue that research energy has been lost in SE studies due to population/sampling constraints. Therefore, we believe more investigation must be undertaken to understand how better organizing, enlarging, setting up and sampling SE quantitative studies' population by using, for instance, alternative technologies such as social networks or other crowdsourcing technologies. Copyright 2013 ACM.},
author_keywords={Ecology;  Evidence based software engineering;  Experimental chains;  Food chains;  Meta-analysis;  Population sampling;  Quantitative survey;  Quasi-experiments;  Threats to validity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Barbagallo2009,
author={Barbagallo, D. and Francalanci, C.},
title={The relationship among development skills, design quality, and centrality in open source projects},
journal={17th European Conference on Information Systems, ECIS 2009},
year={2009},
page_count={12},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84870666545&partnerID=40&md5=90449f016d530cb93bb69c3969f9a578},
abstract={In a previous paper, we have found empirical evidence supporting a positive relationship between network centrality and success. However, we have also found that more successful projects have a lower technical quality. A first, straightforward argument explaining previous findings is that more central contributors are also highly skilled developers who are well known for their ability to manage the complexity of code with a lower attention to the software structure. The consolidated metrics of software quality used by the authors in their previous research represent measures of code structure. This paper provides empirical evidence supporting the idea that the negative impact of success on quality is caused by the careless behaviour of skilled developers, who are also hubs within the social network. Research hypotheses are tested on a sample of 56 OS applications from the SourceForge.net repository, with a total of 378 developers. The sample includes some of the most successful and large OS projects, as well as a cross-section of less famous active projects evenly distributed among SourceForge.net's project categories.},
author_keywords={Social networks;  Software design skills;  Software quality},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wnuk2011168,
author={Wnuk, K. and Callele, D.},
title={Requirements scoping visualization for project management},
journal={Lecture Notes in Business Information Processing},
year={2011},
volume={80 LNBIP},
pages={168-180},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79959953258&partnerID=40&md5=f12e0416116f0cafacff49f95d80f6b0},
abstract={Determining requirements process efficiency, and measuring the corresponding monetary impacts, is a challenging but necessary aspect of project management. In this paper, we perform an independent analysis of scoping decisions from a large industrial project with the goal of providing visualizations that facilitate investigations of process efficiency, agility, and the effects of scoping decisions. The visualizations proposed in this paper can be used to analyze scoping dynamics and support process management decisions on a quantitative rather than a qualitative basis. © 2011 Springer-Verlag.},
author_keywords={process evaluation;  project management;  require- ments scope;  Requirements visualization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kwong201197,
author={Kwong, C.K. and Luo, X.G. and Tang, J.F.},
title={A multiobjective optimization approach for product line design},
journal={IEEE Transactions on Engineering Management},
year={2011},
volume={58},
number={1},
pages={97-108},
art_number={5484562},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78951486165&partnerID=40&md5=b257117fb6e5a6a5c886f27489fce2f0},
abstract={Product line design is a key decision area that a product development team has to deal with in the early stages of product development. Previous studies of product line design have focused on single-objective optimization. However, several optimization objectives may be simultaneously pursued, and the solutions that can address the objectives are required in many practical scenarios. In this research, we propose a one-step multiobjective optimization approach for product line design. The proposed optimization model has three objectives: 1) maximizing the market share of a company's products; 2) minimizing the total product development cost of a product line; and 3) minimizing the total product development cycle time. A curve-fitting method is introduced into the part-worth utility models so that the optimization model can be applied to products with level-based attributes and attributes that have continuous values. A multiobjective genetic algorithm is adopted to solve the optimization model, obtaining a set of nondominated solutions. With the solutions, a new product development team can select a preferred solution interactively in a 2-D graph. An example of the optimal design of a product line of digital cameras is used to illustrate the proposed approach. © 2006 IEEE.},
author_keywords={Conjoint analysis (CA);  multiobjective genetic algorithm (MOGA);  product line design;  utility},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mohagheghi2007471,
author={Mohagheghi, P. and Conradi, R.},
title={Quality, productivity and economic benefits of software reuse: A review of industrial studies},
journal={Empirical Software Engineering},
year={2007},
volume={12},
number={5},
pages={471-516},
note={cited By (since 1996)34},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-34748858031&partnerID=40&md5=91cf3d0727aab1a0aafa56d7d420fff8},
abstract={Systematic software reuse is proposed to increase productivity and software quality and lead to economic benefits. Reports of successful software reuse programs in industry have been published. However, there has been little effort to organize the evidence systematically and appraise it. This review aims to assess the effects of software reuse in industrial contexts. Journals and major conferences between 1994 and 2005 were searched to find observational studies and experiments conducted in industry, returning eleven papers of observational type. Systematic software reuse is significantly related to lower problem (defect, fault or error) density in five studies and to decreased effort spent on correcting problems in three studies. The review found evidence for significant gains in apparent productivity in three studies. Other significant benefits of software reuse were reported in single studies or the results were inconsistent. Evidence from industry is sparse and combining results was done by vote-counting. Researchers should pay more attention to using comparable metrics, performing longitudinal studies, and explaining the results and impact on industry. For industry, evaluating reuse of COTS or OSS components, integrating reuse activities in software processes, better data collection and evaluating return on investment are major challenges. © 2007 Springer Science+Business Media, LLC.},
author_keywords={Evidence;  Productivity;  Quality;  Review;  Software reuse},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Höst200512,
author={Höst, M. and Johansson, E.},
title={Performance prediction based on knowledge of prior product versions},
journal={Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
year={2005},
pages={12-20},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-26844536228&partnerID=40&md5=182ee73dff33bb8ed27e9499b0d3290d},
abstract={Performance estimation is traditionally carried out when measurement from a product can be obtained. In many cases there is, however, a need to start to make predictions earlier in a development project, when for example different architectures are compared. In this paper, two methods for subjective predictions of performance are investigated. With one of the methods experts estimate the relative resource usage of software tasks without using any knowledge of earlier versions of the product, and with the other method experts use their experience and knowledge of earlier versions of the system. With both methods there are rather large differences between different individual predictions, but the median of the prediction error indicates that the second method is worth further investigations. © 2005 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Abdellatief2013587,
author={Abdellatief, M. and Sultan, A.B.M. and Ghani, A.A.A. and Jabar, M.A.},
title={A mapping study to investigate component-based software system metrics},
journal={Journal of Systems and Software},
year={2013},
volume={86},
number={3},
pages={587-603},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84872676227&partnerID=40&md5=168cb6d0d2c29588e6a74450ccc234fc},
abstract={A component-based software system (CBSS) is a software system that is developed by integrating components that have been deployed independently. In the last few years, many researchers have proposed metrics to evaluate CBSS attributes. However, the practical use of these metrics can be difficult. For example, some of the metrics have concepts that either overlap or are not well defined, which could hinder their implementation. The aim of this study is to understand, classify and analyze existing research in component-based metrics, focusing on approaches and elements that are used to evaluate the quality of CBSS and its components from a component consumer's point of view. This paper presents a systematic mapping study of several metrics that were proposed to measure the quality of CBSS and its components. We found 17 proposals that could be applied to evaluate CBSSs, while 14 proposals could be applied to evaluate individual components in isolation. Various elements of the software components that were measured are reviewed and discussed. Only a few of the proposed metrics are soundly defined. The quality assessment of the primary studies detected many limitations and suggested guidelines for possibilities for improving and increasing the acceptance of metrics. However, it remains a challenge to characterize and evaluate a CBSS and its components quantitatively. For this reason, much effort must be made to achieve a better evaluation approach in the future. © 2012 Elsevier Inc.},
author_keywords={Component-based software system;  Software components;  Software metrics;  Software quality;  Systematic mapping study},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nagowah2012948,
author={Nagowah, S.D. and Philippe, C.C.C.},
title={Enhancing decision support in an automatic workflow management tool},
journal={2012 International Conference on Computer and Information Science, ICCIS 2012 - A Conference of World Engineering, Science and Technology Congress, ESTCON 2012 - Conference Proceedings},
year={2012},
volume={2},
pages={948-952},
art_number={6297162},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84867871583&partnerID=40&md5=f088bfd211639a0196a3ee35f4f8725d},
abstract={Project managers are often overloaded with large volume of complex information that they have difficulties to grasp the right, timely and reliable information. They subsequently seek the help of Business Intelligence (BI) tools to enable them extract valuable information to help them carry out their tasks more efficiently. For the past years, BI tools have been used in various fields such as banking, healthcare, transportation, agriculture and others. This paper aims to investigate how BI can be applied in the field of software engineering, more precisely, workflow management with the aim of enhancing decision support. For the purpose of this investigation, one of the BI systems, data mining has been integrated in an automatic workflow management tool. The paper discusses how this BI system influences the process of workflow management and resource allocation. Finally, the paper presents the results of the investigation. © 2012 IEEE.},
author_keywords={Automatic Workflow Management Tool;  Business Intelligence;  Data Mining;  Decision Support;  Resource Allocation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Iwata2010,
author={Iwata, K. and Nakashima, T. and Anan, Y. and Ishii, N.},
title={Artificial neural network models to predict effort and errors for embedded software development projects},
journal={IEEJ Transactions on Electronics, Information and Systems},
year={2010},
volume={130},
number={12},
pages={2167-2173+10},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78951483104&partnerID=40&md5=d5737897c7bed78706223be732088ec3},
abstract={In this paper, we establish effort and error prediction models using an artificial neural networks (ANNs). We propose the normalizing method to reduce the margin of errors for ANN models. In addition, we perform an evaluation experiment to compare the accuracy of the ANN models with that of the regression analysis (RA) model and that of two ANN models using Steel-Dwass's multiple comparison test. The results show that each ANN model is more accurate than the RA model and the proposed method can reduce the errors for some cases, since the mean errors of the ANN models are statistically significantly lower. © 2010 The Institute of Electrical Engineers of Japan.},
author_keywords={Artificial nural network;  Development projects;  Embeded software;  Total effort;  Total errors},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Choi2007211,
author={Choi, K.H.T. and Tempero, E.},
title={Dynamic measurement of polymorphism},
journal={Conferences in Research and Practice in Information Technology Series},
year={2007},
volume={62},
pages={211-220},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-83455243927&partnerID=40&md5=361ee11cbf9100515fe9031c1091829d},
abstract={Measuring "reuse" and "reusability" is difficult because there are so many different facets to these concepts. Before we can effectively measure reuse and reusability, we must first be able to effectively measure these different facets. One such facet is the programming language constructs that are available. For example whether or not a language supports polymorphism is believed to affect how reusable a developer can make a code artifact. Effectively measuring polymorphism is a challenge because its behaviour is only observable at run-time. In this paper, we present a metric for polymorphism based on the dynamic behaviour of the code. We evaluate the usefulness of the metric through two case studies. Copyright © 2007, Australian Computer Society, Inc.},
author_keywords={Dynamic profiling;  Inheritance;  Polymorphism;  Software metrics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen201338,
author={Chen, L. and Babar, M.A. and Nuseibeh, B.},
title={Characterizing architecturally significant requirements},
journal={IEEE Software},
year={2013},
volume={30},
number={2},
pages={38-45},
art_number={6365165},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84879293758&partnerID=40&md5=e9e4b5892528234b33853534bc308421},
abstract={This article presents a framework for characterizing architecturally significant requirements (ASRs) on the basis of an empirical study using grounded theory. The study involved interviews with 90 practitioners with an accumulated 1,448 years of software development experiences in more than 500 organizations of various sizes and domains. These findings could provide researchers with a framework for discussing and conducting further research on ASRs and can inform researchers' development of technologies for dealing with ASRs. The findings also enrich understanding of requirements and architecture interactions, allowing the twin peaks to move from aspiration to reality. © 1984-2012 IEEE.},
author_keywords={architecturally significant requirements;  ASR;  empirical study;  grounded theory;  nonfunctional requirements;  quality attributes;  requirements;  software architectures;  software engineering;  specifications},
document_type={Article},
source={Scopus},
}

@ARTICLE{Antón2003151,
author={Antón, A.I. and Potts, C.},
title={Functional paleontology: The evolution of user-visible system services},
journal={IEEE Transactions on Software Engineering},
year={2003},
volume={29},
number={2},
pages={151-166},
note={cited By (since 1996)15},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0037328554&partnerID=40&md5=c202a0c2f056eb4bc07bdd996078cbea},
abstract={It has long been accepted that requirements analysis should precede architectural design and implementation, but in software evolution and reverse engineering this concern with black-box analysis of function has necessarily been de-emphasized in favor of code-based analysis and designer-oriented interpretation. In this paper, we redress this balance by describing "functional paleontology," an approach to analyzing the evolution of user-visible features or services independent of architecture and design intent. We classify the benefits and burdens of interpersonal communication services into core and peripheral categories and investigate the telephony services available to domestic subscribers over a 50-year period. We report that services were introduced in discrete bursts, each of which emphasized different benefits and burdens. We discuss the general patterns of functional evolution that this "fossil record" illustrates and conclude by discussing their implications for forward engineering of software products.},
author_keywords={Empirical methods;  Measurement;  Metrics;  Requirements engineering;  Reverse engineering;  Software evolution},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Raffo20101625,
author={Raffo, D. and Mehta, M. and Anderson, D.J. and Harmon, R.},
title={Integrating lean principles with value based software engineering},
journal={PICMET '10 - Portland International Center for Management of Engineering and Technology, Proceedings - Technology Management for Global Economic Growth},
year={2010},
pages={1625-1634},
art_number={5602127},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78549292126&partnerID=40&md5=6244b7fbc82cff5fb88072ef4b88188e},
abstract={Lean Six-sigma principles and methods have been successfully applied in a variety of industries. Many of the core principles of Lean Six-sigma can be applied to software development to dramatically improve performance with some industry specific adaptations. In this regard, Lean Six-sigma principles can be made to further leverage the benefits of software engineering best practices to improve work flow and the tactical management of the project. The Value Based Software Engineering (VBSE) process as practiced today, begins the software development lifecycle (SDLC) with a sophisticated customer value analysis (CVA) which is coupled with a quantitative evaluation of pricing and profitability options followed by development of the software. The CVA yields a wealth of information not only for product features, but also for improving all processes within the organization that touch the customer. This paper seeks to present and articulate how applicable Lean Six-sigma principles can be coupled with the VBSE to improve the SDLC by improving the identification and delivery of customer value as well as the reduction of wasted resources. The net result is to substantially enhance the VBSE SDLC and the the organization's overall ability to deliver value. © 2010 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sunkle20122,
author={Sunkle, S. and Kulkarni, V. and Roychoudhury, S.},
title={Measuring metadata-based aspect-oriented code in model-driven engineering},
journal={2012 3rd International Workshop on Emerging Trends in Software Metrics, WETSoM 2012 - Proceedings},
year={2012},
pages={2-8},
art_number={6226990},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84864152347&partnerID=40&md5=cdcd52fc261af3bd866557974b3ffe76},
abstract={Metrics measurement for cost estimation in model-driven engineering (MDE) is complex because of number of different artifacts that can potentially be generated. The complexity arises as auto-generated code, manually added code, and non-code artifacts must be sized separately for their contribution to overall effort. In this paper, we address measurement of a special kind of code artifacts called metadata-based aspect-oriented code. Our MDE toolset delivers large database-centric business-critical enterprise applications. We cater to special needs of enterprises by providing support for customization along three concerns, namely design strategies, architecture, and technology platforms (〈d, a, t〉) in customer-specific applications. Code that is generated for these customizations is conditional in nature, in the sense that model-to-text transformation takes place differently based on choices along these concerns. In our recent efforts to apply Constructive Cost Model (COCOMO) II to our MDE practices, we discovered that while the measurement of the rest of code and non-code artifacts can be easily automated, product-line-like nature of code generation for specifics of 〈d, a, t〉 requires special treatment. Our contribution is the use of feature models to capture variations in these dimensions and their mapping to code size estimates. Our initial implementation suggests that this approach scales well considering the size of our applications and takes a step forward in providing complete cost estimation for MDE applications using COCOMO II. © 2012 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Eklin20091,
author={Eklin, M. and Shtub, A. and Arzi, Y.},
title={A comparative study of rough-cut cost estimations in a finite-capacity stochastic environment},
journal={International Journal of Revenue Management},
year={2009},
volume={3},
number={1},
pages={1-36},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-62349133000&partnerID=40&md5=aae7f7969266c78947deadb464ae527e},
abstract={In recent years, several researchers suggested cost estimation models that consider the limited capacity of the shop. In these studies, the stochastic nature of the shop floor is modelled by a time-consuming simulation. This article compares five alternative rough-cut cost estimation methods that can replace the simulation. The study compares the cost estimations derived from these methods. A cost estimation method, based on the forced idle time of the bottleneck workstation, was found to be outperform the others. As the best method, the bottleneck-based method was compared to the actual order's cost and was found to be a replacement to simulation. Copyright © 2009, Inderscience Publishers.},
author_keywords={Aggregate planning;  Cost estimation;  Finite-capacity;  Job shop scheduling;  Revenue management;  Stochastic environment},
document_type={Article},
source={Scopus},
}

@ARTICLE{Das20111197,
author={Das, S. and Kanchanapiboon, A.},
title={A multi-criteria model for evaluating design for manufacturability},
journal={International Journal of Production Research},
year={2011},
volume={49},
number={4},
pages={1197-1217},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650209795&partnerID=40&md5=e70998bbb3da420436494da33f174974},
abstract={The design for manufacturability (DFM) method is most effective when integrated with the new product development (NPD) process. Due to the focused nature of the associated product and process knowledge and the extensive effort required, there are many NPD situations where classical DFM techniques cannot be readily applied. In this paper, Pro-DFM a multi-criteria model for manufacturability analysis that identifies product realisation opportunities (PRO) for cost reduction is presented. The key assumption in Pro-DFM is that the NPD team has a baseline estimate of production costs, and the evaluation question is how DFM issues will affect the expected unit production cost. The Pro-DFM model analyses a new design in three different factors: part procurement and handling, product assembly fabrication processes, and inventory costs. Each of these is independently analysed using a hierarchy of multiple criteria and sub-criteria. This approach is found to be amenable to most NPD processes, and conducive to easy integration. Each evaluation sub-criterion is presented in the form of a simple query, which is associated with a set of responses that is flexibly anchored to a uni-dimensional scale. A case study example is used to demonstrate the DFM evaluation process and the derivation of the cost penalties. © 2011 Taylor & Francis.},
author_keywords={cost estimating;  design for assembly;  design for manufacture},
document_type={Article},
source={Scopus},
}

@ARTICLE{Armbrust201026,
author={Armbrust, O.},
title={Determining organization-specific process suitability},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6195 LNCS},
pages={26-38},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77955444384&partnerID=40&md5=4b8143aa0276ef8655eaa9b378ac48d3},
abstract={Having software processes that fit technological, project, and business demands is one important prerequisite for software-developing organizations to operate successfully in a sustainable way. However, many such organizations suffer from processes that do not fit their demands, either because they do not provide the necessary support, or because they provide features that are no longer necessary. This leads to unnecessary costs during the development cycle, a phenomenon that worsens over time. This paper presents the SCOPE approach for systematically determining the process demands of current and future products and projects, for analyzing existing processes aimed at satisfying these demands, and for subsequently selecting those processes that provide the most benefit for the organization. The validation showed that SCOPE is capable of adjusting an organization's process scope in such a way that the most suitable processes are kept and the least suitable ones can be discarded. © 2010 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jbara201283,
author={Jbara, A. and Matan, A. and Feitelson, D.G.},
title={High-MCC functions in the Linux kernel},
journal={IEEE International Conference on Program Comprehension},
year={2012},
pages={83-92},
art_number={6240512},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84865019147&partnerID=40&md5=d06f8f3263c3654e9fc5dba82b28c0e1},
abstract={McCabe's Cyclomatic Complexity (MCC) is a widely used metric for the complexity of control flow. Common usage decrees that functions should not have an MCC above 50, and preferably much less. However, the Linux kernel includes more than 800 functions with MCC values above 50, and over the years 369 functions have had an MCC of 100 or more. Moreover, some of these functions undergo extensive evolution, indicating that developers are successful in coping with the supposed high complexity. We attempt to explain this by analyzing the structure of such functions and showing that in many cases they are in fact well-structured. At the same time, we observe cases where developers indeed refactor the code in order to reduce complexity. These observations indicate that a high MCC is not necessarily an impediment to code comprehension, and support the notion that complexity cannot be fully captured using simple syntactic code metrics. © 2012 IEEE.},
author_keywords={Linux Kernel;  McCabe Cyclomatic Complexity;  Software Complexity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kiebusch2006222,
author={Kiebusch, S. and Franczyk, B.},
title={Process family points [Prozess-familien-punkte]},
journal={Informatik - Forschung und Entwicklung},
year={2006},
volume={20},
number={4},
pages={222-229},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33745201672&partnerID=40&md5=693ce644503ec379f3e06fce8269b148},
abstract={Software system families are characterized through a structured reuse of components and a high degree of automation based on a common infrastructure. It is possible to increase the efficiency of software system families by an explicit consideration of process flows in application domains which are driven by processes. Based on that fact this article briefly describes the approach of process family engineering. Afterwards the metrics of Process-Family-Points are explained in detail. These are the only framework to measure the size and estimate the effort of process families. Subsequently this paper shows the first results from a validation of the Process-Family-Points in the application domains of Electronic Business and Automotive.},
author_keywords={Automotive;  Electronic Business;  Full Function Point;  Function Point;  Process Families;  Process-Family-Points},
document_type={Article},
source={Scopus},
}

@ARTICLE{McGregor200549,
author={McGregor, J.D.},
title={Metrics},
journal={Journal of Object Technology},
year={2005},
volume={4},
number={2},
pages={49-58},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-22944436148&partnerID=40&md5=c5610302b034f78730309261cb458adf},
abstract={Strategic goals are of no use if you can't tell whether they have been accomplished. A measurement program is an essential element in the strategic arsenal. In this month's issue of Strategic Software Engineering, I will explore some issues about measurement and effective strategic metrics. I will discuss some specific metrics but also how to setup a measurement program that addresses your needs.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Verhoef2007247,
author={Verhoef, C.},
title={Quantifying the effects of IT-governance rules},
journal={Science of Computer Programming},
year={2007},
volume={67},
number={2-3},
pages={247-277},
note={cited By (since 1996)13},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-34250756601&partnerID=40&md5=0a3ec8a1bbe27563f524c82ed2d38af3},
abstract={Via quantitative analyses of large IT-portfolio databases, we detected unique data patterns pointing to certain IT-governance rules and styles, plus their sometimes nonintuitive and negative side-effects. We grouped the most important patterns in seven categories and highlighted them separately. These patterns relate to the five fundamental parameters for IT-governance: data, control, time, cost and functionality. We revealed patterns of overperfect and heterogeneous data signifying reporting anomalies or ambiguous IT-governance rules, respectively. We also detected patterns of overregulation and underregulation, portending bloated control or no IT-control at all, both with negative side-effects: productivity loss, and too costly IT-development. Uniform management on time, cost or functionality showed clear patterns in the time and cost case, and more diffuse combined patterns for functionality. For these in total seven types of patterns, it was possible to take corrective measures to reduce unwanted side-effects, and/or amplify the intended purpose of the underlying IT-governance rules. These modifications ranged from refinements and additions, to eradications of IT-governance rules. For each of the seven patterns we provided lessons learned and recommendations on how to recognize and remove unwanted effects. Some effects were dangerous, and addressing them led to significant risk reduction and cost savings. © 2007 Elsevier B.V. All rights reserved.},
author_keywords={Heterogeneous data;  IT-governance;  IT-governance rules;  IT-portfolio analysis;  Managing on budget;  Managing on functionality;  Managing on time;  Overperfect data;  Overregulation;  Quantitative IT-governance;  Seasonality effects;  Time compression;  Time decompression;  Underregulation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fuentes-Fernández2012247,
author={Fuentes-Fernández, R. and Pavón, J. and Garijo, F.},
title={A model-driven process for the modernization of component-based systems},
journal={Science of Computer Programming},
year={2012},
volume={77},
number={3},
pages={247-269},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84855206523&partnerID=40&md5=e036bc0d4c1990d0c2baf732756a054d},
abstract={Software modernization is critical for organizations that need cost-effective solutions to deal with the rapid obsolescence of software and the increasing demand for new functionality. This paper presents the XIRUP modernization methodology, which proposes a highly iterative process, structured into four phases: preliminary evaluation, understanding, building and migration. This modernization process is feature-driven, component-based, focused on the early elicitation of key information, and relies on a model-driven approach with extensive use of experience from the previous projects. XIRUP has been defined in the European IST project MOMOCS, which has also built a suite of support tools. This paper introduces the process using a case study that illustrates its activities, related tools and results. The discussion highlights the specific characteristics of modernization projects and how a customized methodology can take advantage of them. © 2011 Elsevier B.V. All rights reserved.},
author_keywords={Agile process;  Component;  Model-driven engineering;  Modernization of software systems;  Software engineering;  Software methodology},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Farrell20091031,
author={Farrell, R.S. and Simpson, T.W.},
title={Improving cost effectiveness in an existing product line using component product platforms},
journal={2008 Proceedings of the ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, DETC 2008},
year={2009},
volume={1},
number={PART B},
pages={1031-1041},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-70149118554&partnerID=40&md5=b71d6d069a2c9fc1ea7dfb8bba9dcd20},
abstract={Previously, we introduced a new method for improving commonality in a highly customized, low volume product line using component product platforms. The method provides a bottom-up platform approach to redesign family members originally developed one-at-a-time to meet specific customer requirements. In this paper, we extend the method with an Activity-Based Costing (ABC) model to specifically capture the manufacturing costs in the product line, including the cost associated with implementing a platform strategy. The valve yoke example is revisited in this paper, the customized ABC model is defined, two design strategy alternatives are addressed, and the new method is used to determine which alternative is better at resolving the tradeoff between commonality, total cost, and product performance. The proposed method shows promise for creating a product platform portfolio from a set of candidate component platforms that is most cost-effective within an existing product line. The proposed method allows for arbitrary leveraging as it does not rely solely on the traditional vertical, horizontal, or beachhead strategies advocated for the market segmentation grid, and this is especially beneficial when applied to an existing product line that was develop one-at-a-time time such that artifact designs are inconsistent from one to another. Copyright © 2008 by ASME.},
author_keywords={Activity-based costing (ABC);  Commonality;  Manufacturing cost;  Product line redesign;  Product platform},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Farias2012676,
author={Farias, K. and Garcia, A. and Whittle, J. and Chavez, C. and Lucena, C.},
title={Evaluating the effort of composing design models: A controlled experiment},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7590 LNCS},
pages={676-691},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84867644226&partnerID=40&md5=58f0ce4a690fa41bcdc233cb1ca400b9},
abstract={The lack of empirical knowledge about the effects of model composition techniques on developers' effort is the key impairment for their widespread adoption in practice. This problem applies to both existing categories of model composition techniques, i.e. specification-based (e.g. Epsilon) and heuristic-based (e.g. IBM RSA) techniques. This paper reports on a controlled experiment that investigates the effort to: (1) apply both categories of model composition techniques, and (2) detect and resolve inconsistencies in the output composed models. The techniques are investigated in 144 evolution scenarios, where 2304 compositions of elements of class diagrams were produced. The results suggest that: (1) the employed heuristic-based techniques require less effort to produce the intended model than the chosen specification-based technique, (2) the correctness of the output composed models generated by the techniques is not significantly different, and (3) the use of manual heuristics for model composition outperforms their automated counterparts. © 2012 Springer-Verlag.},
author_keywords={effort measurement;  empirical studies;  Model composition effort},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Farrell20103299,
author={Farrell, R.S. and Simpson, T.W.},
title={Improving cost effectiveness in an existing product line using component product platforms},
journal={International Journal of Production Research},
year={2010},
volume={48},
number={11},
pages={3299-3317},
note={cited By (since 1996)8},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77951108052&partnerID=40&md5=abedbb472f1b99a8930f422d47f01926},
abstract={Previously, we introduced a new method for improving commonality in a highly customised, low volume product line using component product platforms. The method provides a bottom-up platform approach to redesign family members originally developed one-at-a-time to meet specific customer requirements. In this paper, we extend the method with an activity-based costing (ABC) model to specifically capture the manufacturing costs in the product line, including the cost associated with implementing a platform strategy. The valve yoke example is revisited in this paper, the customised ABC model is defined, two design strategy alternatives are addressed, and the new method is used to determine which alternative is better at resolving the trade-off between commonality, total cost, and product performance. The proposed method shows promise for creating a product platform portfolio from a set of candidate component platforms that is most cost-effective within an existing product line. The proposed method allows for arbitrary leveraging as it does not rely solely on the traditional vertical, horizontal, or beachhead strategies advocated for the market segmentation grid, and this is especially beneficial when applied to an existing product line that was developed one-at-a-time time such that artefact designs are inconsistent from one to another. © 2010 Taylor & Francis.},
author_keywords={Activity-based costing (ABC);  Commonality;  Product line redesign;  Product platform},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bjarnason201030,
author={Bjarnason, E. and Wnuk, K. and Regnell, B.},
title={Overscoping: Reasons and consequences - A case study on decision making in software product management},
journal={2010 4th International Workshop on Software Product Management, IWSPM 2010},
year={2010},
pages={30-39},
art_number={5623866},
note={cited By (since 1996)5},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650385240&partnerID=40&md5=ffe1f56bfc12fd31130da16b2271d220},
abstract={Efficient scope management is a core part of software release management and often a key factor in releasing successful software products to the market. In a case when not all the requirements for the next software product release are known 'a priori' and when new requirements are issued throughout the project, the risk of overscoping by including more functionality than can be implemented increases. In this paper, we report on findings from an empirical interview study about understanding the causes and effects of overscoping in a large-scale industrial set up. Six main causes of overscoping have been identified in this work, complemented by root cause analysis of the causes and concluded by effects of overscoping. The results provide an increased understanding of the scoping activity as a continuous activity and outline risks and issues that can lead to a situation of overscoping. ©2010 IEEE.},
author_keywords={Case study;  Empirical study;  Requirements scoping;  Software release planning},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chatzoglou1998211,
author={Chatzoglou, P.D. and Macaulay, L.A.},
title={A Rule-Based Approach to Developing Software Development Prediction Models},
journal={Automated Software Engineering},
year={1998},
volume={5},
number={2},
pages={211-243},
note={cited By (since 1996)5},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0032047894&partnerID=40&md5=4c85991c3823d8d09db22a1ea1fa3a4c},
abstract={Managers of software development projects increasingly recognize the importance of planning and estimation and now have many sophisticated tools at their disposal. Despite this many systems are still delivered way behind schedule, cost far more to produce than original budget estimates and fail to meet user requirements. It is the contention of the authors that many existing tools are inadequate because they fail to embrace the significant body of knowledge accumulated by past and present project managers. This paper presents a new approach to planning which enables project managers to learn from the experience of others. The authors have adopted a bottom-up approach to planning which goes from the specific (planning the requirements capture and analysis process - RCA) to the general (planning the whole development process). A model, called MARCS, was constructed to give predictions of the resources (time, effort, cost, people) needed for the completion of and outcomes of the RCA process. Based on the predictions about the RCA process, the model then attempts to predict the resources and outcomes of the whole development process. MARCS is a combination of rule-based models and its main advantage is that it incorporates both qualitative and quantitative factors that can be easily identified and measured in the beginning of the development process. Empirical data concerning 107 projects developed by more than 70 organizations within UK, gathered through a two-stage mail survey was used for the construction and validation of the MARCS planning model.},
author_keywords={Decision rules;  IS project planning;  Planning models;  Requirements},
document_type={Article},
source={Scopus},
}

@ARTICLE{Luo20111345,
author={Luo, X.-G. and Cai, L.-Q. and Kwong, C.K.},
title={Multi-objective optimization method for product family design},
journal={Jisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS},
year={2011},
volume={17},
number={7},
pages={1345-1355},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80051553118&partnerID=40&md5=711e882c281475b8174850a260b7d860},
abstract={To help the product development team select optimization solutions with multiple objectives interactively in the product family design process, a multi-objective optimization model for product family design was proposed, and multi-objective genetic algorithm was designed to solve the proposed model. Factors such as market segmentation, product utility, competitive products, consumer purchasing behavior and product development cost were considered in the modeling process. A case study was provided to evaluate the feasibility of the proposed model and method.},
author_keywords={Conjoint analysis;  Genetic algorithms;  Multi-objective optimization;  Product development;  Product family},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Farrell20081031,
author={Farrell, R.S. and Simpson, T.W.},
title={Improving cost effectiveness in an existing product line using component product platforms},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={2008},
volume={1},
number={PARTS A AND B},
pages={1031-1041},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-81155138262&partnerID=40&md5=431ae1462837f93b29992d3c7ea7af24},
abstract={Previously, we introduced a new method for improving commonality in a highly customized, low volume product line using component product platforms. The method provides a bottom-up platform approach to redesign family members originally developed one-at-a-time to meet specific customer requirements. In this paper, we extend the method with an Activity-Based Costing (ABC) model to specifically capture the manufacturing costs in the product line, including the cost associated with implementing a platform strategy. The valve yoke example is revisited in this paper, the customized ABC model is defined, two design strategy alternatives are addressed, and the new method is used to determine which alternative is better at resolving the tradeoff between commonality, total cost, and product performance. The proposed method shows promise for creating a product platform portfolio from a set of candidate component platforms that is most cost-effective within an existing product line. The proposed method allows for arbitrary leveraging as it does not rely solely on the traditional vertical, horizontal, or beachhead strategies advocated for the market segmentation grid, and this is especially beneficial when applied to an existing product line that was develop one-at-a-time time such that artifact designs are inconsistent from one to another. Copyright © 2008 by ASME.},
author_keywords={Activity-based costing (ABC);  Commonality;  Manufacturing cost;  Product line redesign;  Product platform},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mohagheghi20091646,
author={Mohagheghi, P. and Dehlen, V. and Neple, T.},
title={Definitions and approaches to model quality in model-based software development - A review of literature},
journal={Information and Software Technology},
year={2009},
volume={51},
number={12},
pages={1646-1669},
note={cited By (since 1996)18},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-70349567623&partnerID=40&md5=c3fe6a55979e68095e09b35157dc79ee},
abstract={More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research. © 2009 Elsevier B.V. All rights reserved.},
author_keywords={Model quality;  Model-driven development;  Modelling;  Systematic review;  UML},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sun2011327,
author={Sun, Y. and Tao, X. and Zhang, C. and Zhang, X.},
title={The estimate model improvement of software cost based on fuzzy matrix technology},
journal={Advanced Materials Research},
year={2011},
volume={186},
pages={327-331},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79751509568&partnerID=40&md5=8b98abf726f3c98f0371c50d7f77b602},
abstract={Combining the current situation of China's domestic software enterprises, the cost driving factors of COCOMO model is improved to enhance the the accuracy of the estimate results by the fuzzy matrix method in this paper. In virtue of fuzzy matrix and fuzzy linear transformation, not only the fuzzy comprehensive ablity's quantitative value is got, but also sigle fuzzy ability's quantitative value is got. The type of software personnel can be distinguished by comparing the fuzzy integrated ability assessing valur of software personnel with pre-set threshold, enhancing the the accuracy of the estimate results.},
author_keywords={Cost driving factors;  Cost estimate;  Fuzzy matrix},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2010276,
author={Wang, H. and Hou, L. and Liu, C. and Wang, Q.},
title={Cost estimation model of modular product family},
journal={Applied Mechanics and Materials},
year={2010},
volume={37-38},
pages={276-279},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650908721&partnerID=40&md5=635035a7a0629d8ba52038c9b125b2aa},
abstract={For the more effective cost management and control of product platform, on the basis of modular product family (MPF) realization process and the cost of product family, the cost estimation model of modular product family was proposed. The manufacturing and management cost of component part level, module level and product level was estimated, and summed up to the cost of product family. Finally, a case of wheeling loader product family was illustrated to demonstrate the validity of the model. © (2010) Trans Tech Publications.},
author_keywords={Cost estimation;  Mass customization;  Modular product family (MPF);  Product platform},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cooper1988,
author={Cooper, J.C. and Suver, J.D.},
title={Product line cost estimation: a standard cost approach.},
journal={Healthcare financial management : journal of the Healthcare Financial Management Association},
year={1988},
volume={42},
number={4},
pages={60, 62, 64 passim},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0023991775&partnerID=40&md5=8f73696fdc7a032d71f0fb5d448e42bb},
abstract={Product line managers often must make decisions based on inaccurate cost information. A method is needed to determine costs more accurately. By using a standard costing model, product line managers can better estimate the cost of intermediate and end products, and hence better estimate the costs of the product line.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Boulding2009958,
author={Boulding, W. and Christen, M.},
title={Pioneering plus a broad product line strategy: Higher profits or deeper losses?},
journal={Management Science},
year={2009},
volume={55},
number={6},
pages={958-967},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-67651083306&partnerID=40&md5=a6c35cb89d7cbed8d338738afaa27d81},
abstract={Previous research suggests firms can build a market share advantage by preempting later entrants with a broad product line and expanding rapidly into related markets. Whether such a strategy leads to a pioneering profit advantage relative to followers also depends on its cost effects. In this paper, we examine when the market share advantage of a pioneering firm with a broad product line strategy translates into a profit advantage by examining the cost effects of this strategy. Using the profit impact of marketing strategies data and an estimation method that controls for various unobserved factors, we find significant differences between different industry settings. From these contrasting findings, we generate an emerging theoretical framework that we subject to empirical testing. We conjecture, and empirically verify, that creating a broad product line with a versioning strategy-creating variety from a standard product in anticipating customer demand-does not increase the pioneering cost disadvantage, and thus results in a pioneering profit advantage. On the other hand, with a tailoring strategy-creating variety by customizing a product to actual customer demand-a broad product line substantially increases the pioneering cost disadvantage, thereby making a preemption strategy counterproductive.},
author_keywords={Business unit profitability;  IV estimation;  Pioneering;  Preemption;  Product line strategy},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gorschek201283,
author={Gorschek, T. and Gomes, A. and Pettersson, A. and Torkar, R.},
title={Introduction of a process maturity model for market-driven product management and requirements engineering},
journal={Journal of software: Evolution and Process},
year={2012},
volume={24},
number={1},
pages={83-113},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84863310297&partnerID=40&md5=311dc5e92249abdf5a00aad510aeb8da},
abstract={The area of software product development of software intensive products has received much attention, especially in the area of requirements engineering and product management. Many companies are faced with new challenges when operating in an environment where potential requirements number in thousands or even tens of thousands, and where a product does not have a customer, but any number of customers or markets. The development organization carries not only all the costs of development, but also takes all the risks. In this environment traditional bespoke requirements engineering, together with traditional process assessment and improvement models fall short as they do not address the unique challenges of a marketdriven environment. This paper introduces the Market-driven Requirements Engineering Process Model, aimed at enabling process improvement and process assurance for organizations faced with these new challenges. The model is also validated in the industry through three case studies where the model is used for process assessment and improvement suggestion. Initial results show that the model is appropriate for process improvement for organizations operating in a market-driven environment. In addition, the model was designed to be light weight in terms of low cost and thus adapted not only for large organizations but suitable for small and medium enterprises as well. © 2011 John Wiley & Sons, Ltd.},
author_keywords={Market driven;  Maturity model;  Process assessment;  Process improvement;  Requirements engineering;  Software product management;  Technology product management},
document_type={Article},
source={Scopus},
}

@ARTICLE{Simon20131,
author={Simon, D. and Fischbach, K. and Schoder, D.},
title={An exploration of enterprise architecture research},
journal={Communications of the Association for Information Systems},
year={2013},
volume={32},
number={1},
pages={1-71},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84877089831&partnerID=40&md5=cf2efac6ed7cfe74f38700ad820c2d24},
abstract={Management of the enterprise architecture has become increasingly recognized as a crucial part of both business and IT management. Still, a common understanding and methodological consistency seems far from being developed. Acknowledging the significant role of research in moving the development process along, this article employs different bibliometric methods, complemented by an extensive qualitative interpretation of the research field, to provide a unique overview of the enterprise architecture literature. After answering our research questions about the collaboration via co-authorships, the intellectual structure of the research field and its most influential works, and the principal themes of research, we propose an agenda for future research based on the findings from the above analyses and their comparison to empirical insights from the literature. In particular, our study finds a considerable degree of co-authorship clustering and a positive impact of the extent of co-authorship on the diffusion of works on enterprise architecture. In addition, this article identifies three major research streams and shows that research to date has revolved around specific themes, while some of high practical relevance receive minor attention. Hence, the contribution of our study is manifold and offers support for researchers and practitioners alike. © 2013 by the Association for Information Systems.},
author_keywords={Enterprise architecture;  Information technology management;  Scientometrics;  Survey},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jackson2001149,
author={Jackson, T.},
title={Using computerised patient-level costing data for setting DRG weights: The Victorian (Australia) cost weight studies},
journal={Health Policy},
year={2001},
volume={56},
number={2},
pages={149-163},
note={cited By (since 1996)31},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0035105908&partnerID=40&md5=38cc0d6dc36059c5499a8d028bbf1874},
abstract={Casemix-funding systems for hospital inpatient care require a set of resource weights which will not inadvertently distort patterns of patient care. Few health systems have very good sources of cost information, and specific studies to derive empirical cost relativities are themselves costly. This paper reports a 5 year program of research into the use of data from hospital management information systems (clinical costing systems) to estimate resource relativities for inpatient hospital care used in Victoria's DRG-based payment system. The paper briefly describes international approaches to cost weight estimation. It describes the architecture of clinical costing systems, and contrasts process and job costing approaches to cost estimation. Techniques of data validation and reliability testing developed in the conduct of four of the first five of the Victorian Cost Weight Studies (1993-1998) are described. Improvement in sampling, data validity and reliability are documented over the course of the research program, the advantages of patient-level data are highlighted. The usefulness of these byproduct data for estimation of relative resource weights and other policy applications may be an important factor in hospital and health system decisions to invest in clinical costing technology. Copyright © 2001 Elsevier Science Ireland Ltd.},
author_keywords={Casemix;  Cost accounting;  DRGs;  Hospital costs},
document_type={Review},
source={Scopus},
}

@ARTICLE{Tyagi20121519,
author={Tyagi, S. and Yang, K. and Tyagi, A. and Verma, A.},
title={A fuzzy goal programming approach for optimal product family design of mobile phones and multiple-platform architecture},
journal={IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
year={2012},
volume={42},
number={6},
pages={1519-1530},
art_number={6204352},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84871722398&partnerID=40&md5=ed6a5f35f453cf2eda5e006218fe9757},
abstract={Competitiveness of any manufacturing industry depends on its ability to respond quickly to market niches and to produce a variety of high-utility products at relatively low costs. The promising tool to achieve aforesaid goals is the development of an efficient product family design strategy. The collection of shared components across the product family is termed as a platform that allows the saving in additional cost. Unfortunately, a single platform is advantageous only up to a certain extent; firms have sensed the requirement of multiple platforms. In this context, this paper deals with the exploration of product family design and multiple-platform architecture with a view to maximize the overall utility and to minimize the total production cost. This multiobjective problem has two conflicting and incommensurate objectives; therefore, a fuzzy goal programming model is adopted for modeling. The adoption of fuzzy goal programming model aids in combining the two objectives as well as captures the inherent uncertainty involved in decision making. The problem is formulated as a mixed integer program, and, additionally, random search optimization techniques, namely, genetic algorithm, simulated annealing, and Tabu search are being used to resolve the underlying issues. Moreover, in order to illustrate the proposed framework, a hypothetical case study-a family of mobile phones-is considered. Extensive experiments are performed on the underlying case study, and computational results are reported to validate the efficacy of multiple platforms over the single platform. © 1998-2012 IEEE.},
author_keywords={Conjoint analysis;  fuzzy goal programming;  multiple-platform architecture;  product family design;  random search optimization techniques (RSOTs)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sarang2008366,
author={Sarang, N. and Sanglikar, M.A.},
title={An analysis of effort variance in software maintenance projects},
journal={Advances in Computer and Information Sciences and Engineering},
year={2008},
pages={366-371},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650136084&partnerID=40&md5=e1c558583d1fb0221669b927efb84b37},
abstract={Quantitative project management, understanding process variations and improving overall process capability, are fundamental aspects of process improvements and are now strongly propagated by all best-practice models of process improvement. Organizations are moving to the next level of quantitative management where empirical methods are used to establish process predictability, thus enabling better project planning and management. In this paper we use empirical methods to analyze Effort Variance in software maintenance projects. The Effort Variance model established was used to identify process improvements and baseline performance. © Springer Science+Business Media B.V. 2008.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dehaghani201363,
author={Dehaghani, S.M.H. and Hajrahimi, N.},
title={Which factors affect software projects maintenance cost more?},
journal={Acta Informatica Medica},
year={2013},
volume={21},
number={1},
pages={63-66},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84874978096&partnerID=40&md5=a687d2482c6b68a1632c11b5490636ef},
abstract={Introduction: The software industry has had significant progress in recent years. T he entire life of software includes two phases: production and maintenance. Software maintenance cost is increasingly growing and estimates showed that about 90% of software life cost is related to its maintenance phase. Extraction and considering the factors affecting the software maintenance cost help to estimate the cost and reduce it by controlling the factors. Methods: In this study, the factors affecting software maintenance cost were determined then were ranked based on their priority and after that effective ways to reduce the maintenance costs were presented. This paper is a research study. 15 software related to health care centers information systems in Isfahan University of Medical Sciences and hospitals function were studied in the years 2010 to 2011. Results and discussion: Among Medical software maintenance team members, 40 were selected as sample. After interviews with experts in this field, factors affecting maintenance cost were determined. In order to prioritize the factors derived by AHP, at first, measurement criteria (factors found) were appointed by members of the maintenance team and eventually were prioritized with the help of EC software. Based on the results of this study, 32 factors were obtained which were classified in six groups. "Project" was ranked the most effective feature in maintenance cost with the highest priority. By taking into account some major elements like careful feasibility of IT projects, full documentation and accompany the designers in the maintenance phase good results can be achieved to reduce maintenance costs and increase longevity of the software. © AVICENA 2013.},
author_keywords={AHP model;  Cost;  Effective factors;  Health information systems;  Software maintenance},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2010,
title={Proceedings - 2010 IEEE International Conference on Software Maintenance, ICSM 2010},
journal={IEEE International Conference on Software Maintenance, ICSM},
year={2010},
page_count={612},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650123416&partnerID=40&md5=306df77cde6fe0c2c031d3f853558b80},
abstract={The proceedings contain 83 papers. The topics discussed include: using clone detection to identify bugs in concurrent software; revisiting common bug prediction findings using effort-aware models; conversion of fast inter-procedural static analysis to model checking; cost drivers of software corrective maintenance: an empirical study in two companies; automatic test case selection and generation for regression testing of composite service based on extensible BPEL flow graph; software modularization operators; automatically repairing test cases for evolving method declarations; MAGISTER: quality assurance of magic applications for software developers and end users; improved size and effort estimation models for software maintenance; log filtering and interpretation for root cause analysis; history-sensitive recovery of product line features; and automatic verification of loop invariants.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Liu2011891,
author={Liu, Z. and Wong, Y.S. and Lee, K.S.},
title={A manufacturing-oriented approach for multi-platforming product family design with modified genetic algorithm},
journal={Journal of Intelligent Manufacturing},
year={2011},
volume={22},
number={6},
pages={891-907},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84855504844&partnerID=40&md5=f4826488701e8682c80e3041053397ab},
abstract={With highly fragmented market and increased competition, platform-based product family design has been recognized as an effective method to construct a product line that satisfies diverse customer's demands while aiming to keep design and production cost-effective. The success of the resulting product family often relies on properly resolving the inherent tradeoff between commonality across the family and performance loss. In this paper, a systematic multi-platforming product family approach is proposed to design a scale-based product family. In the light of the basic premise that increased commonality implies enhanced manufacturing efficiency, we present an effective platform decision strategy to quantify family design configuration using a commonality index that couples design varieties with production variation. Meanwhile, unlikemany existingmethods that assume a single given platform configuration, the proposed method addresses the multi-platforming configuration across the family, and can generate alternative product family solutions with different levels of commonality. A modified genetic algorithm is developed to solve the aggregated multiobjective optimization problem and an industrial example of a planetary gear train for drills is given to demonstrate the proposed method. © Springer Science+Business Media, LLC 2009.},
author_keywords={Commonality index;  Design for manufacturing;  Genetic algorithm;  Multi-platforming;  Product family},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Park2005959,
author={Park, J. and Simpson, T.W.},
title={AN activity-based costing method for product family design in the early of development},
journal={Proceedings of the ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference - DETC2005},
year={2005},
volume={2 B},
pages={959-968},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33144485219&partnerID=40&md5=aec8ffd6f20d5c1b9d5128619edfc4c9},
abstract={As the marketplace is changing so rapidly, it becomes a key issue for companies to best meet customers' diverse demands by providing a variety of products in a cost-effective and timely manner. In the meantime, an increasing variety of capability and functionality of products has made it more difficult for companies that develop only one product at a time to maintain competitive production costs and reclaim market share. By designing a product family based on a robust product platform, overall production cost can be more competitive than competitors selling one product at a time while delivering highly differentiated products. In order to design cost-effective product families and product platforms, we are developing a production cost estimation framework in which relevant costs are collected, estimated, and analyzed. Since the framework is quite broad, this paper is dedicated to refining the estimation framework in a practical way by developing an activity-based costing (ABC) system in which activity costs are mapped to individual parts in the product family, which is called cost modularization, and the activity costs affected by product family design decisions are reconstructed to make the costs relevant to these decisions. A case study involving a family of power tools is used to demonstrate the proposed use of the ABC system. Copyright © 2005 by ASME.},
author_keywords={Activity-Based Costing (ABC) System;  Cost Modularization;  Product Families;  Product Platforms},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Park200899,
author={Park, J. and Simpson, T.W.},
title={Toward an activity-based costing system for product families and product platforms in the early stages of development},
journal={International Journal of Production Research},
year={2008},
volume={46},
number={1},
pages={99-130},
note={cited By (since 1996)12},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-36249000655&partnerID=40&md5=a8682e111f03119ca2727b3fa7db837d},
abstract={As the marketplace has been changing so rapidly, it has been a key issue for companies to best meet customers' diverse demands by providing a variety of products in a cost-effective and timely manner. In the meantime, an increasing variety of capability and functionality of products has made it more difficult for companies that develop only one product at a time to maintain competitive production cost and reclaim market share. By designing a product family based on a robust product platform, overall production cost can be more competitive than competitors selling one product at a time while delivering highly differentiated products. In order to design cost-effective product families and product platforms, a production cost estimation framework was presented in order that relevant costs are collected, estimated, and analysed. Since the framework is quite broad, this paper is dedicated to refining the estimation framework in a practical way by developing an activity-based costing (ABC) system in which activity costs are mapped to individual parts in the product family, which is called cost modularization, and the activity costs affected by product family design decisions are restructured to make the costs relevant to these decisions. A case study involving a family of power tools is used to demonstrate the proposed use of the ABC system.},
author_keywords={Activity-based costing (ABC);  Cost modularization;  Platforms;  Product families},
document_type={Article},
source={Scopus},
}

@ARTICLE{Huang2011115,
author={Huang, Y. and Huang, G.Q. and Newman, S.T.},
title={Coordinating pricing and inventory decisions in a multi-level supply chain: A game-theoretic approach},
journal={Transportation Research Part E: Logistics and Transportation Review},
year={2011},
volume={47},
number={2},
pages={115-129},
note={cited By (since 1996)6},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650305416&partnerID=40&md5=41a7d3d41e4bb4ea0a93fe83dec56c51},
abstract={This paper concerns coordination of enterprise decisions such as suppliers and components selection, pricing and inventory in a multi-level supply chain composed of multiple suppliers, a single manufacturer and multiple retailers. The problem is modeled as a three-level dynamic non-cooperative game. Analytical and computational methods are developed to determine the Nash equilibrium of the game. Finally, a numerical study in computer industry is conducted to understand the influence of the market scale parameter and the components selection strategy on the optimal decisions and profits of the supply chain as well as its constituent members. Several research findings have been obtained. © 2010 Elsevier Ltd.},
author_keywords={Dynamic non-cooperative game;  Inventory;  Multi-level supply chain;  Nash equilibrium;  Pricing;  Product family design},
document_type={Article},
source={Scopus},
}

@ARTICLE{Siddique2001285,
author={Siddique, Z. and Repphun, B.},
title={Estimating cost savings when implementing a product platform approach},
journal={Concurrent Engineering Research and Applications},
year={2001},
volume={9},
number={4},
pages={285-293},
note={cited By (since 1996)10},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0035744436&partnerID=40&md5=ad2109000ab5f594e9d2da78959f7819},
abstract={Many market forces are driving companies to improve their targeting of increasingly small market niches. To accomplish this efficiently, products are organized into product families that typically share common platforms. To reorganize the current product offerings or new products into a product family, using a platform approach, requires estimating the savings for such a modification. One of the problems encountered in estimating development and design cost is the lack of availability of hard information during the initial design phases. The purpose of this paper is to estimate the design and development cost, when moving towards a platform approach, using simple models. The activity based product family cost models are developed from existing single product design activities, which are modified and extended to reflect activities related to development of product platform and subsequent product family members supported by the platform. Uncertainty related to cost associated with activities are included in the model, which is solved using Monte Carlo simulation. The approach is demonstrated using a hard disk drive spindle motor platform development for a family of hard disks.},
author_keywords={Activity based costing;  Cost estimation;  Product family design},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Conte2007146,
author={Conte, T. and Massollar, J. and Mendes, E. and Travassos, G.H.},
title={Usability evaluation based on Web design perspectives},
journal={Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007},
year={2007},
pages={146-155},
art_number={4343742},
note={cited By (since 1996)8},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-47949094621&partnerID=40&md5=5d201c744fc0c502e4c3c1dea5fac1a0},
abstract={Given the growth in the number and size of Web Applications worldwide, Web quality assurance, and more specifically Web usability have become key success factors. Therefore, this work proposes a usability evaluation technique based on the combination of Web design perspectives adapted from existing literature, and heuristics. This new technique is assessed using a controlled experiment aimed at measuring the efficiency and effectiveness of our technique, in comparison to Nielsen's heuristic evaluation. Results indicated that our technique was significantly more effective than and as efficient as Nielsen's heuristic evaluation. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ataie20111,
author={Ataie, E. and Jelodar, M.B. and Aghaei, F.},
title={Analysis of quality driven software architecture},
journal={Communications in Computer and Information Science},
year={2011},
volume={241 CCIS},
pages={1-14},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-83755171390&partnerID=40&md5=f22d402dff581d6c9ce6273c60491dfc},
abstract={This paper presents an analysis on quality driven approaches which embodies non-functional requirements into software architecture design. The analysis characterizes vocabularies and concepts of the area, with exhibiting a comparison of the two main techniques. In the first technique, architectural tactics are represented and their semantics is clearly defined as a UML-based pattern specification notation called RBML. Given a set of non-functional requirements, architectural tactics are selected and composed into an initial architecture for the application. The second technique designates some attribute primitives which are similar to architectural patterns. It then introduces a method called Attribute Driven Design, to involve attribute primitives for satisfying a set of general scenarios. In this analysis, we intend to give a brief description of the both approaches. © 2011 Springer-Verlag.},
author_keywords={Architecture;  Functional Requirement;  Non-functional Requirement;  Quality;  Software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chowdhury2011,
author={Chowdhury, S. and Messac, A. and Khire, R.A.},
title={Comprehensive product platform planning (CP 3) framework},
journal={Journal of Mechanical Design, Transactions of the ASME},
year={2011},
volume={133},
number={10},
art_number={101004},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80455132535&partnerID=40&md5=2a04b0e5b6632266701b4a07d7555239},
abstract={Development of a family of products that satisfies different market niches introduces significant challenges to today's manufacturing industries-from development time to aftermarket services. A product family with a common platform paradigm offers a powerful solution to these daunting challenges. This paper presents a new approach, the Comprehensive Product Platform Planning (CP 3) framework, to design optimal product platforms. The CP 3 framework formulates a generalized mathematical model for the complex platform planning process. This model (i) is independent of the solution strategy, (ii) allows the formation of sub-families of products, (iii) allows the simultaneous identification of platform design variables and the determination of the corresponding variable values, and (iv) seeks to avoid traditional distinctions between modular and scalable product families from the optimization standpoint. The CP 3 model yields a mixed integer nonlinear programming problem, which is carefully reformulated to allow for the application of continuous optimization using a novel Platform Segregating Mapping Function (PSMF). The PSMF can be employed using any standard global optimization methodology (hence not restrictive); particle swarm optimization has been used in this paper. A preliminary cost function is developed to represent the cost of a product family as a function of the number of products manufactured and the commonality among these products. The proposed CP 3 framework is successfully implemented on a family of universal electric motors. Key observations are made regarding the sensitivity of the optimized product platform to the intended production volume. © 2011 American Society of Mechanical Engineers.},
author_keywords={commonality matrix;  MINLP;  optimization;  particle swarm;  platform;  product family},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Park2003165,
author={Park, J. and Simpson, T.W.},
title={Production cost modeling to support product family design optimization},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={2003},
volume={2 A},
pages={165-174},
note={cited By (since 1996)2},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-1842660794&partnerID=40&md5=d15830cb19efe6f65ba4dfb41ba0521f},
abstract={Product family design involves carefully balancing the commonality of the product platform with the distinctiveness of the individual products in the family. While a variety of optimization methods have been developed to help designers determine the best design variable settings for the product platform and individual products within the family, production costs are thought to be an important criterion to choose the best platform among candidate platform designs. Thus, it is prerequisite to have an appropriate production cost model to be able to estimate the production costs incurred by having common and variant components within a product family. In this paper, we propose a production cost model based on a production cost framework associated with the manufacturing activities. The production cost model can be easily integrated within optimization frameworks to support a Decision-Based Design approach for product family design. As an example, the production cost model is utilized to estimate the production costs of a family of cordless power screwdrivers.},
author_keywords={Decision-Based Design;  Product Family;  Product Platform;  Production Cost Model},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khajaviniai2009379,
author={Khajaviniai, R.},
title={The basis for building a business case in software development, a case study},
journal={IEEE EUROCON 2009, EUROCON 2009},
year={2009},
pages={379-385},
art_number={5167659},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-70449625298&partnerID=40&md5=5681b40e13127d3c6071823aa5096539},
abstract={In many software companies, software engineers and business decision makers live in separate worlds, using their own terminology, decision criteria, and working methods. Building a business case is one possible way to bridge the gap between business and software engineering and to increase the quality and the profitability of software development. Main empirical findings of this study are that case companies used a software business case to allocate resources between concurrent projects, to support sales and pricing activities and to identify the technical platform of their customers' products. A business case was considered as an important part of the software development process. This paper presents the rationale for using a business case for software projects and IT services. We also present that a business case can decrease the number of service barriers, factors that affect negatively a potential IT service end user. Our results are based on both the empirical data from interviews with three case companies within our research project and the literature review. © 2009 IEEE.},
author_keywords={Business case;  Business models;  ITIL;  Software development;  UML},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li2007387,
author={Li, M.},
title={TRISO-model: A new approach to integrated software process assessment and improvement},
journal={Software Process Improvement and Practice},
year={2007},
volume={12},
number={5},
pages={387-398},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-35348918728&partnerID=40&md5=d10a12d43edf471e40bfc3e9bb8413d9},
abstract={The increasing complexity and dynamic change of software development have become the most critical challenges in the final success of a software project. As one of the new emerging methodologies to these problems, TRISO (TRidimensional Integrated SOftware)-Model uses an integrated 3-D structure to classify and organize the essential elements in software development. In order to evaluate and improve the integrated software development processes, a new TRISO-Model-based process assessment and improvement approach is proposed in this article. The article also introduces TRISO-Model implementation and its practice in the integrated software development processes in China. Copyright © 2007 John Wiley & Sons, Ltd.},
author_keywords={Integrated software development;  Software process assessment;  Software process improvement;  TRISO-model},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li201212,
author={Li, J. and Zhang, H. and Zhu, L. and Jeffery, R. and Wang, Q. and Li, M.},
title={Preliminary results of a systematic review on requirements evolution},
journal={IET Seminar Digest},
year={2012},
volume={2012},
number={1},
pages={12-21},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84865520818&partnerID=40&md5=d5e791ca2ff023a61462a4d89551b8f1},
abstract={Background: Software systems must evolve in order to adapt in a timely fashion to the rapid changes of stakeholder needs, technologies, business environment and society regulations. Numerous studies have shown that cost, schedule or defect density of a software project may escalate as the requirements evolve. Requirements evolution management has become one important topic in requirements engineering research. Aim: To depict a holistic state-of-the-art of requirement evolution management. Method: We undertook a systematic review on requirements evolution management. Results: 125 relevant studies were identified and reviewed. This paper reports the preliminary results from this review: (1) the terminology and definition of requirements evolution; (2) fourteen key activities in requirements evolution management; (3) twenty-eight metrics of requirements evolution for three measurement goals. Conclusions: Requirements evolution is a process of continuous change of requirements in a certain direction. Most existing studies focus on how to deal with evolution after it happens. In the future, more research attention on exploring the evolution laws and predicting evolution is encouraged.},
author_keywords={management process;  measurement;  requirements change;  requirements evolution;  systematic literature review},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Karania20071225,
author={Karania, R. and Kazmer, D.},
title={Low volume plastics manufacturing strategies},
journal={Journal of Mechanical Design, Transactions of the ASME},
year={2007},
volume={129},
number={12},
pages={1225-1233},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-38849108703&partnerID=40&md5=ddc49b9d72c9845468a5cebb9ffbe463},
abstract={Plastic components are vital components of many engineered products, frequently representing 20-40% of the product value. While injection molding is the most common process for economically producing complex designs in large quantities, a large initial monetary investment and extended development time are required to develop appropriate tooling. For applications with lower or unknown production quantities, designers may prefer another process that has a lower development cost and lead time albeit with higher marginal costs and production times. A methodology is presented that assists the designer to select the most appropriate manufacturing process that trades off the total production costs with production lead times. The approach is to develop aggregate component cost and lead-time models as a function of production quantity from extensive industry data for an electrical enclosure consisting of two components. Binding quotes were secured from multiple suppliers for a variety of manufacturing processes including computer numerical control machining, fused deposition modeling, selective laser sintering, vacuum casting, direct fabrication, and injection molding with soft prototype and production tooling. The methodology yields a Pareto optimal set that compares the production costs and lead times as a function of the production quantity. The results indicate that the average cost per enclosure assembly is highly sensitive to the production quantity, with average costs varying by more than a factor of 100 for production quantities varying between 100 and 10,000 assemblies. Each of the processes is competitive with respect to total production cost and total production lead time under differing conditions; a flow chart is provided as an example of a decision support tool that can be provided to assist process selection during the product development process and thereby reduce the product development time and cost. Copyright © 2007 by ASME.},
author_keywords={Injection molding;  Low volume production;  Plastic part design;  Rapid prototyping},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shahrokni20131,
author={Shahrokni, A. and Feldt, R.},
title={A systematic review of software robustness},
journal={Information and Software Technology},
year={2013},
volume={55},
number={1},
pages={1-17},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84867401202&partnerID=40&md5=d8fcf40fcfa6a024f97c711330f248ab},
abstract={Context: With the increased use of software for running key functions in modern society it is of utmost importance to understand software robustness and how to support it. Although there have been many contributions to the field there is a lack of a coherent and summary view. Objective: To address this issue, we have conducted a literature review in the field of robustness. Method: This review has been conducted by following guidelines for systematic literature reviews. Systematic reviews are used to find and classify all existing and available literature in a certain field. Results: From 9193 initial papers found in three well-known research databases, the 144 relevant papers were extracted through a multi-step filtering process with independent validation in each step. These papers were then further analyzed and categorized based on their development phase, domain, research, contribution and evaluation type. The results indicate that most existing results on software robustness focus on verification and validation of Commercial of the shelf (COTS) or operating systems or propose design solutions for robustness while there is a lack of results on how to elicit and specify robustness requirements. The research is typically solution proposals with little to no evaluation and when there is some evaluation it is primarily done with small, toy/academic example systems. Conclusion: We conclude that there is a need for more software robustness research on real-world, industrial systems and on software development phases other than testing and design, in particular on requirements engineering. © 2012 Elsevier B.V. All rights reserved.},
author_keywords={Robustness;  Software robustness;  Systematic review},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu20103657,
author={Liu, Z. and Wong, Y.S. and Lee, K.S.},
title={Modularity analysis and commonality design: A framework for the top-down platform and product family design},
journal={International Journal of Production Research},
year={2010},
volume={48},
number={12},
pages={3657-3680},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77951143241&partnerID=40&md5=90e30f826b772955677e4c0698bbc92a},
abstract={With a highly fragmented market and increased competition, platform-based product family design is recognised as an effective method for constructing a product line that satisfies diverse customer demand while keeping design and production cost- and time-effective. Recognising the need for modularity and commonality in platform development, this paper presents a systematic framework to assist in implementing top-down platform and product family design, which aims to achieve system-level modularity for variety generation, and rationalise the commonality configuration for module instantiation. In the first phase of platform development, a robust and flexible product family architecture is constructed to accommodate variations by analysing the external varieties of the generic product architecture, and provide a modularity design space, wherein the design tasks are further decomposed into module instantiation. The second phase of detailed platform development aims to enhance commonality in terms of engineering efficiency by coordinating with the back-end product realisation stage. A tractable optimisation method is used to capture and resolve the trade-off between commonality configuration and individual product performance. A family of power tool designs is used to demonstrate the potential and feasibility of the proposed framework at the system level and detailed design stages. © 2010 Taylor & Francis.},
author_keywords={Commonality;  Modularity;  Product development;  Product family design},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yan2006273,
author={Yan, W. and Chen, C.-H. and Shieh, M.-D.},
title={Product concept generation and selection using sorting technique and fuzzy c-means algorithm},
journal={Computers and Industrial Engineering},
year={2006},
volume={50},
number={3},
pages={273-285},
note={cited By (since 1996)10},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33747760003&partnerID=40&md5=bed0a4bd6abcf38b3db817cfe058eb78},
abstract={Product conceptualization is regarded as a key activity in new product development (NPD). In this stage, product concept generation and selection plays a crucial role. This paper presents a product concept generation and selection (PCGS) approach, which was proposed to assist product designers in generating and selecting design alternatives during the product conceptualization stage. In the PCGS, general sorting was adapted for initial requirements acquisition and platform definition; while a fuzzy c-means (FCM) algorithm was integrated with a design alternatives generation strategy for clustering design options and selecting preferred product concepts. The PCGS deliberates and embeds a psychology-originated method, i.e., sorting technique, to widen domain coverage and improve the effectiveness in initial platform formation. Furthermore, it successfully improves the FCM algorithm in such a way that more accurate clustering results can be obtained. A case study on a wood golf club design was used for illustrating the proposed approach. The results were promising and revealed the potential of the PCGS method. © 2006 Elsevier Ltd. All rights reserved.},
author_keywords={Design option;  Fuzzy c-means algorithm;  Product concept generation and selection;  Product platform;  Sorting technique},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chatzoglou1999503,
author={Chatzoglou, P.D. and Soteriou, A.C.},
title={A DEA framework to assess the efficiency of the software requirements capture and analysis process},
journal={Decision Sciences},
year={1999},
volume={30},
number={2},
pages={503-528},
note={cited By (since 1996)12},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0039523252&partnerID=40&md5=6ba42467407d4bea950f9fefbd364a6d},
abstract={In this paper a theoretical framework to assess the efficiency of the Requirements Capture and Analysis (RCA) process in software development is introduced. Although it is widely recognized that successful implementation of the first stages of the software development process is critical for the overall development process, RCA efficiency assessments have not been given much attention. The presented theoretical framework to assess RCA efficiency follows a production approach to model the early stages of a software project. An approach based on Data Envelopment Analysis that utilizes the proposed framework to isolate the effects of exogenous factors, such as the environment or the type of project, on the project's RCA efficiency is also presented. Finally, the applicability of the methodology through an exploratory empirical study is demonstrated, and managerial implications are discussed.},
author_keywords={Data envelopment analysis;  Requirements capture and analysis;  Software development},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2005695,
author={Zhang, H. and Jarzabek, S.},
title={A Bayesian Network approach to rational architectural design},
journal={International Journal of Software Engineering and Knowledge Engineering},
year={2005},
volume={15},
number={4},
pages={695-717},
note={cited By (since 1996)6},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-24644444230&partnerID=40&md5=a9a9e83eccf0d65634d41d0591b0d5d9},
abstract={In software architecture design, we explore design alternatives and make decisions about adoption or rejection of a design from a web of complex and often uncertain information. Different architectural design decisions may lead to systems that satisfy the same set of functional requirements but differ in certain quality attributes. In this paper, we propose a Bayesian Network based approach to rational architectural design. Our Bayesian Network helps software architects record and make design decisions. We can perform both qualitative and quantitative analysis over the Bayesian Network to understand how the design decisions influence system quality attributes, and to reason about rational design decisions. We use the KWIC (Key Word In Context) example to illustrate the principles of our approach. © World Scientific Publishing Company.},
author_keywords={Automated design;  Bayesian network;  Knowledge capture;  KWIC;  Software architecture design;  Software quality},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ueda2009681,
author={Ueda, K. and Takenaka, T. and Váncza, J. and Monostori, L.},
title={Value creation and decision-making in sustainable society},
journal={CIRP Annals - Manufacturing Technology},
year={2009},
volume={58},
number={2},
pages={681-700},
note={cited By (since 1996)25},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-70449527452&partnerID=40&md5=959e1eec4e65e48a73b6d5966653a1cd},
abstract={Manufacturing exists to create value. However, historically, discussion of economic issues in manufacturing primarily emphasizes cost. It is becoming more difficult to understand and control values of products and services in response to rapid globalization and networking. This paper presents a discussion of the nature of value considering a history of axiology, design problems of artifacts, social dilemmas, network externalities, and sustainability. Promising academic methodologies are presented herein with emphasis on transdisciplinary and synthetic approaches. Value creation models based on Emergent Synthesis and co-creative decision-making are presented. This paper involves some important study examples of service and production toward sustainable value creation in society. © 2009 CIRP.},
author_keywords={Emergent synthesis;  Sustainability;  Value creation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kulkarni2013253,
author={Kulkarni, V. and Sunkle, S.},
title={Toward innovative model based enterprise IT outsourcing},
journal={Lecture Notes in Business Information Processing},
year={2013},
volume={148 LNBIP},
pages={253-263},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84879856187&partnerID=40&md5=754096b0881528601109d1e65050e7f6},
abstract={There are signs that cost arbitrage model of outsourcing engagement between enterprises and service providers will have to change to one based on value generation. Automation decisions taken by siloized businesses under cost arbitrage models have led to complex cost-ineffective situations. Both these situations may be addressed by an innovative business model based on product family concepts that systematically targets transactional and transformational needs of enterprises with focus on value generation using analysis and operational world views of enterprise IT systems. We motivate and elaborate such an approach. Our contributions are innovation that leads to mutual win-win situation by enabling service providers to service IT needs of multiple enterprises of same vertical and by enabling enterprises to reap value-oriented benefits with analysis and operational world views of IT systems thus serviced. © 2013 Springer-Verlag.},
author_keywords={Analysis;  Enterprise Architecture;  Enterprise Modeling;  Operationalization;  Outsourcing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{DeMarsMartin198530,
author={De Mars Martin, P. and Boyer, F.J.},
title={Developing a consistent method for costing hospital services},
journal={Healthcare Financial Management},
year={1985},
volume={39},
number={2},
pages={30-37},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0021919085&partnerID=40&md5=bf92d70dd1bc5efc943bd160ecee36b0},
abstract={In the fall of 1983, Children's Memorial Hospital (CMH) in Chicago, Ill., recognized several developments that would require the calculation of patient care costs by product line.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nazareth2004245,
author={Nazareth, D.L. and Rothenberger, M.A.},
title={Assessing the cost-effectiveness of software reuse: A model for planned reuse},
journal={Journal of Systems and Software},
year={2004},
volume={73},
number={2},
pages={245-255},
note={cited By (since 1996)16},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-3242702428&partnerID=40&md5=27da1755fbc2d672884d4812f58ad7a1},
abstract={Information systems development is typically acknowledged as an expensive and lengthy process, often producing code that is of uneven quality and difficult to maintain. Software reuse has been advocated as a means of revolutionizing this process. The claimed benefits from software reuse are reduction in development cost and time, improvement in software quality, increase in programmer productivity, and improvement in maintainability. Software reuse entails undeniable costs of creating, populating, and maintaining a library of reusable components. There is anecdotal evidence to suggest that some organizations benefit from reuse. However, many software developers practicing reuse claim these benefits without formal demonstration thereof. There is little research to suggest when the benefits are expected and to what extent they will be realized. For example, does a larger library of reusable components lead to increased savings? What is the impact of component size on the effectiveness of reuse? This research seeks to address some of these questions. It represents the first step in a series wherein the effects of software reuse on overall development effort and costs are modeled with a view to understanding when it is most effective. © 2003 Elsevier Inc. All rights reserved.},
author_keywords={Economic model;  Metrics;  Productivity;  Reuse cost/benefit;  Reuse repository;  Software reuse},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Allada2006,
author={Allada, V. and Choudhury, A.K. and Pakala, P.K. and Simpson, T.W. and Scott, M.J. and Valliyappan, S.},
title={Product platform problem taxonomy: Classification and identification of benchmark problems},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={2006},
volume={2006},
page_count={13},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33751343705&partnerID=40&md5=960b505f3253f3fc4ad179a3bdb24fb5},
abstract={Many companies are using product platform concepts to gain economies of scale and to identify new market opportunities. Though the area of product platforming continues to be actively investigated by both industry and academia, there is no comprehensive classification scheme that can provide a clear picture of the existing problems and possible future research directions. Hence, in the present paper, we introduce a broad taxonomy that classifies product platform problems based on the product development stages. This can serve as a basis to: (1) Extract and categorize problems from research literature; (2) Identify potential extensions and/or new problems that have not been addressed in the literature; and (3) Identify existing problem sets and/or develop new problem sets for benchmarking purposes. We introduce a Conditions and Assumptions Code (CAC) scheme and use it in the identification of benchmark problems as well as in analyzing two classes of evaluation methods adopted for the platform problems: metrics-based and optimization-based Thus, we have not only categorized existing problems but also identified possible future research problems in each of the categories. This categorization serves as a navigation tool to understand the progress made in this field so far and to identify new research directions. Copyright © 2006 by ASME.},
author_keywords={Benchmarking problems;  Product platforms;  Strategic and tactical decisions;  Taxonomy},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cleverley198739,
author={Cleverley, W.O.},
title={Product costing for health care firms},
journal={Health Care Management Review},
year={1987},
volume={12},
number={4},
pages={39-48},
note={cited By (since 1996)2},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0023575308&partnerID=40&md5=a6966232370cedca1041baff62943a77},
abstract={The implementation of the prospective payment system has brought enormous interest in cost accounting, and this interest is not limited to the hospital industry. The interest in developing sophisticated cost accounting systems is present in all sectors of the health care industry. Most, if not all, of this interest is a reflection of fixed prices for services and increasing economic competition among health care providers. This article identifies a framework for the discussion and development of cost accounting systems in the health care sector. Great reliance is placed on the similarities rather than the differences in cost accounting systems as they exist in industry. The framework requires two sets of standards, one relating to the production of departmental products, and one relating to the treatment protocol for defined patient categories.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bosch-Mauchand2013453,
author={Bosch-Mauchand, M. and Belkadi, F. and Bricogne, M. and Eynard, B.},
title={Knowledge-based assessment of manufacturing process performance: Integration of product lifecycle management and value-chain simulation approaches},
journal={International Journal of Computer Integrated Manufacturing},
year={2013},
volume={26},
number={5},
pages={453-473},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84878714541&partnerID=40&md5=273792b0ea0660d4cf108e4b7df1f8ff},
abstract={Regarding the market globalisation, manufacturers have to improve the performance and efficiency of their production plants. To fulfil this goal, managers need methods and tools to assess their product development and their production engineering processes by assessing the performance and the value of the specified technical solutions. This article presents a new approach to support this assessment of manufacturing enterprise processes in terms of performance and value indicators based on knowledge management (KM) integration. Based on the principles of value chain, in one hand, and on methods of KM, on the other hand, the aim of the proposal is to help experts to make relevant decisions on product development and/or production process planning. The approach originality is to use of product lifecycle management (PLM) capabilities to achieve semi-automated capitalisation of heterogeneous knowledge from various enterprise information systems. Capitalised knowledge is then used as an input of the assessment module. This module implements a set of simulation algorithms for the assessment and evaluation of different alternatives of value chains according to required performance criteria. © 2013 Taylor & Francis Group, LLC.},
author_keywords={knowledge management;  product lifecycle management;  value-chain simulation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sarfaraz1993688,
author={Sarfaraz, Ahmad R. and Emamizadeh, Bahram},
title={Product costing for the concurrent engineering environment},
journal={Annual International Conference Proceedings - American Production and Inventory Control Society},
year={1993},
pages={688-690},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0027872699&partnerID=40&md5=e451227b9b973958dd170f588448b8b2},
abstract={The international competition created by recent advances in manufacturing systems has made companies change their product costing methods. Among them, concurrent engineering is appropriately used as a manufacturing strategy. It is the simultaneous design of product and the process required to produce and support it. In this paper, product costing for the concurrent engineering environment will be explained. When it is implemented, it can help cost management systems to compare profit margins of different product lines with related resources actually utilized.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Oumlil2008507,
author={Oumlil, A.B.},
title={Warranty planning and development framework: A case study of a high-tech multinational firm},
journal={Journal of Business and Industrial Marketing},
year={2008},
volume={23},
number={7},
pages={507-517},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-51749094613&partnerID=40&md5=3a06aa412907b8768856e36e20706f4c},
abstract={Purpose - The purpose of this paper is to address what a sound warranty policy entails by identifying the key variables involved in the development of a warranty program. Design/methodology/approach - The sample population was composed of employees in the US division involved with high-tech product warranties. A survey questionnaire was used to collect data from the participants. Findings - The paper finds that the formality of the warranty policy should depend on its complexity. Differences exist between types of warranty based on the product knowledge of the buyer. Although a standardized warranty is easy to administer, as the product line diversifies, it becomes more challenging to standardize. Research limitations/implications - This study can be expanded by examining how companies balance the cost/quality/warranty ability of the product, the techniques used to allocate warranty costs, and to evaluate multiple companies/industries, perhaps with a longitudinal focus. Practical implications - The formality can be used to communicate the product warranty throughout the organization. Each department has a responsibility to the customer, so team members from service, product development, and marketing should plan and develop the warranty. A standardized warranty can send a clearer message to a customer about a firm's products. Simplifying front and back-end processing and streamlining support structures can reduce costs. Originality/value - In this paper, the identified key variable is brought out in warranty management framework. The development of this framework will satisfy a current, critical need to provide guidelines with all the steps needed to develop a warranty policy. © Emerald Group Publishing Limited.},
author_keywords={Corporate strategy;  Customer orientation;  Servicing;  United States of America;  Warranties},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Thevenot20051009,
author={Thevenot, H.J. and Nanda, J. and Simpson, T.W.},
title={A methodology to support product family redesign using a genetic algorithm and commonality indices},
journal={Proceedings of the ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference - DETC2005},
year={2005},
volume={2 B},
pages={1009-1018},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33144481856&partnerID=40&md5=31c4c97246667f3dce44db1d55eef536},
abstract={Many of today's manufacturing companies are using platform-based product development to realize families of products with sufficient variety to meet customers' demands while keeping costs relatively low. The challenge when designing or redesigning a product family is in resolving the tradeoff between product commonality and distinctiveness. Several methodologies have been proposed to redesign existing product families; however, a problem with most of these methods is that they require a considerable amount of information that is not often readily available, and hence their use has been limited. In this research, we propose a methodology to help designers during product family redesign. This methodology is based on the use of a genetic algorithm and commonality indices - metrics to assess the level of commonality within a product family. Unlike most other research in which the redesign of a product family is the result of many human computations, the proposed methodology reduces human intervention and improves accuracy, repeatability, and robustness of the results. Moreover, it is based on data that is relatively easy to acquire. As an example, a family of computer mice is analyzed using the Product Line Commonality Index. Recommendations are given at the product family level (assessment of the overall design of the product family), and at the component level (which components to redesign and how to redesign them). The methodology provides a systematic methodology for product family redesign. Copyright © 2005 by ASME.},
author_keywords={Commonality Indices;  Genetic Algorithm;  Product Family Redesign},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Luo20114195,
author={Luo, X.G. and Kwong, C.K. and Tang, J.F. and Deng, S.F. and Gong, J.},
title={Integrating supplier selection in optimal product family design},
journal={International Journal of Production Research},
year={2011},
volume={49},
number={14},
pages={4195-4222},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79958839943&partnerID=40&md5=97cef97dea2d0e619d8ef9a4b25806ae},
abstract={Product family design and supplier selection are traditionally separated in two successive stages. First, product development teams determine optimal levels of the attributes of product components for each product variant of a product family, and purchasing departments then choose the qualified suppliers with the lowest cost. However, decoupling the two decision processes may lead to suboptimal solutions with regard to the total market profit of a company. In this article, a unified optimisation model, which integrates product family design with supplier selection, is established with the objective of maximising the total profit of a product family. Consumer purchase behaviour, supplier availability and outsourcing-related cost are considered in the model. A linear programming embedded genetic algorithm was developed to solve the proposed model. A case study is presented to illustrate the feasibility and effectiveness of the proposed model and algorithms. © 2011 Taylor & Francis.},
author_keywords={genetic algorithm;  product family;  product platform;  supplier selection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li200615,
author={Li, M.},
title={Assessing 3-D integrated software development processes: A new benchmark},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={3966 LNCS},
pages={15-38},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33745889021&partnerID=40&md5=3c6235e920bc7beb612e48a60609dc33},
abstract={The increasing complexity and dynamic of software development have become the most critical challenges for large projects. As one of the new emerged methodologies to these problems, TRISO-Model uses an integrated three-dimensional structure to classify and organize the essential elements in software development. In order to simulate and evaluate the modeling ability of TRISO-Model, a new benchmark is created in this paper, called SPW-2006 Example, by extending the ISPW-6 Example. It may be used to evaluate other software process models, and/or to evaluate software organizations, software projects and also software development processes, particularly 3-D integrated software development processes. With the SPW-2006 Example and its evolution for quantitative evaluation to 3-D integrated software development processes, a new approach of TRISO-Model based assessment and improvement is enabled. © Springer-Verlag Berlin Heidelberg 2006.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jciblonowski2011133,
author={Jciblonowski, C. and Ettehad, A. and Ogunyomi, B. and Srour, I.},
title={Integrating learning curves in probabilistic well-construction estimates},
journal={SPE Drilling and Completion},
year={2011},
volume={26},
number={1},
pages={133-138},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-79953097045&partnerID=40&md5=119903861e048e3ddc8dfb1bcc4fd0d9},
abstract={For multiple-well drilling and completion campaigns, cost and schedule performance tend to improve over time. This trend in improvement is commonly referred to as a "learning curve." When a learning curve is anticipated, the campaign cost and schedule estimates may be reduced dramatically relative to an assumption of constant performance. That is, ignoring the learning curve will lead to overly pessimistic estimates. While learning curves can be observed in campaigns of various lengths and complexity, they are typically most important in large campaigns where the majority of wells are drilled after a significant portion of the learning has occurred. Conversely, they may not be appropriate in short campaigns where there is a limited time to implement learnings, or in campaigns with highly idiosyncratic wells where learning does not necessarily translate across projects. Many operators consider the use of learning curves a best practice and provide procedures for estimation and implementation in their cost-estimating guidelines. In cases where comparison projects exist, estimating a learning curve for a prospective project can be achieved with some certainty. This form of deterministic learning is a well-established topic in the drilling-engineering literature and in practice. However, in cases where the sample of comparison projects is small, there may be significant uncertainty in the rate and magnitude of learning over time, and some form of probabilistic learning is more appropriate. This form of learning is not well established in the literature or in practice. This paper investigates methods for systematic integration of learning curves in probabilistic estimates. Brief reviews of probabilistic estimating methods and learning curves are provided. A general method and specific procedures for integrating learning curves in probabilistic estimates are then provided. For each method, the key assumptions are itemized and discussed and a demonstration is provided. While no single procedure will fit every situation, it is concluded that the general method is straightforward, transparent, and can be implemented using off-the-shelf spreadsheet software. The proposed procedures generate results that provide engineers and decision makers with a refined representation of uncertainty and can improve capital-investment valuation and decision making. Copyright © 2011 Society of Petroleum Engineers.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu2007300,
author={Liu, Z. and Wong, Y.S. and Lee, K.S.},
title={Towards effective multi-platforming design of product family using genetic algorithm},
journal={Proceedings of the 3rd IEEE International Conference on Automation Science and Engineering, IEEE CASE 2007},
year={2007},
pages={300-305},
art_number={4341674},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-44449121471&partnerID=40&md5=85a54a6367e5103b5e875a19002093d7},
abstract={Platform-based product family design is recognized as an effective method to construct a product line that satisfies diverse performance requirements while aiming to keep design and production costs low. The success of the resulting product family often relies on properly resolving the tradeoff between increasing commonality across the family and performance loss compared to individual design. In this paper, a systematic approach is proposed to design the scale-based product family with multi-platforming configuration and it contributes in three aspects. Firstly, the effect of commonality on the related product life-cycle activities is evaluated in the platform decision to decide the expected degree of sharing for each design variable and thus generate the anticipated platform configuration. Secondly, unlike many existing methods that assumes a given single-platform, the proposed method addresses the multi-platforming configuration across the family, and can generate alternative product family settings with different levels of commonality. Finally, the product family design is formulated as a multi-optimization problem and solved using a modified genetic algorithm. An industrial example of a planetary gear train for cordless drills is presented to demonstrate the proposed method. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fogliatto201214,
author={Fogliatto, F.S. and Da Silveira, G.J.C. and Borenstein, D.},
title={The mass customization decade: An updated review of the literature},
journal={International Journal of Production Economics},
year={2012},
volume={138},
number={1},
pages={14-25},
note={cited By (since 1996)9},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84859908356&partnerID=40&md5=0d435b905b2865ead62b0ec3c0ca633d},
abstract={The analysis and implementation of mass customization (MC) systems has received growing consideration by researchers and practitioners since the late 1980s. In this paper we update the literature review on MC presented in a previous paper (Da Silveira, G., Borenstein, D., Fogliatto, F.S., 2001. Mass customization: literature review and research directions. International Journal of Production Economics, 72 (1), 1-13), and identify research gaps to be investigated in the future. Major areas of research in MC, and journals in which works have been published are explored through summary statistics. The result is a concise compendium of the relevant literature produced on the topic in the past decade. © 2012 Elsevier B.V. All rights reserved.},
author_keywords={Literature review;  Mass customization;  Research gaps},
document_type={Review},
source={Scopus},
}

@ARTICLE{Johnson2009653,
author={Johnson, M.D. and Kirchain, R.E.},
title={Quantifying the effects of product family decisions on material selection: A process-based costing approach},
journal={International Journal of Production Economics},
year={2009},
volume={120},
number={2},
pages={653-668},
note={cited By (since 1996)10},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-67651177413&partnerID=40&md5=6700d4f2722f8d51d0d2370451faeb39},
abstract={Product designers must continually assess trade-offs among various performance attributes and cost. Materials choice can play an important role in that decision-making process. Product platforms are used to meet the demand for increased product variety, while also managing costs. Because of their interdependent effects, it is possible that platforming strategies may alter preferred material choice. This paper examines the interrelationship of these early stage design choices through the application of process-based cost modeling. A case study is detailed concerning two alternative material options for an automotive instrument panel beam: a die-cast magnesium design and a conventional design (i.e., discrete stamped steel components) which allows for more component sharing than the integrated magnesium design. The effects of component sharing on product family costs are analyzed. It is shown that the magnesium design is less competitive in platformed scenarios. © 2009 Elsevier B.V. All rights reserved.},
author_keywords={Automobile;  Case study;  Cost;  Materials selection;  Product families},
document_type={Article},
source={Scopus},
}

@ARTICLE{Johnson2010634,
author={Johnson, M.D. and Kirchain, R.},
title={Developing and assessing commonality metrics for product families: A process-based cost-modeling approach},
journal={IEEE Transactions on Engineering Management},
year={2010},
volume={57},
number={4},
pages={634-648},
art_number={5340516},
note={cited By (since 1996)4},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-77958483606&partnerID=40&md5=e585670cea5994e7c1b499dd2dda0f5a},
abstract={To be competitive in todays global economy, firms must deliver more products that are viable in the marketplace for shorter times. The use of product families allows firms to meet these needs in a cost-competitive manner. The determination of which components to share and which should be unique is very important to the development of product families. Commonality metrics are presented with the goal of assessing (at the early stages of development) the ability of the product family to reduce costs. The methodology of process-based cost modeling is used to project product development, fabrication, and assembly costs in both the standalone and shared situations. A case study of two automotive instrument panel beams is analyzed. Linear-regression analysis shows that when compared to total cost savings, a simple piece commonality metric and a fabrication-investment-weighted metric have higher R2 than a mass- or piece-cost-weighted metric. When correlated to fixed cost savings, the fabrication-investment-weighted metric has the highest R2 (0.62) and is significant at the 0.025 level. Fixed cost savings are proposed as the desired quantity when assessing product family efficiency. © 2006 IEEE.},
author_keywords={Case study;  commonality metrics;  cost modeling;  product families},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen20121537,
author={Chen, Y.-S. and Chang, K.-C. and Chang, C.-H.},
title={Nonlinear influence on R&D project performance},
journal={Technological Forecasting and Social Change},
year={2012},
volume={79},
number={8},
pages={1537-1547},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84866049354&partnerID=40&md5=d43ec9182ca81dec8bc008d2cc6e720f},
abstract={This study applies artificial neural network (ANN) to explore the relationships between the performance of R&D projects and its determinants. The results indicate that the quality of project environment has an inverse U-shaped effect on the performance of R&D projects, and both project managers' skills and the effectiveness of teamwork have monotonic positive influences on it. Besides, this study utilizes self-organizing map (SOM) to classify the Taiwanese information and electronics companies into three groups and further provides some suggestions. In addition, this paper uses an in-depth interview of qualitative research to explore why the quality of project environment has an inverse U-shaped effect on the R&D project performance, and finds out the main reason. There are two managerial implications in this study. First, the relationships between the performance of R&D projects and its determinants are not always linear in the complex and uncertain environment nowadays. Second, companies must care about the inverse U-shaped effect of the quality of project environment on the performance of R&D projects, although they can enhance the extent of project managers' skills and the effectiveness of their teamwork as much as possible. © 2012 Elsevier Inc.},
author_keywords={Effectiveness of teamwork;  Nonlinear nature of management;  Project managers' skills;  Project performance;  Quality of project environment},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Durelli201130,
author={Durelli, V.H.S. and Araujo, R.F. and Silva, M.A.G. and Oliveira, R.A.P. and Maldonado, J.C. and Delamaro, M.E.},
title={What a long, strange trip it's been: Past, present, and future perspectives on software testing research},
journal={Proceedings - 25th Brazilian Symposium on Software Engineering, SBES 2011},
year={2011},
pages={30-39},
art_number={6065143},
note={cited By (since 1996)2},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-82255172046&partnerID=40&md5=cd89afa234a6f03b9cae5e3e617ab60d},
abstract={Over the past 25 years the Brazilian Symposium on Software Engineering (SBES) has evolved to become the most important event on software engineering in Brazil. Throughout these years, SBES has gathered a large body of studies in software testing. Aimed at providing an insightful understanding of what has already been published in such event, we synthesized its rich 25-year history of research on software testing. Using information drawn from this overview we attempted to highlight which types of study have been the most applied for conveying software testing efforts. We also devised a co-authorship network to obtain a bird's-eye view of which research groups and scholars have been the most prolific ones. Moreover, by performing a citation analysis of the selected studies we set out to ascertain the importance of SBES in a wider scenario. Finally, borne out by the information extracted from the studies, we shed some light on the state-of-the-art of software testing in Brazil and provide an outlook on its foreseeable future. © 2011 IEEE.},
author_keywords={Software Testing;  systematic mapping},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li2000411,
author={Li, H. and Azarm, S.},
title={Product design selection under uncertainty and with competitive advantage},
journal={Journal of Mechanical Design, Transactions of the ASME},
year={2000},
volume={122},
number={4},
pages={411-418},
note={cited By (since 1996)124},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0000785341&partnerID=40&md5=67ee5f32a3b29bf8f6ac127c5d1fde4d},
abstract={This paper presents an approach wherein product design is viewed as a selection process with two main stages: design alternative generation and design alternative evaluation. The focus of this paper is mainly on a design alternative evaluation model in that designer's preferences, customers' preferences, and market competition are accounted for in order to select the best possible design. In the model, uncertainties in the product design life, market size and its yearly change, cost and its yearly change, price, and discount rate are considered. Product design selection of a cordless screwdriver is used as a demonstration example. However, the emphasis in the example is on the approach, and not on the details per se.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ben-Arieh20091969,
author={Ben-Arieh, D. and Easton, T. and Choubey, A.M.},
title={Solving the multiple platforms configuration problem},
journal={International Journal of Production Research},
year={2009},
volume={47},
number={7},
pages={1969-1988},
note={cited By (since 1996)6},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-60749085143&partnerID=40&md5=f8deda8d377c2abb4e00d5a50103b688},
abstract={The focus of manufacturing has been shifting from mass production to mass customization and producers are seeking ways to reduce production costs, still offering a competitive basket of products. One approach for implementing mass customization is to develop or produce products based on platform architecture. Variant products make use of the product platform as the starting point and then add or remove components to change features of the base product. This allows the manufacturer to offer the variety of products that meet market demands without developing each product independently. In this paper, we propose multiple platforms for the production of a given product family while minimizing the overall production cost. The methodology considers the demand for each product variant, with the decision variables as the optimal number of platforms, optimal configuration of each platform, and assignment of the products to the platforms. The problem is formulated as a mixed integer program, and both the optimal formulation and an evolutionary strategy based on Genetic Algorithm are presented. The approach is illustrated with an example from a family of cordless drills.},
author_keywords={Cost optimization;  Genetic and evolutionary search heuristics;  Mass customization;  Product platform},
document_type={Article},
source={Scopus},
}

@ARTICLE{Danese2004863,
author={Danese, P. and Romano, P.},
title={Improving inter-functional coordination to face high product variety and frequent modifications},
journal={International Journal of Operations and Production Management},
year={2004},
volume={24},
number={9},
pages={863-885},
note={cited By (since 1996)17},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-4444337585&partnerID=40&md5=045603ef648bafd529e0857e41b17a01},
abstract={In an effort to better respond to heterogeneous customer needs, an increasing number of companies in different sectors deal with the combination of high variety and frequent product changes/modifications. This entails planning, designing, purchasing and manufacturing activities and exacerbates the alignment of Sales, Production Planning and Engineering goals. This paper aims to suggest a way of improving coordination between such functional areas in contexts characterised by high variety and frequent product changes. Based on the data from an action Research study, a method for taking the contrasting requests of Sales, Production Planning and Engineering into account simultaneously and to facilitate the coordination of their activities is developed. It leverages Modular Bills and product modularity to improve inter-functional coordination. Initial indications are promising. However, given the specific nature of the studied case, further research is required to evaluate the generalizability of the findings.},
author_keywords={Process planning;  Product management;  Product variants;  Research},
document_type={Article},
source={Scopus},
}

@ARTICLE{Miller200735,
author={Miller, J.R. and Genc, I. and Driscoll, A.},
title={Wine price and quality: In search of a signaling equilibrium in 2001 California cabernet sauvignon},
journal={Journal of Wine Research},
year={2007},
volume={18},
number={1},
pages={35-46},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-34548386128&partnerID=40&md5=619223c841caa91cf877602e16b5455c},
abstract={Wine varies greatly in price and quality. In the limited sample we study here, 2001 California Cabernet Sauvignon wines rated by an expert in Wine Spectator, price ranges from $7 to $300 per bottle. Similarly, quality ratings on a 100-point scale range from 68 to 96 points. Our purpose here is twofold. First, we wish to estimate empirically a relationship between price and quality rating. Second, we hope to provide a discussion of characteristics of wine markets that might help refocus theoretical economic research on the wine price-quality relationship. Specifically, we suggest a movement away from hedonic pricing methods and their reliance on assumptions of competitive markets and perfect information. Rather, we suggest that imperfect competition and imperfect, asymmetric information characterize wine markets, and that the theoretical underpinnings of empirical work need to recognize this fact.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Qian20103,
author={Qian, L. and Jiao, J.},
title={A review of joint decisions on marketing and supply chain in product planning and design},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={2010},
volume={6},
pages={3-16},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-80054991098&partnerID=40&md5=906c98d8ad04a7232eb318407fa8b433},
abstract={Manufacturers design and develop numerous product variants to address different customer preferences in the competitive market. One product could be characterized by a vector of attributes such as sale price, reliability, and functionality. The challenge is how to make decisions on product or product family planning and design, supply chain, and marketing in a concurrent and integrated manner. Research on integration and coordination of product design, supply chain configuration, and marketing decisions is receiving much attention recently and need further investigation. The paper provides a comprehensive review on recent research incorporating marketing, management and engineering considerations in product planning and design. Copyright © 2010 by ASME.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ko200885,
author={Ko, O.-Y. and Paek, J.-H.},
title={Korea experience project management consortium on the U.S. Forces Korea Relocation Program},
journal={Journal of Asian Architecture and Building Engineering},
year={2008},
volume={7},
number={1},
pages={85-92},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-49249136333&partnerID=40&md5=9967f063e2c1903b1e88044e591d9878},
abstract={Program management is important for the successful completion of construction projects. The Korean government has encountered many problems such as poor cooperation, limited trust, and ineffective communications, often resulting in adversarial relationships between stakeholders, and thus poor project implementation in terms of time, cost, and quality. Organizations such as the Associated General Contractors of America, the American Society of Civil Engineering, and the Army Corps of Engineers have championed partnering through educational programs, workshops, and training manuals. However, few studies have conducted an empirical investigation of the process. Our aim was to leverage advantages for the product line construction of a mega military construction project using dedicated partnering based on a program management consortium (PMC) model and to implement an ongoing feedback program of best practice/lessons learned to minimize the mistakes made in sequenced construction. We discuss key developmental aspects of the United States Forces and a Korean relocation program management consortium. Our findings will benefit in the performance of the United States Forces, Korean relocation program management, and other large government-run public or private consortium-funded projects. They will also aid in the identification of new and creative ways to solve issues associated with the establishment of program management consortiums.},
author_keywords={Consortium;  Partnering;  Project management;  Project management consortium},
document_type={Article},
source={Scopus},
}

@ARTICLE{Atkinson2003207,
author={Atkinson, C. and Bunse, C. and Wüst, J.},
title={Driving component-based software development through quality modelling},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2003},
volume={2693},
pages={207-224},
note={cited By (since 1996)3},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33749071298&partnerID=40&md5=9cf893945ff225d49d2a596f7e8dfe10},
abstract={With the advent of the OMG's new Model Driven Architecture (MDA), and the growing uptake of the UML, the concept of model-driven development is receiving increasing attention. Many software organizations have identified the MDA as being of strategic importance to their businesses and many UML-tool vendors now market their tools as supporting model-driven development. However, most UML tools today support only a very limited concept of model driven development - the idea of first creating platform independent models and then mapping them into executable code. In contrast, true model-driven development implies that the development flow of a project is in some way "driven" (i.e. guided) by models. Quality attributes of models (e.g., measures derived from structural attributes) could be used in this regard, but although many different types of measures have been proposed (e.g. coupling, complexity, cohesion) they are not widely used in practice. This chapter discusses the issues involved in supporting this more general view of model driven development. It first presents some strategies for deriving useful quality-related information from UML models and then illustrates how this information can be use to optimize project effort and develop high-quality components. We pay special attention to how quality modelling based on structural properties can be integrated into the OMG's Model Driven Architecture (MDA) initiative. © Springer-Verlag Berlin Heidelberg 2003.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jiao20075,
author={Jiao, J. and Simpson, T.W. and Siddique, Z.},
title={Product family design and platform-based product development: A state-of-the-art review},
journal={Journal of Intelligent Manufacturing},
year={2007},
volume={18},
number={1},
pages={5-29},
note={cited By (since 1996)172},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-34547236488&partnerID=40&md5=03d28174bdc100706eb325503cb76e53},
abstract={Product family design and platform-based product development has received much attention over the last decade. This paper provides a comprehensive review of the state-of-the-art research in this field. A decision framework is introduced to reveal a holistic view of product family design and platform-based product development, encompassing both front-end and back-end issues. The review is organized according to various topics in relation to product families, including fundamental issues and definitions, product portfolio and product family positioning, platform-based product family design, manufacturing and production, as well as supply chain management. Major challenges and future research directions are also discussed. © 2007 Springer Science+Business Media, LLC.},
author_keywords={Commonality;  Configuration;  Customization;  Product architecture;  Product family;  Product platform;  Variety modularity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li200654,
author={Li, M.},
title={Expanding the horizons of software development processes: A 3-D integrated methodology},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={3840 LNCS},
pages={54-67},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33745125902&partnerID=40&md5=93a90486f10610472a575705fc9abfc3},
abstract={This paper investigates how to define and improve software development processes. Based on examining the software development during last two decades, it proposes a breakthrough point of an updated view of requirements, called Great Requirements, and presents a 3-D integrated software engineering methodology for improving software development activities. It expands the horizons of possible future software development processes. © Springer-Verlag Berlin Heidelberg 2005.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Durelli2013934,
author={Durelli, V.H.S. and Araujo, R.F. and Silva, M.A.G. and Oliveira, R.A.P.D. and Maldonado, J.C. and Delamaro, M.E.},
title={A scoping study on the 25 years of research into software testing in Brazil and an outlook on the future of the area},
journal={Journal of Systems and Software},
year={2013},
volume={86},
number={4},
pages={934-950},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84875256671&partnerID=40&md5=1a211c7b0c507bd4d444bd7b3094d703},
abstract={Over the past 25 years the Brazilian Symposium on Software Engineering (SBES) has evolved to become the most important event on software engineering in Brazil. Throughout these years, SBES has gathered a large body of studies in software testing. Aimed at providing an insightful understanding of what has already been published in such event, we have synthesized its 25-year history of research on software testing. Using information drawn from this overview we highlighted which software testing topics have been the most extensively surveyed in SBES literature. We have also devised a co-authorship network to depict the most prolific research groups and researchers. Moreover, by performing a citation analysis of the selected studies we have tried to ascertain the importance of SBES in a wider scenario. Finally, using the information extracted from the studies, we have shed light on the state-of-the-art of software testing in Brazil and provided an outlook on its foreseeable future. © 2012 Elsevier Inc.},
author_keywords={Brazilian research;  Software testing;  Systematic mapping},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Krishnan2006813,
author={Krishnan, V. and Zhu, W.},
title={Designing a family of development-intensive products},
journal={Management Science},
year={2006},
volume={52},
number={6},
pages={813-825},
note={cited By (since 1996)29},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-33744793209&partnerID=40&md5=7b968d18573e296add552f3cd395a8e4},
abstract={Faced with fragmented markets, saturated and demanding customers, and global competition, firms increasingly must design and offer a line of innovative, quality-differentiated products to target customers with differing willingness to pay (WTP). In this context, designing a special class of products that we term development-intensive products (DIPs) - for which the fixed costs of development far outweigh the unit-variable costs - presents some unique managerial challenges. Examples of such development-intensive offerings abound in a number of industries, including the pharmaceutical, information, and entertainment sectors of the economy. Our contributions in this paper are threefold: (a) to show that managerial insights from the traditional approach to product-line design developed for unit-variable cost-intensive products do not carry over to DIPs, (b) to present new mechanisms and managerial guidelines for designing a family of products for which development costs cannot be ignored, and (c) to illustrate the insights with an extended industry example. We find that the design approach based on degrading (or subtracting value from) a high-end product to obtain a subsumed low-end edition, shown in the literature to be an effective approach for designing unit cost-intensive products, can be inappropriate for DIPs. This limitation of value subtraction has implications for the number of variants and the sequence in which they are developed. As an alternative to a subsumed product-design strategy, we propose and examine the overlapped product-design approach, in which a low-end product is not completely subsumed within its high-end counterpart, but differentiated on additional vertical quality dimensions. Our results both explain the recent challenges of firms with subsumed low-end products and guide them in designing a product line to successfully address emerging low-end market segments. © 2006 INFORMS.},
author_keywords={Development-intensive offerings;  Multiple quality dimensions;  Product-family design},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ghapanchi2011238,
author={Ghapanchi, A.H. and Aurum, A.},
title={Antecedents to IT personnel's intentions to leave: A systematic literature review},
journal={Journal of Systems and Software},
year={2011},
volume={84},
number={2},
pages={238-249},
note={cited By (since 1996)10},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78650604580&partnerID=40&md5=54cdd36f5359438f2e54e2ec004b2c1d},
abstract={This paper undertakes a systematic review to gain insight into existing studies on the turnover of information technology (IT) personnel. Our systematic review of 72 studies from 1980 to 2008 examines the background and trend of research into IT personnel's intentions to leave their workplaces, in addition to providing a taxonomy of the determinants of their intentions to quit as captured in IT literature. We note a huge growth in the number of academic papers on the topic since 1998. Moreover, most of the research on IT turnover has been undertaken in North America, followed by Asia. Based on the 72 extracted studies, we found a total of 70 conceptually distinct IT turnover drivers. We classified them into the 5 broad categories of individual, organisational, job-related, psychological, and environmental, each containing three to four sub-categories. Finally, this paper presents insightful recommendations for IT practitioners as well as for the research community. © 2010 Elsevier Inc. All rights reserved.},
author_keywords={Employee retention;  Employee turnover;  Intention to leave;  IT personnel;  Systematic review},
document_type={Review},
source={Scopus},
}

@ARTICLE{Finkler2003348,
author={Finkler, S.A. and Ward, D.M.},
title={The Case for the Use of Evidence-based Management Research for the Control of Hospital Costs},
journal={Health Care Management Review},
year={2003},
volume={28},
number={4},
pages={348-365},
note={cited By (since 1996)20},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-0242573314&partnerID=40&md5=a94e8c849a47804824953a089cfdd717},
abstract={This article explores the current state of the creation and use of evidence by managers for cost containment in hospitals. We assert that hospitals do not know enough about what things cost, and until they get evidence on costs, it is not likely that much can be done to narrow the chasm between common practice and best practice. Part of the problem is that managers do not seek out available evidence that exists, and part of the problem is a lack of sufficient research efforts to generate evidence for managers to use. The article strives to help direct future efforts by researchers and managers in the area of evidence-based cost containment research by presenting a framework for priorities that managers and researchers can use to increase the amount of research done to generate evidence and to increase the use of evidence by health care managers.},
author_keywords={Cost;  Cost containment;  Cost control;  Evidence;  Evidence-based management;  Health care;  Hospital;  Management research},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Poma2010786,
author={Poma, C. and Verda, V. and Consonni, S.},
title={Design and performance evaluation of a waste-to-energy plant integrated with a combined cycle},
journal={Energy},
year={2010},
volume={35},
number={2},
pages={786-793},
note={cited By (since 1996)11},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-76449089336&partnerID=40&md5=e1ef74e2e28983a9d5198882027e3674},
abstract={In this paper, a waste-to-energy (WTE) system integrated with a gas fuelled combined cycle is considered. The plant is designed as a possible future option for thermal utilization of urban wastes in the northern part of the Turin Province, Italy. The plant should provide electricity (about 160 MW at maximum electric load) to the grid and heat to a district heating network (about 50 MW at maximum thermal load). This kind of plants is particularly interesting because of the high net electric efficiency (about 46%) that is possible to achieve, compared with the equivalent global efficiency of the separate plants (about +7% waste utilization efficiency with respect to conventional plants), and the complex design that is required. The initial plant design is improved through a thermoeconomic procedure. The optimal plant is characterized by -0.2% unit cost of electricity and +0.6 MW electricity production with respect to the initial design. An economic analysis is also performed. Economic indicators are estimated and used to complete the comparison between the conventional and the integrated solutions under different market conditions. With respect to a stand-alone waste-to-energy plant, the integrated plant is characterized by similar pay-back period and higher net benefit cost ratio. © 2009 Elsevier Ltd. All rights reserved.},
author_keywords={Thermoeconomic analysis;  Waste-to-energy},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nascimento2013107,
author={Nascimento, A.S. and Rubira, C.M.F. and Burrows, R. and Castor, F.},
title={A systematic review of design diversity-based solutions for fault-tolerant SOAs},
journal={ACM International Conference Proceeding Series},
year={2013},
pages={107-118},
note={cited By (since 1996)0},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84877261010&partnerID=40&md5=3858eeb4494e612f9c96c43e31837828},
abstract={Context: Over recent years, software developers have been evaluating the benefits of both Service-Oriented Architecture and software fault tolerance techniques based on design diversity by creating fault-tolerant composite services that leverage functionally equivalent services, or variant services. Three major design issues need to be considered while building software fault-tolerant architectures based on design diversity namely, selection and execution of variants and selection of an adjudication algorithm to determine the correct or adjudicated result from the variants. Each design issue, in turn, can be realized by a set of alternative design solutions, which present different degrees of quality requirements (e.g. memory consumption and reliability). Objective: To investigate whether existing approaches for fault-tolerant composite services support the above mentioned design issues and to provide a detailed classification of the analysed approaches. Method: A systematic literature review of diversity-based approaches for fault-tolerant composite services, which compose our primary studies. Results: We found 17 primary studies providing direct evidence about the research question. Our findings reveal that the primary studies support a wide variety of design decisions. For example, (i) variant services may be chosen at different points during the software lifecycle; (ii) both parallel and sequential execution schemes have been addressed; and (iii) a variety of adjudication mechanisms were found amongst the target papers. Conclusion: We build up a broad picture of what design issues have been addressed by existing diversity-based approaches for fault-tolerant composite services. Finally, practical issues and difficulties are summarized and directions for future work are suggested. Copyright 2013 ACM.},
author_keywords={Composite services;  Fault tolerance;  SLR;  SOA},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Riaz2009119,
author={Riaz, M. and Sulayman, M. and Naqvi, H.},
title={Architectural decay during continuous software evolution and impact of 'design for change' on software architecture},
journal={Communications in Computer and Information Science},
year={2009},
volume={59 CCIS},
pages={119-126},
note={cited By (since 1996)2},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-78049356237&partnerID=40&md5=ed73756d04a8fd2381f58b9927ea8692},
abstract={Software architecture is the blue print of software and guides the development and evolution of the software. A good design produces quality software and careful evolution of software leads to a longer life of the software whereas a bad design and careless evolution leads to decay of the software. This paper discusses the phenomenon of architectural decay and gives an account of the practices suggested in the literature for identification, resolution and prevention of architectural decay. The observations from a controlled experiment to study the impact of the prevention practice 'design for change' are also discussed. The results from the studied metrics suggest that software created without following a proper design has a greater tendency to decay. © 2009 Springer-Verlag.},
author_keywords={Architectural Decay;  Software Architecture;  Software Evolution},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Aleksandrova2013,
author={Aleksandrova, I.V. and Koresheva, E.R. and Koshelev, E.L. and Osipov, I.E.},
title={Pilot target supply system based on the FST technologies: Main building blocks, layout algorithms and results of the testing experiments},
journal={Plasma and Fusion Research},
year={2013},
volume={8},
number={SPL.ISS.2},
art_number={3404052},
note={cited By (since 1996)1},
url={http://www.scopus.com/inward/record.url?eid=2-s2.0-84877985611&partnerID=40&md5=f6eacbf1f6694074c9013b0465aaf999},
abstract={In this report, we discuss the main building blocks and interface units of the FST supply system, layout algorithm and new results of the testing experiments. In this area, we firstly give a presentation of the FST approach to design and construction of the target supply system for HiPER project; describe possible ways of integration of our latest developments into engineering applications (e.g. FST-layering module with a doublespiral layering channel), and discuss the cost estimations of the FST supply system in the case of HIPER-scale targets. © 2013 The Japan Society of Plasma Science and Nuclear Fusion Research.},
author_keywords={Burst mode;  Cryogenic layering;  Cryogenic target injection;  Free-standing target;  Rep-rate mode},
document_type={Article},
source={Scopus},
}
